{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a33344",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "## Peeringdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1c32292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>org_id</th>\n",
       "      <th>name</th>\n",
       "      <th>aka</th>\n",
       "      <th>name_long</th>\n",
       "      <th>website</th>\n",
       "      <th>social_media</th>\n",
       "      <th>asn</th>\n",
       "      <th>looking_glass</th>\n",
       "      <th>route_server</th>\n",
       "      <th>...</th>\n",
       "      <th>policy_ratio</th>\n",
       "      <th>policy_contracts</th>\n",
       "      <th>allow_ixp_update</th>\n",
       "      <th>status_dashboard</th>\n",
       "      <th>rir_status</th>\n",
       "      <th>rir_status_updated</th>\n",
       "      <th>logo</th>\n",
       "      <th>created</th>\n",
       "      <th>updated</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8897</td>\n",
       "      <td>GTT Communications (AS4436)</td>\n",
       "      <td>Formerly known as nLayer Communications</td>\n",
       "      <td></td>\n",
       "      <td>http://www.gtt.net</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'http://...</td>\n",
       "      <td>4436</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>Required</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2022-07-27T05:33:22Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>Akamai Technologies</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://www.akamai.com/</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'https:/...</td>\n",
       "      <td>20940</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Not Required</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.akamaistatus.com/</td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2025-10-20T12:16:12Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>DALnet IRC Network</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>http://www.dal.net</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'http://...</td>\n",
       "      <td>31800</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Not Required</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2025-01-09T13:42:07Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>9350</td>\n",
       "      <td>Swisscom</td>\n",
       "      <td>IP-Plus</td>\n",
       "      <td></td>\n",
       "      <td>http://www.swisscom.com</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'http://...</td>\n",
       "      <td>3303</td>\n",
       "      <td></td>\n",
       "      <td>telnet://route-server.ip-plus.net</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>Required</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2025-08-12T06:33:30Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>Cox Communications</td>\n",
       "      <td>Cox Communications</td>\n",
       "      <td></td>\n",
       "      <td>http://www.cox.com/peering</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'http://...</td>\n",
       "      <td>22773</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Required</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2022-11-28T22:55:17Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  org_id                         name  \\\n",
       "0   1    8897  GTT Communications (AS4436)   \n",
       "1   2      14          Akamai Technologies   \n",
       "2   3      17           DALnet IRC Network   \n",
       "3   5    9350                     Swisscom   \n",
       "4   6      23           Cox Communications   \n",
       "\n",
       "                                       aka name_long  \\\n",
       "0  Formerly known as nLayer Communications             \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                  IP-Plus             \n",
       "4                       Cox Communications             \n",
       "\n",
       "                      website  \\\n",
       "0          http://www.gtt.net   \n",
       "1     https://www.akamai.com/   \n",
       "2          http://www.dal.net   \n",
       "3     http://www.swisscom.com   \n",
       "4  http://www.cox.com/peering   \n",
       "\n",
       "                                        social_media    asn looking_glass  \\\n",
       "0  [{'service': 'website', 'identifier': 'http://...   4436                 \n",
       "1  [{'service': 'website', 'identifier': 'https:/...  20940                 \n",
       "2  [{'service': 'website', 'identifier': 'http://...  31800                 \n",
       "3  [{'service': 'website', 'identifier': 'http://...   3303                 \n",
       "4  [{'service': 'website', 'identifier': 'http://...  22773                 \n",
       "\n",
       "                        route_server  ... policy_ratio policy_contracts  \\\n",
       "0                                     ...         True         Required   \n",
       "1                                     ...        False     Not Required   \n",
       "2                                     ...        False     Not Required   \n",
       "3  telnet://route-server.ip-plus.net  ...         True         Required   \n",
       "4                                     ...        False         Required   \n",
       "\n",
       "  allow_ixp_update               status_dashboard  rir_status  \\\n",
       "0            False                           None          ok   \n",
       "1            False  https://www.akamaistatus.com/          ok   \n",
       "2            False                                         ok   \n",
       "3            False                                         ok   \n",
       "4            False                                         ok   \n",
       "\n",
       "     rir_status_updated  logo               created               updated  \\\n",
       "0  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2022-07-27T05:33:22Z   \n",
       "1  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2025-10-20T12:16:12Z   \n",
       "2  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2025-01-09T13:42:07Z   \n",
       "3  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2025-08-12T06:33:30Z   \n",
       "4  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2022-11-28T22:55:17Z   \n",
       "\n",
       "   status  \n",
       "0      ok  \n",
       "1      ok  \n",
       "2      ok  \n",
       "3      ok  \n",
       "4      ok  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "filepath = Path('peeringdb/peeringdb_2_dump_2025_10_21.json')\n",
    "\n",
    "with filepath.open('r', encoding='utf-8') as f:\n",
    "    dump = json.load(f)\n",
    "\n",
    "# extract the net.data section and load into a DataFrame\n",
    "net_data = dump.get('net', {}).get('data')\n",
    "if net_data is None:\n",
    "    raise KeyError(\"JSON does not contain 'net' -> 'data' structure\")\n",
    "\n",
    "net_df = pd.DataFrame(net_data)\n",
    "net_df['asn'] = net_df['asn'].astype(int)\n",
    "net_df = net_df[net_df['info_type'] != '']\n",
    "\n",
    "# show a quick preview\n",
    "net_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2f20d5",
   "metadata": {},
   "source": [
    "# Caida AS Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c190e0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aut</th>\n",
       "      <th>changed</th>\n",
       "      <th>org_id</th>\n",
       "      <th>source</th>\n",
       "      <th>org_name</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20240618.0</td>\n",
       "      <td>LPL-141-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Level 3 Parent, LLC</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20231108.0</td>\n",
       "      <td>UNIVER-19-Z-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>University of Delaware</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>20100927.0</td>\n",
       "      <td>MIT-2-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Massachusetts Institute of Technology</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>20230929.0</td>\n",
       "      <td>USC-32-Z-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>University of Southern California</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>20200723.0</td>\n",
       "      <td>WGL-117-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>WFA Group LLC</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   aut     changed            org_id source  \\\n",
       "0    1  20240618.0      LPL-141-ARIN   ARIN   \n",
       "1    2  20231108.0  UNIVER-19-Z-ARIN   ARIN   \n",
       "2    3  20100927.0        MIT-2-ARIN   ARIN   \n",
       "3    4  20230929.0     USC-32-Z-ARIN   ARIN   \n",
       "4    5  20200723.0      WGL-117-ARIN   ARIN   \n",
       "\n",
       "                                org_name country  \n",
       "0                    Level 3 Parent, LLC      US  \n",
       "1                 University of Delaware      US  \n",
       "2  Massachusetts Institute of Technology      US  \n",
       "3      University of Southern California      US  \n",
       "4                          WFA Group LLC      US  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "with open('./caida/20251001.as-org2info.txt', 'r', newline='', encoding='utf-8') as input_file:\n",
    "    lines = input_file.readlines()   \n",
    "    # Buffers initialisieren\n",
    "    aut_lines = []\n",
    "    org_lines = []\n",
    "    mode = None\n",
    "    total_lines = len(lines)\n",
    "    aut_count = 0\n",
    "    org_count = 0 \n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"# format:aut\"):\n",
    "            mode = \"aut\"\n",
    "            continue\n",
    "        elif line.startswith(\"# format:org_id\"):\n",
    "            mode = \"org\"\n",
    "            continue\n",
    "        elif line.startswith(\"#\") or not line:\n",
    "            # Andere Kommentar- oder Leerzeilen überspringen\n",
    "            continue      \n",
    "        if mode == \"aut\":\n",
    "            aut_lines.append(line)\n",
    "            aut_count += 1\n",
    "        elif mode == \"org\":\n",
    "            org_lines.append(line)\n",
    "            org_count += 1\n",
    "    # StringIO-Objekte aus den gesammelten Zeilen bauen\n",
    "    aut_buffer = io.StringIO(\"\\n\".join(aut_lines))\n",
    "    org_buffer = io.StringIO(\"\\n\".join(org_lines))\n",
    "    # DataFrames einlesen\n",
    "    aut_df = pd.read_csv(aut_buffer, sep=\"|\",\n",
    "                        names=[\"aut\", \"changed\", \"aut_name\", \"org_id\", \"opaque_id\", \"source\"], usecols=[\"aut\", \"org_id\", \"source\", \"changed\"])\n",
    "    org_df = pd.read_csv(org_buffer, sep=\"|\",\n",
    "                        names=[\"org_id\", \"changed\", \"org_name\", \"country\", \"source\"], usecols=[\"org_id\", \"org_name\", \"country\"])\n",
    "\n",
    "    # Join the DataFrames\n",
    "    joined_df = pd.merge(aut_df, org_df, on=\"org_id\", how=\"left\")\n",
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73e5047",
   "metadata": {},
   "source": [
    "## Join both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6de8ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asn</th>\n",
       "      <th>org_name</th>\n",
       "      <th>country</th>\n",
       "      <th>source</th>\n",
       "      <th>info_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4436</td>\n",
       "      <td>GTT Americas, LLC</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>NSP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20940</td>\n",
       "      <td>Akamai International B.V.</td>\n",
       "      <td>NL</td>\n",
       "      <td>RIPE</td>\n",
       "      <td>Content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31800</td>\n",
       "      <td>DALnet</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Non-Profit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3303</td>\n",
       "      <td>Swisscom (Schweiz) AG</td>\n",
       "      <td>CH</td>\n",
       "      <td>RIPE</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22773</td>\n",
       "      <td>Cox Communications Inc.</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     asn                   org_name country source      info_type\n",
       "0   4436          GTT Americas, LLC      US   ARIN            NSP\n",
       "1  20940  Akamai International B.V.      NL   RIPE        Content\n",
       "2  31800                     DALnet      US   ARIN     Non-Profit\n",
       "3   3303      Swisscom (Schweiz) AG      CH   RIPE  Cable/DSL/ISP\n",
       "4  22773    Cox Communications Inc.      US   ARIN  Cable/DSL/ISP"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peering_df_joined = pd.merge(net_df, joined_df, left_on='asn', right_on='aut', how='left')\n",
    "peering_df_joined = peering_df_joined[['asn', 'org_name', 'country', 'source', 'info_type']]\n",
    "peering_df_joined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cf69bc",
   "metadata": {},
   "source": [
    "## Caida AS Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12bd17ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching AS Rank data: first=5000, offset=0\n"
     ]
    },
    {
     "ename": "ConnectTimeout",
     "evalue": "HTTPSConnectionPool(host='api.asrank.caida.org', port=443): Max retries exceeded with url: /v2/restful/asns/?first=5000&offset=0 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7cbcb3f7d280>, 'Connection to api.asrank.caida.org timed out. (connect timeout=None)'))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/urllib3/connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/urllib3/util/connection.py:85\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/urllib3/util/connection.py:73\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     72\u001b[39m     sock.bind(source_address)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[31mTimeoutError\u001b[39m: [Errno 110] Connection timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mConnectTimeoutError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:488\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    487\u001b[39m         new_e = _wrap_proxy_error(new_e, conn.proxy.scheme)\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/urllib3/connection.py:753\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    752\u001b[39m sock: socket.socket | ssl.SSLSocket\n\u001b[32m--> \u001b[39m\u001b[32m753\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    754\u001b[39m server_hostname: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28mself\u001b[39m.host\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/urllib3/connection.py:207\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[32m    208\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    209\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConnection to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.host\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.timeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    210\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mConnectTimeoutError\u001b[39m: (<urllib3.connection.HTTPSConnection object at 0x7cbcb3f7d280>, 'Connection to api.asrank.caida.org timed out. (connect timeout=None)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMaxRetryError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/urllib3/util/retry.py:519\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    518\u001b[39m     reason = error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    521\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mIncremented Retry for (url=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, url, new_retry)\n",
      "\u001b[31mMaxRetryError\u001b[39m: HTTPSConnectionPool(host='api.asrank.caida.org', port=443): Max retries exceeded with url: /v2/restful/asns/?first=5000&offset=0 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7cbcb3f7d280>, 'Connection to api.asrank.caida.org timed out. (connect timeout=None)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectTimeout\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m(next_page):\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFetching AS Rank data: first=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfirst\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, offset=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moffset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://api.asrank.caida.org/v2/restful/asns/?first=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfirst\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m&offset=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moffset\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRequest failed with status code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/requests/adapters.py:665\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    662\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, ConnectTimeoutError):\n\u001b[32m    663\u001b[39m     \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n\u001b[32m    664\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, NewConnectionError):\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeout(e, request=request)\n\u001b[32m    667\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, ResponseError):\n\u001b[32m    668\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request=request)\n",
      "\u001b[31mConnectTimeout\u001b[39m: HTTPSConnectionPool(host='api.asrank.caida.org', port=443): Max retries exceeded with url: /v2/restful/asns/?first=5000&offset=0 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7cbcb3f7d280>, 'Connection to api.asrank.caida.org timed out. (connect timeout=None)'))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "next_page = True\n",
    "nodes = []\n",
    "first=5000\n",
    "offset=0\n",
    "while(next_page):\n",
    "    print(f\"Fetching AS Rank data: first={first}, offset={offset}\")\n",
    "    response = requests.get(f\"https://api.asrank.caida.org/v2/restful/asns/?first={first}&offset={offset}\")\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(f\"Request failed with status code {response.status_code}\")\n",
    "\n",
    "    as_rank_dump = response.json()\n",
    "    nodes.extend(as_rank_dump['data']['asns']['edges'])\n",
    "    if not as_rank_dump['data']['asns']['pageInfo']['hasNextPage']:\n",
    "        next_page = False\n",
    "    offset += first\n",
    "\n",
    "edges = [e['node'] for e in nodes]\n",
    "as_rank_df = pd.DataFrame(edges)\n",
    "# 1️⃣ asnDegree (dict) in eigene Spalten auflösen\n",
    "asnDegree_df = as_rank_df[\"asnDegree\"].apply(pd.Series)\n",
    "asnDegree_df.columns = [f\"asnDegree_{c}\" for c in asnDegree_df.columns]\n",
    "\n",
    "# 2️⃣ wieder an den Haupt-DataFrame anhängen\n",
    "as_rank_df = pd.concat([as_rank_df.drop(columns=[\"asnDegree\"]), asnDegree_df], axis=1)\n",
    "\n",
    "# 1️⃣ asnDegree (dict) in eigene Spalten auflösen\n",
    "asnCone_df = as_rank_df[\"cone\"].apply(pd.Series)\n",
    "asnCone_df.columns = [f\"cone_{c}\" for c in asnCone_df.columns]\n",
    "\n",
    "# 2️⃣ wieder an den Haupt-DataFrame anhängen\n",
    "as_rank_df = pd.concat([as_rank_df.drop(columns=[\"cone\"]), asnCone_df], axis=1)\n",
    "\n",
    "# show a quick preview\n",
    "as_rank_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9198f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "as_rank_df.to_csv('as_rank_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "408a3dd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'as_rank_df.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m as_rank_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mas_rank_df.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'as_rank_df.csv'"
     ]
    }
   ],
   "source": [
    "as_rank_df = pd.read_csv('as_rank_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f80590c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asn</th>\n",
       "      <th>rank</th>\n",
       "      <th>asnDegree_total</th>\n",
       "      <th>asnDegree_customer</th>\n",
       "      <th>asnDegree_peer</th>\n",
       "      <th>asnDegree_provider</th>\n",
       "      <th>cone_numberAsns</th>\n",
       "      <th>cone_numberPrefixes</th>\n",
       "      <th>cone_numberAddresses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3356</td>\n",
       "      <td>1</td>\n",
       "      <td>6613</td>\n",
       "      <td>6545</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>53986</td>\n",
       "      <td>873410</td>\n",
       "      <td>3468642119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1299</td>\n",
       "      <td>2</td>\n",
       "      <td>2567</td>\n",
       "      <td>2509</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>41193</td>\n",
       "      <td>776707</td>\n",
       "      <td>3219679484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>174</td>\n",
       "      <td>3</td>\n",
       "      <td>6723</td>\n",
       "      <td>6626</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>38887</td>\n",
       "      <td>730166</td>\n",
       "      <td>3034352967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3257</td>\n",
       "      <td>4</td>\n",
       "      <td>1853</td>\n",
       "      <td>1816</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>36040</td>\n",
       "      <td>612491</td>\n",
       "      <td>2791999209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2914</td>\n",
       "      <td>5</td>\n",
       "      <td>1541</td>\n",
       "      <td>1483</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>25179</td>\n",
       "      <td>576134</td>\n",
       "      <td>2918763154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119390</th>\n",
       "      <td>56279</td>\n",
       "      <td>78320</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119391</th>\n",
       "      <td>215758</td>\n",
       "      <td>78320</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119392</th>\n",
       "      <td>144817</td>\n",
       "      <td>78320</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119393</th>\n",
       "      <td>144068</td>\n",
       "      <td>78320</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119394</th>\n",
       "      <td>2303</td>\n",
       "      <td>78320</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119395 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           asn   rank  asnDegree_total  asnDegree_customer  asnDegree_peer  \\\n",
       "0         3356      1             6613                6545              68   \n",
       "1         1299      2             2567                2509              58   \n",
       "2          174      3             6723                6626              97   \n",
       "3         3257      4             1853                1816              37   \n",
       "4         2914      5             1541                1483              58   \n",
       "...        ...    ...              ...                 ...             ...   \n",
       "119390   56279  78320                0                   0               0   \n",
       "119391  215758  78320                0                   0               0   \n",
       "119392  144817  78320                0                   0               0   \n",
       "119393  144068  78320                0                   0               0   \n",
       "119394    2303  78320                0                   0               0   \n",
       "\n",
       "        asnDegree_provider  cone_numberAsns  cone_numberPrefixes  \\\n",
       "0                        0            53986               873410   \n",
       "1                        0            41193               776707   \n",
       "2                        0            38887               730166   \n",
       "3                        0            36040               612491   \n",
       "4                        0            25179               576134   \n",
       "...                    ...              ...                  ...   \n",
       "119390                   0                1                    0   \n",
       "119391                   0                1                    0   \n",
       "119392                   0                1                    0   \n",
       "119393                   0                1                    0   \n",
       "119394                   0                1                    0   \n",
       "\n",
       "        cone_numberAddresses  \n",
       "0                 3468642119  \n",
       "1                 3219679484  \n",
       "2                 3034352967  \n",
       "3                 2791999209  \n",
       "4                 2918763154  \n",
       "...                      ...  \n",
       "119390                     0  \n",
       "119391                     0  \n",
       "119392                     0  \n",
       "119393                     0  \n",
       "119394                     0  \n",
       "\n",
       "[119395 rows x 9 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges = [e['node'] for e in nodes]\n",
    "as_rank_df = pd.DataFrame(edges)\n",
    "asnDegree_df = as_rank_df[\"asnDegree\"].apply(pd.Series)\n",
    "asnDegree_df.columns = [f\"asnDegree_{c}\" for c in asnDegree_df.columns]\n",
    "\n",
    "# 2️⃣ wieder an den Haupt-DataFrame anhängen\n",
    "as_rank_df = pd.concat([as_rank_df.drop(columns=[\"asnDegree\"]), asnDegree_df], axis=1)\n",
    "\n",
    "# 1️⃣ asnDegree (dict) in eigene Spalten auflösen\n",
    "asnCone_df = as_rank_df[\"cone\"].apply(pd.Series)\n",
    "asnCone_df.columns = [f\"cone_{c}\" for c in asnCone_df.columns]\n",
    "\n",
    "# 2️⃣ wieder an den Haupt-DataFrame anhängen\n",
    "as_rank_df = pd.concat([as_rank_df.drop(columns=[\"cone\"]), asnCone_df], axis=1)\n",
    "\n",
    "as_rank_df['asn'] = as_rank_df['asn'].astype(int)\n",
    "\n",
    "\n",
    "as_rank_df = as_rank_df[['asn', 'rank', 'asnDegree_total', 'asnDegree_customer', 'asnDegree_peer', 'asnDegree_provider', 'cone_numberAsns', 'cone_numberPrefixes', 'cone_numberAddresses']]\n",
    "\n",
    "# show a quick preview\n",
    "as_rank_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a9da4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asn</th>\n",
       "      <th>org_name</th>\n",
       "      <th>country</th>\n",
       "      <th>source</th>\n",
       "      <th>info_type</th>\n",
       "      <th>rank</th>\n",
       "      <th>asnDegree_total</th>\n",
       "      <th>asnDegree_customer</th>\n",
       "      <th>asnDegree_peer</th>\n",
       "      <th>asnDegree_provider</th>\n",
       "      <th>cone_numberAsns</th>\n",
       "      <th>cone_numberPrefixes</th>\n",
       "      <th>cone_numberAddresses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4436</td>\n",
       "      <td>GTT Americas, LLC</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>NSP</td>\n",
       "      <td>78320.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20940</td>\n",
       "      <td>Akamai International B.V.</td>\n",
       "      <td>NL</td>\n",
       "      <td>RIPE</td>\n",
       "      <td>Content</td>\n",
       "      <td>1894.0</td>\n",
       "      <td>485.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8945.0</td>\n",
       "      <td>14612752.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31800</td>\n",
       "      <td>DALnet</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Non-Profit</td>\n",
       "      <td>47745.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3303</td>\n",
       "      <td>Swisscom (Schweiz) AG</td>\n",
       "      <td>CH</td>\n",
       "      <td>RIPE</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "      <td>81.0</td>\n",
       "      <td>1273.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>733.0</td>\n",
       "      <td>22131.0</td>\n",
       "      <td>42899794.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22773</td>\n",
       "      <td>Cox Communications Inc.</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "      <td>110.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>505.0</td>\n",
       "      <td>11982.0</td>\n",
       "      <td>31992440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23625</th>\n",
       "      <td>154232</td>\n",
       "      <td>MAX TECHNOLOGY &amp; SUPPORT SERVICES PRIVATE LIMITED</td>\n",
       "      <td>IN</td>\n",
       "      <td>APNIC</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23626</th>\n",
       "      <td>204856</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Educational/Research</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23627</th>\n",
       "      <td>204917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23628</th>\n",
       "      <td>210796</td>\n",
       "      <td>Bjoern Schleyer</td>\n",
       "      <td>DE</td>\n",
       "      <td>RIPE</td>\n",
       "      <td>NSP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23629</th>\n",
       "      <td>400926</td>\n",
       "      <td>KIWI TELECOM</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>NSP</td>\n",
       "      <td>78320.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23630 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          asn                                           org_name country  \\\n",
       "0        4436                                  GTT Americas, LLC      US   \n",
       "1       20940                          Akamai International B.V.      NL   \n",
       "2       31800                                             DALnet      US   \n",
       "3        3303                              Swisscom (Schweiz) AG      CH   \n",
       "4       22773                            Cox Communications Inc.      US   \n",
       "...       ...                                                ...     ...   \n",
       "23625  154232  MAX TECHNOLOGY & SUPPORT SERVICES PRIVATE LIMITED      IN   \n",
       "23626  204856                                                NaN     NaN   \n",
       "23627  204917                                                NaN     NaN   \n",
       "23628  210796                                    Bjoern Schleyer      DE   \n",
       "23629  400926                                       KIWI TELECOM      US   \n",
       "\n",
       "      source             info_type     rank  asnDegree_total  \\\n",
       "0       ARIN                   NSP  78320.0              0.0   \n",
       "1       RIPE               Content   1894.0            485.0   \n",
       "2       ARIN            Non-Profit  47745.0             78.0   \n",
       "3       RIPE         Cable/DSL/ISP     81.0           1273.0   \n",
       "4       ARIN         Cable/DSL/ISP    110.0            499.0   \n",
       "...      ...                   ...      ...              ...   \n",
       "23625  APNIC         Cable/DSL/ISP      NaN              NaN   \n",
       "23626    NaN  Educational/Research      NaN              NaN   \n",
       "23627    NaN         Cable/DSL/ISP      NaN              NaN   \n",
       "23628   RIPE                   NSP      NaN              NaN   \n",
       "23629   ARIN                   NSP  78320.0              0.0   \n",
       "\n",
       "       asnDegree_customer  asnDegree_peer  asnDegree_provider  \\\n",
       "0                     0.0             0.0                 0.0   \n",
       "1                    14.0           366.0               105.0   \n",
       "2                     0.0            74.0                 4.0   \n",
       "3                   166.0          1101.0                 6.0   \n",
       "4                   489.0             8.0                 2.0   \n",
       "...                   ...             ...                 ...   \n",
       "23625                 NaN             NaN                 NaN   \n",
       "23626                 NaN             NaN                 NaN   \n",
       "23627                 NaN             NaN                 NaN   \n",
       "23628                 NaN             NaN                 NaN   \n",
       "23629                 0.0             0.0                 0.0   \n",
       "\n",
       "       cone_numberAsns  cone_numberPrefixes  cone_numberAddresses  \n",
       "0                  1.0                  0.0                   0.0  \n",
       "1                 15.0               8945.0            14612752.0  \n",
       "2                  1.0                  2.0                 512.0  \n",
       "3                733.0              22131.0            42899794.0  \n",
       "4                505.0              11982.0            31992440.0  \n",
       "...                ...                  ...                   ...  \n",
       "23625              NaN                  NaN                   NaN  \n",
       "23626              NaN                  NaN                   NaN  \n",
       "23627              NaN                  NaN                   NaN  \n",
       "23628              NaN                  NaN                   NaN  \n",
       "23629              1.0                  0.0                   0.0  \n",
       "\n",
       "[23630 rows x 13 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peering_df_joined_with_asrank = pd.merge(\n",
    "    peering_df_joined,\n",
    "    as_rank_df,\n",
    "    left_on='asn',\n",
    "    right_on='asn',\n",
    "    how='left'\n",
    ")\n",
    "peering_df_joined_with_asrank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfbbfeb",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f565817d",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fce41b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          asn                                           org_name country  \\\n",
      "0        4436                                  gtt americas, llc      US   \n",
      "1       20940                          akamai international b.v.      NL   \n",
      "2       31800                                             dalnet      US   \n",
      "3        3303                              swisscom (schweiz) ag      CH   \n",
      "4       22773                            cox communications inc.      US   \n",
      "...       ...                                                ...     ...   \n",
      "23625  154232  max technology & support services private limited      IN   \n",
      "23626  204856                                            unknown     NaN   \n",
      "23627  204917                                            unknown     NaN   \n",
      "23628  210796                                    bjoern schleyer      DE   \n",
      "23629  400926                                       kiwi telecom      US   \n",
      "\n",
      "      source             info_type  \n",
      "0       ARIN                   NSP  \n",
      "1       RIPE               Content  \n",
      "2       ARIN            Non-Profit  \n",
      "3       RIPE         Cable/DSL/ISP  \n",
      "4       ARIN         Cable/DSL/ISP  \n",
      "...      ...                   ...  \n",
      "23625  APNIC         Cable/DSL/ISP  \n",
      "23626    NaN  Educational/Research  \n",
      "23627    NaN         Cable/DSL/ISP  \n",
      "23628   RIPE                   NSP  \n",
      "23629   ARIN                   NSP  \n",
      "\n",
      "[23630 rows x 5 columns]\n",
      "Verwendete Klassen: ['Cable/DSL/ISP', 'NSP', 'Content', 'Enterprise', 'Educational/Research', 'Network Services', 'Route Server', 'Non-Profit', 'Government', 'Route Collector']\n",
      "DataFrame nach Filterung: 23630 Zeilen\n",
      "\n",
      "=== TF-IDF-Modell ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhart/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Accuracy: 0.5445963541666666\n",
      "TF-IDF Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       Cable/DSL/ISP       0.67      0.78      0.72      1532\n",
      "             Content       0.35      0.40      0.37       323\n",
      "Educational/Research       0.53      0.50      0.52       189\n",
      "          Enterprise       0.25      0.19      0.22       224\n",
      "          Government       0.36      0.25      0.30        16\n",
      "                 NSP       0.37      0.28      0.32       518\n",
      "    Network Services       0.13      0.06      0.08       105\n",
      "          Non-Profit       0.61      0.28      0.38        80\n",
      "     Route Collector       0.18      0.50      0.27         4\n",
      "        Route Server       0.46      0.48      0.47        81\n",
      "\n",
      "            accuracy                           0.54      3072\n",
      "           macro avg       0.39      0.37      0.36      3072\n",
      "        weighted avg       0.52      0.54      0.53      3072\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhart/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5771484375\n",
      "Macro-F1: 0.3068511259777624\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       Cable/DSL/ISP       0.60      0.93      0.73      1532\n",
      "             Content       0.48      0.28      0.35       323\n",
      "Educational/Research       0.65      0.38      0.48       189\n",
      "          Enterprise       0.35      0.10      0.15       224\n",
      "          Government       0.33      0.12      0.18        16\n",
      "                 NSP       0.46      0.20      0.27       518\n",
      "    Network Services       0.25      0.03      0.05       105\n",
      "          Non-Profit       0.63      0.21      0.32        80\n",
      "     Route Collector       0.00      0.00      0.00         4\n",
      "        Route Server       0.57      0.49      0.53        81\n",
      "\n",
      "            accuracy                           0.58      3072\n",
      "           macro avg       0.43      0.27      0.31      3072\n",
      "        weighted avg       0.53      0.58      0.51      3072\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhart/.venv/lib/python3.12/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jhart/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/jhart/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/jhart/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# DataFrame (dein echter Datensatz, hier Beispiel\n",
    "df = peering_df_joined\n",
    "# Preprocessing\n",
    "df['org_name'] = df['org_name'].fillna('Unknown').str.lower()\n",
    "\n",
    "# Filtere Klassen mit <2 Einträgen\n",
    "class_counts = df['info_type'].value_counts()\n",
    "valid_classes = class_counts[class_counts >= 2].index\n",
    "df = df[df['info_type'].isin(valid_classes)]\n",
    "print(df)\n",
    "print(f\"Verwendete Klassen: {valid_classes.tolist()}\")\n",
    "print(f\"DataFrame nach Filterung: {len(df)} Zeilen\")\n",
    "\n",
    "# --- TF-IDF-Modell ---\n",
    "print(\"\\n=== TF-IDF-Modell ===\")\n",
    "# TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), lowercase=True)\n",
    "X_tfidf = vectorizer.fit_transform(df['org_name'])\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, df['info_type'], test_size=0.13, random_state=42, stratify=df['info_type'])\n",
    "\n",
    "# Balancing\n",
    "smote = SMOTE(random_state=42)\n",
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_tfidf, y_train)\n",
    "X_train_bal, y_train_bal = rus.fit_resample(X_train_res, y_train_res)\n",
    "\n",
    "# Classifier\n",
    "classifier_tfidf = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs', class_weight='balanced')\n",
    "classifier_tfidf.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Evaluation\n",
    "y_pred_tfidf = classifier_tfidf.predict(X_test_tfidf)\n",
    "print(\"TF-IDF Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
    "print(\"TF-IDF Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_tfidf))\n",
    "\n",
    "svm = LinearSVC(C=1.0, class_weight=\"balanced\")\n",
    "clf_cal = CalibratedClassifierCV(svm, method=\"sigmoid\", cv=5)  # liefert predict_proba\n",
    "clf_cal.fit(X_train_tfidf, y_train)\n",
    "\n",
    "\n",
    "y_pred = clf_cal.predict(X_test_tfidf)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Macro-F1:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a706a136",
   "metadata": {},
   "source": [
    "## Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa48be66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhart/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU-Name: NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition\n",
      "          asn                                           org_name country  \\\n",
      "0        4436                                  gtt americas, llc      US   \n",
      "1       20940                          akamai international b.v.      NL   \n",
      "2       31800                                             dalnet      US   \n",
      "3        3303                              swisscom (schweiz) ag      CH   \n",
      "4       22773                            cox communications inc.      US   \n",
      "...       ...                                                ...     ...   \n",
      "23625  154232  max technology & support services private limited      IN   \n",
      "23626  204856                                            unknown     NaN   \n",
      "23627  204917                                            unknown     NaN   \n",
      "23628  210796                                    bjoern schleyer      DE   \n",
      "23629  400926                                       kiwi telecom      US   \n",
      "\n",
      "      source             info_type  \n",
      "0       ARIN                   NSP  \n",
      "1       RIPE               Content  \n",
      "2       ARIN            Non-Profit  \n",
      "3       RIPE         Cable/DSL/ISP  \n",
      "4       ARIN         Cable/DSL/ISP  \n",
      "...      ...                   ...  \n",
      "23625  APNIC         Cable/DSL/ISP  \n",
      "23626    NaN  Educational/Research  \n",
      "23627    NaN         Cable/DSL/ISP  \n",
      "23628   RIPE                   NSP  \n",
      "23629   ARIN                   NSP  \n",
      "\n",
      "[23630 rows x 5 columns]\n",
      "Verwendete Klassen: ['Cable/DSL/ISP', 'NSP', 'Content', 'Enterprise', 'Educational/Research', 'Network Services', 'Route Server', 'Non-Profit', 'Government', 'Route Collector']\n",
      "DataFrame nach Filterung: 23630 Zeilen\n",
      "Eindeutige org_name: 20832\n",
      "\n",
      "=== BERT-Modell ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BERT-Embeddings: 100%|██████████| 326/326 [00:04<00:00, 76.82it/s]\n",
      "/home/jhart/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Accuracy: 0.34551495016611294\n",
      "BERT Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       Cable/DSL/ISP       0.77      0.41      0.53      1418\n",
      "             Content       0.22      0.29      0.25       277\n",
      "Educational/Research       0.37      0.46      0.41       170\n",
      "          Enterprise       0.15      0.23      0.18       201\n",
      "          Government       0.40      0.50      0.44        16\n",
      "                 NSP       0.24      0.24      0.24       436\n",
      "    Network Services       0.03      0.12      0.05        91\n",
      "          Non-Profit       0.10      0.30      0.15        61\n",
      "     Route Collector       0.00      0.00      0.00         2\n",
      "        Route Server       0.10      0.35      0.16        37\n",
      "\n",
      "            accuracy                           0.35      2709\n",
      "           macro avg       0.24      0.29      0.24      2709\n",
      "        weighted avg       0.51      0.35      0.39      2709\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhart/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "# Initialize parallel_pandas\n",
    "\n",
    "# Prüfe GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU-Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Warnung: Keine GPU verfügbar, CPU wird verwendet.\")\n",
    "\n",
    "# DataFrame (dein echter Datensatz, hier Beispiel\n",
    "df = peering_df_joined\n",
    "# Preprocessing\n",
    "df['org_name'] = df['org_name'].fillna('Unknown').str.lower()\n",
    "\n",
    "# Filtere Klassen mit <2 Einträgen\n",
    "class_counts = df['info_type'].value_counts()\n",
    "valid_classes = class_counts[class_counts >= 2].index\n",
    "df = df[df['info_type'].isin(valid_classes)]\n",
    "print(df)\n",
    "print(f\"Verwendete Klassen: {valid_classes.tolist()}\")\n",
    "print(f\"DataFrame nach Filterung: {len(df)} Zeilen\")\n",
    "\n",
    "# Deduplizierung\n",
    "unique_df = df.drop_duplicates(subset=['org_name'])\n",
    "print(f\"Eindeutige org_name: {len(unique_df)}\")\n",
    "\n",
    "\n",
    "# --- BERT-Modell ---\n",
    "print(\"\\n=== BERT-Modell ===\")\n",
    "# BERT-Tokenizer und Model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "model.eval()\n",
    "\n",
    "# BERT Embeddings\n",
    "def get_bert_embedding(text, batch_size=64):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(text), batch_size), desc=\"BERT-Embeddings\"):\n",
    "        batch = text[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            embeddings.append(cls_embeddings.cpu().numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Training\n",
    "X_bert = get_bert_embedding(unique_df['org_name'].tolist())\n",
    "y = unique_df['info_type']\n",
    "X_train_bert, X_test_bert, y_train, y_test = train_test_split(X_bert, y, test_size=0.13, random_state=42, stratify=y)\n",
    "\n",
    "# Balancing\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_bert, y_train)\n",
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_train_bal, y_train_bal = rus.fit_resample(X_train_res, y_train_res)\n",
    "\n",
    "# Classifier\n",
    "classifier_bert = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs', class_weight='balanced')\n",
    "classifier_bert.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Evaluation\n",
    "y_pred_bert = classifier_bert.predict(X_test_bert)\n",
    "print(\"BERT Accuracy:\", accuracy_score(y_test, y_pred_bert))\n",
    "print(\"BERT Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd11dd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: 10 Klassen -> ['Cable/DSL/ISP', 'Content', 'Educational/Research', 'Enterprise', 'Government', 'NSP', 'Network Services', 'Non-Profit', 'Route Collector', 'Route Server']\n",
      "Device: cuda\n",
      "GPU-Name: NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_254258/91491946.py:163: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [0.018 0.086 0.147 0.125 1.697 0.054 0.267 0.35  6.912 0.344]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2580' max='4300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2580/4300 05:27 < 03:38, 7.87 it/s, Epoch 12/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.273400</td>\n",
       "      <td>2.156799</td>\n",
       "      <td>0.150065</td>\n",
       "      <td>0.137683</td>\n",
       "      <td>0.211061</td>\n",
       "      <td>0.246358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.997800</td>\n",
       "      <td>1.868589</td>\n",
       "      <td>0.449544</td>\n",
       "      <td>0.242383</td>\n",
       "      <td>0.252025</td>\n",
       "      <td>0.325933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.814600</td>\n",
       "      <td>1.825576</td>\n",
       "      <td>0.394857</td>\n",
       "      <td>0.268147</td>\n",
       "      <td>0.255242</td>\n",
       "      <td>0.366774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.733000</td>\n",
       "      <td>1.812551</td>\n",
       "      <td>0.470052</td>\n",
       "      <td>0.296443</td>\n",
       "      <td>0.274726</td>\n",
       "      <td>0.374749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.621200</td>\n",
       "      <td>1.820322</td>\n",
       "      <td>0.481771</td>\n",
       "      <td>0.311110</td>\n",
       "      <td>0.306926</td>\n",
       "      <td>0.368980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.556600</td>\n",
       "      <td>1.829663</td>\n",
       "      <td>0.511719</td>\n",
       "      <td>0.322918</td>\n",
       "      <td>0.298778</td>\n",
       "      <td>0.370429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.595300</td>\n",
       "      <td>1.825398</td>\n",
       "      <td>0.503581</td>\n",
       "      <td>0.322288</td>\n",
       "      <td>0.311144</td>\n",
       "      <td>0.370295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.506700</td>\n",
       "      <td>1.832770</td>\n",
       "      <td>0.497396</td>\n",
       "      <td>0.343228</td>\n",
       "      <td>0.318494</td>\n",
       "      <td>0.399282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.454300</td>\n",
       "      <td>1.746025</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>0.349097</td>\n",
       "      <td>0.320165</td>\n",
       "      <td>0.421186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.420900</td>\n",
       "      <td>1.827954</td>\n",
       "      <td>0.482422</td>\n",
       "      <td>0.353087</td>\n",
       "      <td>0.334497</td>\n",
       "      <td>0.400125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.314900</td>\n",
       "      <td>1.789773</td>\n",
       "      <td>0.486328</td>\n",
       "      <td>0.343323</td>\n",
       "      <td>0.321808</td>\n",
       "      <td>0.422446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.334800</td>\n",
       "      <td>1.836919</td>\n",
       "      <td>0.473958</td>\n",
       "      <td>0.344894</td>\n",
       "      <td>0.321913</td>\n",
       "      <td>0.409083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval: {'eval_loss': 1.8279536962509155, 'eval_accuracy': 0.482421875, 'eval_f1_macro': 0.3530874014181581, 'eval_precision': 0.3344966477967031, 'eval_recall': 0.4001249720033334, 'eval_runtime': 2.0125, 'eval_samples_per_second': 1526.464, 'eval_steps_per_second': 15.901, 'epoch': 12.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('xlmr_org_trainer_out/model/tokenizer_config.json',\n",
       " 'xlmr_org_trainer_out/model/special_tokens_map.json',\n",
       " 'xlmr_org_trainer_out/model/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Ersatz für den HF-Datasets-Teil (kein pyarrow/datasets nötig) ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding, TrainingArguments, Trainer,\n",
    "                          EarlyStoppingCallback, TextClassificationPipeline)\n",
    "\n",
    "# --------- Konfig ---------\n",
    "MODEL_NAME   = \"xlm-roberta-base\"   # multilingual, starkes Baseline-Modell\n",
    "MAX_LENGTH   = 64                   # Org-Namen sind kurz -> 64 reicht\n",
    "LR           = 1e-5\n",
    "EPOCHS       = 20\n",
    "BATCH_SIZE   = 32\n",
    "WARMUP_RATIO = 0.06\n",
    "SEED         = 42\n",
    "OUT_DIR      = \"xlmr_org_trainer_out\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "le = LabelEncoder()\n",
    "\n",
    "le = LabelEncoder()\n",
    "df = peering_df_joined\n",
    "df[\"label_id\"] = le.fit_transform(df[\"info_type\"])\n",
    "num_labels = len(le.classes_)\n",
    "print(f\"Labels: {num_labels} Klassen ->\", list(le.classes_))\n",
    "\n",
    "df[\"label_id\"] = le.fit_transform(df[\"info_type\"])\n",
    "\n",
    "\n",
    "df.fillna('Unknown', inplace=True)\n",
    "\n",
    "# Prüfe GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU-Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Warnung: Keine GPU verfügbar, CPU wird verwendet.\")\n",
    "\n",
    "# Train/Validation Split (stratifiziert)\n",
    "train_df, eval_df = train_test_split(\n",
    "    df[[\"org_name\", \"label_id\"]],\n",
    "    test_size=0.13,\n",
    "    random_state=SEED,\n",
    "    stratify=df[\"label_id\"]\n",
    ")\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "eval_df  = eval_df.reset_index(drop=True)\n",
    "\n",
    "# Texte & Labels aus den bereits vorbereiteten DataFrames (train_df, eval_df)\n",
    "train_texts = train_df[\"org_name\"].tolist()\n",
    "eval_texts  = eval_df[\"org_name\"].tolist()\n",
    "y_train_np  = train_df[\"label_id\"].to_numpy()\n",
    "y_eval_np   = eval_df[\"label_id\"].to_numpy()\n",
    "num_labels  = df[\"label_id\"].nunique()\n",
    "\n",
    "\n",
    "\n",
    "# Tokenisierung OHNE Padding (Padding macht später der DataCollator)\n",
    "train_enc = tok(train_texts, truncation=True, max_length=MAX_LENGTH)\n",
    "eval_enc  = tok(eval_texts,  truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "class SimpleHFLikeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.enc = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(int(self.labels[idx]))\n",
    "        return item\n",
    "\n",
    "ds_train = SimpleHFLikeDataset(train_enc, y_train_np)\n",
    "ds_eval  = SimpleHFLikeDataset(eval_enc,  y_eval_np)\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tok)\n",
    "\n",
    "valid_classes = sorted(df[\"info_type\"].unique())\n",
    "\n",
    "# ---- Modell + Class Weights wie gehabt ----\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label={int(i): c for i, c in enumerate(valid_classes)},\n",
    "    label2id={c: int(i) for i, c in enumerate(valid_classes)}\n",
    ").to(device)\n",
    "\n",
    "# Class-Weights aus dem Trainingssplit\n",
    "class_counts = np.bincount(y_train_np, minlength=num_labels)\n",
    "weights = class_counts.sum() / np.maximum(class_counts, 1)\n",
    "weights = weights / weights.mean()\n",
    "class_weights = torch.tensor(weights, dtype=torch.float, device=device)\n",
    "print(\"Class weights:\", np.round(weights, 3))\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):  # <- **kwargs!\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n",
    "        # ModelOutput kann dict-ähnlich oder Attribute sein:\n",
    "        logits = outputs[\"logits\"] if isinstance(outputs, dict) else outputs.logits\n",
    "\n",
    "        # class_weights sicher auf dasselbe Device wie logits legen:\n",
    "        cw = class_weights.to(logits.device)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=cw)\n",
    "        loss = loss_fct(logits.view(-1, num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    def compute_metrics(eval_pred):\n",
    "    # eval_pred ist i.d.R. EvalPrediction\n",
    "        try:\n",
    "            preds = eval_pred.predictions\n",
    "            labels = eval_pred.label_ids\n",
    "        except Exception:\n",
    "            preds, labels = eval_pred  # Fallback\n",
    "\n",
    "        # Bei manchen Modellen kommt predictions als Tuple (logits, ...)\n",
    "        if isinstance(preds, (tuple, list)):\n",
    "            preds = preds[0]\n",
    "\n",
    "        y_pred = np.argmax(preds, axis=-1)\n",
    "        return {\n",
    "            \"accuracy\":  float(accuracy_score(labels, y_pred)),\n",
    "            \"f1_macro\":  float(f1_score(labels, y_pred, average=\"macro\")),\n",
    "            \"precision\": float(precision_score(labels, y_pred, average=\"macro\", zero_division=0)),\n",
    "            \"recall\":    float(recall_score(labels, y_pred, average=\"macro\")),\n",
    "        }\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUT_DIR + \"/checkpoints\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LR,\n",
    "    eval_strategy=\"epoch\",     # <- statt eval_strategy\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    fp16=(device.type==\"cuda\"),\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    seed=SEED,\n",
    "    report_to=[\"none\"],\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "    return {\n",
    "        \"accuracy\":  float(accuracy_score(labels, preds)),\n",
    "        \"f1_macro\":  float(f1_score(labels, preds, average=\"macro\")),\n",
    "        \"precision\": float(precision_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"recall\":    float(recall_score(labels, preds, average=\"macro\")),\n",
    "    }\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_eval,\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Eval:\", metrics)\n",
    "\n",
    "trainer.save_model(OUT_DIR + \"/model\")\n",
    "tok.save_pretrained(OUT_DIR + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1c33e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biggest length of org_name is: 203\n"
     ]
    }
   ],
   "source": [
    "# Calculate the maximum length of org_name strings in peering_df_joined, ignoring NaN values\n",
    "max_org_name_length = joined_df['org_name'].dropna().str.len().max()\n",
    "print(f\"The biggest length of org_name is: {max_org_name_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97f335e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Class weights: [0.018 0.086 0.147 0.125 1.697 0.054 0.267 0.35  6.912 0.344]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/tmp/ipykernel_18848/1218507509.py:105: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  feats = torch.tensor([b[\"features\"] for b in batch], dtype=torch.float)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7716' max='16075' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 7716/16075 34:18 < 37:10, 3.75 it/s, Epoch 12/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.165100</td>\n",
       "      <td>3.971931</td>\n",
       "      <td>0.032552</td>\n",
       "      <td>0.084598</td>\n",
       "      <td>0.090505</td>\n",
       "      <td>0.213724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.053700</td>\n",
       "      <td>3.797562</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.171292</td>\n",
       "      <td>0.236636</td>\n",
       "      <td>0.251107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.831100</td>\n",
       "      <td>3.804070</td>\n",
       "      <td>0.058268</td>\n",
       "      <td>0.181972</td>\n",
       "      <td>0.205395</td>\n",
       "      <td>0.279771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.684700</td>\n",
       "      <td>3.792244</td>\n",
       "      <td>0.043294</td>\n",
       "      <td>0.175030</td>\n",
       "      <td>0.228536</td>\n",
       "      <td>0.225101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.700400</td>\n",
       "      <td>3.778271</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.199512</td>\n",
       "      <td>0.419165</td>\n",
       "      <td>0.257946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.448400</td>\n",
       "      <td>3.805672</td>\n",
       "      <td>0.063802</td>\n",
       "      <td>0.196939</td>\n",
       "      <td>0.387277</td>\n",
       "      <td>0.259332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.680100</td>\n",
       "      <td>3.821486</td>\n",
       "      <td>0.073568</td>\n",
       "      <td>0.208435</td>\n",
       "      <td>0.314729</td>\n",
       "      <td>0.260500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.708400</td>\n",
       "      <td>3.815170</td>\n",
       "      <td>0.065104</td>\n",
       "      <td>0.201670</td>\n",
       "      <td>0.315783</td>\n",
       "      <td>0.240665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.532800</td>\n",
       "      <td>3.806083</td>\n",
       "      <td>0.067057</td>\n",
       "      <td>0.209493</td>\n",
       "      <td>0.328686</td>\n",
       "      <td>0.259360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.254700</td>\n",
       "      <td>3.857722</td>\n",
       "      <td>0.076823</td>\n",
       "      <td>0.226401</td>\n",
       "      <td>0.308085</td>\n",
       "      <td>0.278495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.387100</td>\n",
       "      <td>3.856137</td>\n",
       "      <td>0.077474</td>\n",
       "      <td>0.208983</td>\n",
       "      <td>0.288652</td>\n",
       "      <td>0.273469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.545700</td>\n",
       "      <td>3.862251</td>\n",
       "      <td>0.076497</td>\n",
       "      <td>0.219715</td>\n",
       "      <td>0.296945</td>\n",
       "      <td>0.270848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval: {'eval_loss': 3.8577215671539307, 'eval_accuracy': 0.07682291666666667, 'eval_f1_macro': 0.22640100638590477, 'eval_precision': 0.3080854194667569, 'eval_recall': 0.2784948584697037, 'eval_runtime': 1.1953, 'eval_samples_per_second': 2570.038, 'eval_steps_per_second': 80.314, 'epoch': 12.0}\n",
      "Aux artefacts saved to xlmr_org_trainer_out_mixed/model/aux.pkl\n"
     ]
    }
   ],
   "source": [
    "# ==== Text + numerische AS-Rank-Features in einem HF-Trainer ====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# --------- Konfig ---------\n",
    "MODEL_NAME   = \"xlm-roberta-base\"\n",
    "MAX_LENGTH   = 64           # 64 reicht für Org-Namen\n",
    "LR           = 2e-5\n",
    "EPOCHS       = 25\n",
    "BATCH_SIZE   = 32\n",
    "WARMUP_RATIO = 0.06\n",
    "SEED         = 100\n",
    "OUT_DIR      = \"xlmr_org_trainer_out_mixed\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# --------- Daten ---------\n",
    "df = peering_df_joined_with_asrank.copy()\n",
    "\n",
    "# Label-Encode\n",
    "le = LabelEncoder()\n",
    "df[\"label_id\"] = le.fit_transform(df[\"info_type\"].astype(str))\n",
    "num_labels = len(le.classes_)\n",
    "id2label = {i: c for i,c in enumerate(le.classes_)}\n",
    "label2id = {c: i for i,c in enumerate(le.classes_)}\n",
    "\n",
    "# Text\n",
    "df[\"org_name\"] = df[\"org_name\"].fillna(\"Unknown\").astype(str).str.strip()\n",
    "\n",
    "# Numerische Featureliste (deine Spalten)\n",
    "FEAT_COLS = [\n",
    "    \"rank\",\n",
    "    \"asnDegree_total\", \"asnDegree_customer\", \"asnDegree_peer\", \"asnDegree_provider\",\n",
    "    \"cone_numberAsns\", \"cone_numberPrefixes\", \"cone_numberAddresses\",\n",
    "]\n",
    "num_feats = scaler.fit_transform(df[FEAT_COLS].fillna(0))\n",
    "# NaNs -> 0, sinnvolle Skalen (log1p für stark schiefe Größen)\n",
    "num_df = df[FEAT_COLS].copy()\n",
    "for c in FEAT_COLS:\n",
    "    if c in (\"cone_numberPrefixes\",\"cone_numberAddresses\",\"cone_numberAsns\"):\n",
    "        num_df[c] = np.log1p(pd.to_numeric(num_df[c], errors=\"coerce\").fillna(0))\n",
    "    else:\n",
    "        num_df[c] = pd.to_numeric(num_df[c], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# Split\n",
    "train_idx, eval_idx = train_test_split(\n",
    "    np.arange(len(df)),\n",
    "    test_size=0.13,\n",
    "    random_state=SEED,\n",
    "    stratify=df[\"label_id\"]\n",
    ")\n",
    "train_texts = df.loc[train_idx, \"org_name\"].tolist()\n",
    "eval_texts  = df.loc[eval_idx,  \"org_name\"].tolist()\n",
    "y_train_np  = df.loc[train_idx, \"label_id\"].to_numpy()\n",
    "y_eval_np   = df.loc[eval_idx,  \"label_id\"].to_numpy()\n",
    "\n",
    "Xnum_train = num_df.loc[train_idx].to_numpy(dtype=np.float32)\n",
    "Xnum_eval  = num_df.loc[eval_idx].to_numpy(dtype=np.float32)\n",
    "\n",
    "# Standardisieren (nur auf Train fitten!)\n",
    "scaler = StandardScaler()\n",
    "Xnum_train = scaler.fit_transform(Xnum_train)\n",
    "Xnum_eval  = scaler.transform(Xnum_eval)\n",
    "\n",
    "# Tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Tokenisierung (ohne Padding – das macht der Collator)\n",
    "enc_train = tok(train_texts, truncation=True, max_length=MAX_LENGTH)\n",
    "enc_eval  = tok(eval_texts,  truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "# Dataset: gibt Listen (keine Tensors) zurück -> Collator kümmert sich ums Padding/Stacking\n",
    "class TextNumDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, num_feats, labels):\n",
    "        self.enc = encodings\n",
    "        self.num = num_feats\n",
    "        self.y   = labels\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: self.enc[k][idx] for k in self.enc}  # ids/attn as plain lists\n",
    "        item[\"features\"] = self.num[idx]\n",
    "        item[\"labels\"]   = int(self.y[idx])\n",
    "        return item\n",
    "\n",
    "ds_train = TextNumDataset(enc_train, Xnum_train, y_train_np)\n",
    "ds_eval  = TextNumDataset(enc_eval,  Xnum_eval,  y_eval_np)\n",
    "\n",
    "# Custom Collator: pad Text + stapel numerische Features\n",
    "class MixedCollator:\n",
    "    def __init__(self, tokenizer): self.tok = tokenizer\n",
    "    def __call__(self, batch):\n",
    "        text = {k: [b[k] for b in batch] for k in [\"input_ids\",\"attention_mask\"]}\n",
    "        text = self.tok.pad(text, return_tensors=\"pt\")\n",
    "        feats = torch.tensor([b[\"features\"] for b in batch], dtype=torch.float)\n",
    "        labels = torch.tensor([b[\"labels\"] for b in batch], dtype=torch.long)\n",
    "        text[\"features\"] = feats\n",
    "        text[\"labels\"]   = labels\n",
    "        return text\n",
    "\n",
    "collator = MixedCollator(tok)\n",
    "\n",
    "# Class-Weights (aus Train)\n",
    "class_counts = np.bincount(y_train_np, minlength=num_labels)\n",
    "w = class_counts.sum() / np.maximum(class_counts, 1)\n",
    "w = w / w.mean()\n",
    "class_weights = torch.tensor(w, dtype=torch.float, device=device)\n",
    "print(\"Class weights:\", np.round(w, 3))\n",
    "\n",
    "# Modell: XLM-R Encoder + mean pooling + numerische Features -> MLP-Classifier\n",
    "class TextPlusNumClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, num_num_feats, dropout=0.2, use_attn_pool=False):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        hidden = self.backbone.config.hidden_size  # z.B. 768\n",
    "\n",
    "        # 🔸 numerische Features erst in einen eigenen Raum projizieren\n",
    "        self.num_proj = nn.Sequential(\n",
    "            nn.Linear(num_num_feats, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(128),\n",
    "        )\n",
    "\n",
    "        # (optional) Attention-Pooling statt Mean-Pooling\n",
    "        self.use_attn_pool = use_attn_pool\n",
    "        if self.use_attn_pool:\n",
    "            self.attn = nn.Linear(hidden, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden + 128, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, num_labels),\n",
    "        )\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def _pool(self, last_hidden, attn_mask):\n",
    "        if not self.use_attn_pool:\n",
    "            # Mean-Pooling\n",
    "            mask = attn_mask.unsqueeze(-1)                   # [B,T,1]\n",
    "            return (last_hidden * mask).sum(1) / mask.sum(1).clamp(min=1e-9)\n",
    "        # Attention-Pooling\n",
    "        scores = self.attn(last_hidden).squeeze(-1)          # [B,T]\n",
    "        scores = scores.masked_fill(attn_mask == 0, -1e9)\n",
    "        weights = scores.softmax(dim=-1).unsqueeze(-1)       # [B,T,1]\n",
    "        return (last_hidden * weights).sum(1)                # [B,H]\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, features=None, labels=None):\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = self._pool(out.last_hidden_state, attention_mask)      # [B,H]\n",
    "\n",
    "        # 🔸 numerische Features projizieren & mit Text-Embedding konkatenieren\n",
    "        num_emb = self.num_proj(features)                                # [B,128]\n",
    "        z = torch.cat([pooled, num_emb], dim=1)                          # [B,H+128]\n",
    "\n",
    "        logits = self.classifier(self.dropout(z))\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Label smoothing hilft oft bei Imbalance leicht\n",
    "            loss = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)(logits, labels)\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "model = TextPlusNumClassifier(MODEL_NAME, num_labels=num_labels, num_num_feats=Xnum_train.shape[1]).to(device)\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\":  float(accuracy_score(labels, preds)),\n",
    "        \"f1_macro\":  float(f1_score(labels, preds, average=\"macro\")),\n",
    "        \"precision\": float(precision_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"recall\":    float(recall_score(labels, preds, average=\"macro\")),\n",
    "    }\n",
    "\n",
    "# TrainingArguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUT_DIR + \"/checkpoints\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=(device.type==\"cuda\"),\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    seed=SEED,\n",
    "    report_to=[\"none\"],\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_eval,\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Eval:\", trainer.evaluate())\n",
    "\n",
    "# Modelle speichern (Backbone+Head + LabelEncoder & Scaler separat speichern, falls gewünscht)\n",
    "trainer.save_model(OUT_DIR + \"/model\")\n",
    "tok.save_pretrained(OUT_DIR + \"/model\")\n",
    "\n",
    "# Tipp: Speichere auch den StandardScaler, damit du bei Inferenz die numerischen Features identisch transformierst\n",
    "import joblib, os\n",
    "os.makedirs(OUT_DIR + \"/model\", exist_ok=True)\n",
    "joblib.dump({\"scaler\": scaler, \"feat_cols\": FEAT_COLS, \"label_encoder\": le}, OUT_DIR + \"/model/aux.pkl\")\n",
    "print(\"Aux artefacts saved to\", OUT_DIR + \"/model/aux.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a3de1e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "as_rank_df.to_csv('as_rank_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d5563b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "as_rank_df = pd.read_csv('as_rank_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
