{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a33344",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "## Peeringdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1c32292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>org_id</th>\n",
       "      <th>name</th>\n",
       "      <th>aka</th>\n",
       "      <th>name_long</th>\n",
       "      <th>website</th>\n",
       "      <th>social_media</th>\n",
       "      <th>asn</th>\n",
       "      <th>looking_glass</th>\n",
       "      <th>route_server</th>\n",
       "      <th>...</th>\n",
       "      <th>policy_ratio</th>\n",
       "      <th>policy_contracts</th>\n",
       "      <th>allow_ixp_update</th>\n",
       "      <th>status_dashboard</th>\n",
       "      <th>rir_status</th>\n",
       "      <th>rir_status_updated</th>\n",
       "      <th>logo</th>\n",
       "      <th>created</th>\n",
       "      <th>updated</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8897</td>\n",
       "      <td>GTT Communications (AS4436)</td>\n",
       "      <td>Formerly known as nLayer Communications</td>\n",
       "      <td></td>\n",
       "      <td>http://www.gtt.net</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'http://...</td>\n",
       "      <td>4436</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>Required</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2022-07-27T05:33:22Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>Akamai Technologies</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://www.akamai.com/</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'https:/...</td>\n",
       "      <td>20940</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Not Required</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.akamaistatus.com/</td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2025-10-20T12:16:12Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>DALnet IRC Network</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>http://www.dal.net</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'http://...</td>\n",
       "      <td>31800</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Not Required</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2025-01-09T13:42:07Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>9350</td>\n",
       "      <td>Swisscom</td>\n",
       "      <td>IP-Plus</td>\n",
       "      <td></td>\n",
       "      <td>http://www.swisscom.com</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'http://...</td>\n",
       "      <td>3303</td>\n",
       "      <td></td>\n",
       "      <td>telnet://route-server.ip-plus.net</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>Required</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2025-08-12T06:33:30Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>Cox Communications</td>\n",
       "      <td>Cox Communications</td>\n",
       "      <td></td>\n",
       "      <td>http://www.cox.com/peering</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'http://...</td>\n",
       "      <td>22773</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Required</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2022-11-28T22:55:17Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  org_id                         name  \\\n",
       "0   1    8897  GTT Communications (AS4436)   \n",
       "1   2      14          Akamai Technologies   \n",
       "2   3      17           DALnet IRC Network   \n",
       "3   5    9350                     Swisscom   \n",
       "4   6      23           Cox Communications   \n",
       "\n",
       "                                       aka name_long  \\\n",
       "0  Formerly known as nLayer Communications             \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                  IP-Plus             \n",
       "4                       Cox Communications             \n",
       "\n",
       "                      website  \\\n",
       "0          http://www.gtt.net   \n",
       "1     https://www.akamai.com/   \n",
       "2          http://www.dal.net   \n",
       "3     http://www.swisscom.com   \n",
       "4  http://www.cox.com/peering   \n",
       "\n",
       "                                        social_media    asn looking_glass  \\\n",
       "0  [{'service': 'website', 'identifier': 'http://...   4436                 \n",
       "1  [{'service': 'website', 'identifier': 'https:/...  20940                 \n",
       "2  [{'service': 'website', 'identifier': 'http://...  31800                 \n",
       "3  [{'service': 'website', 'identifier': 'http://...   3303                 \n",
       "4  [{'service': 'website', 'identifier': 'http://...  22773                 \n",
       "\n",
       "                        route_server  ... policy_ratio policy_contracts  \\\n",
       "0                                     ...         True         Required   \n",
       "1                                     ...        False     Not Required   \n",
       "2                                     ...        False     Not Required   \n",
       "3  telnet://route-server.ip-plus.net  ...         True         Required   \n",
       "4                                     ...        False         Required   \n",
       "\n",
       "  allow_ixp_update               status_dashboard  rir_status  \\\n",
       "0            False                           None          ok   \n",
       "1            False  https://www.akamaistatus.com/          ok   \n",
       "2            False                                         ok   \n",
       "3            False                                         ok   \n",
       "4            False                                         ok   \n",
       "\n",
       "     rir_status_updated  logo               created               updated  \\\n",
       "0  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2022-07-27T05:33:22Z   \n",
       "1  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2025-10-20T12:16:12Z   \n",
       "2  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2025-01-09T13:42:07Z   \n",
       "3  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2025-08-12T06:33:30Z   \n",
       "4  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2022-11-28T22:55:17Z   \n",
       "\n",
       "   status  \n",
       "0      ok  \n",
       "1      ok  \n",
       "2      ok  \n",
       "3      ok  \n",
       "4      ok  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "filepath = Path('peeringdb/peeringdb_2_dump_2025_10_21.json')\n",
    "\n",
    "with filepath.open('r', encoding='utf-8') as f:\n",
    "    dump = json.load(f)\n",
    "\n",
    "# extract the net.data section and load into a DataFrame\n",
    "net_data = dump.get('net', {}).get('data')\n",
    "if net_data is None:\n",
    "    raise KeyError(\"JSON does not contain 'net' -> 'data' structure\")\n",
    "\n",
    "net_df = pd.DataFrame(net_data)\n",
    "net_df['asn'] = net_df['asn'].astype(int)\n",
    "net_df = net_df[net_df['info_type'] != '']\n",
    "\n",
    "# show a quick preview\n",
    "net_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2f20d5",
   "metadata": {},
   "source": [
    "# Caida AS Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c190e0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aut</th>\n",
       "      <th>changed</th>\n",
       "      <th>org_id</th>\n",
       "      <th>source</th>\n",
       "      <th>org_name</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20240618.0</td>\n",
       "      <td>LPL-141-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Level 3 Parent, LLC</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20231108.0</td>\n",
       "      <td>UNIVER-19-Z-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>University of Delaware</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>20100927.0</td>\n",
       "      <td>MIT-2-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Massachusetts Institute of Technology</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>20230929.0</td>\n",
       "      <td>USC-32-Z-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>University of Southern California</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>20200723.0</td>\n",
       "      <td>WGL-117-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>WFA Group LLC</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   aut     changed            org_id source  \\\n",
       "0    1  20240618.0      LPL-141-ARIN   ARIN   \n",
       "1    2  20231108.0  UNIVER-19-Z-ARIN   ARIN   \n",
       "2    3  20100927.0        MIT-2-ARIN   ARIN   \n",
       "3    4  20230929.0     USC-32-Z-ARIN   ARIN   \n",
       "4    5  20200723.0      WGL-117-ARIN   ARIN   \n",
       "\n",
       "                                org_name country  \n",
       "0                    Level 3 Parent, LLC      US  \n",
       "1                 University of Delaware      US  \n",
       "2  Massachusetts Institute of Technology      US  \n",
       "3      University of Southern California      US  \n",
       "4                          WFA Group LLC      US  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "with open('/workspaces/pytorch-gpu-2/preprocessing/data/caida/20251001.as-org2info.txt', 'r', newline='', encoding='utf-8') as input_file:\n",
    "    lines = input_file.readlines()   \n",
    "    # Buffers initialisieren\n",
    "    aut_lines = []\n",
    "    org_lines = []\n",
    "    mode = None\n",
    "    total_lines = len(lines)\n",
    "    aut_count = 0\n",
    "    org_count = 0 \n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"# format:aut\"):\n",
    "            mode = \"aut\"\n",
    "            continue\n",
    "        elif line.startswith(\"# format:org_id\"):\n",
    "            mode = \"org\"\n",
    "            continue\n",
    "        elif line.startswith(\"#\") or not line:\n",
    "            # Andere Kommentar- oder Leerzeilen überspringen\n",
    "            continue      \n",
    "        if mode == \"aut\":\n",
    "            aut_lines.append(line)\n",
    "            aut_count += 1\n",
    "        elif mode == \"org\":\n",
    "            org_lines.append(line)\n",
    "            org_count += 1\n",
    "    # StringIO-Objekte aus den gesammelten Zeilen bauen\n",
    "    aut_buffer = io.StringIO(\"\\n\".join(aut_lines))\n",
    "    org_buffer = io.StringIO(\"\\n\".join(org_lines))\n",
    "    # DataFrames einlesen\n",
    "    aut_df = pd.read_csv(aut_buffer, sep=\"|\",\n",
    "                        names=[\"aut\", \"changed\", \"aut_name\", \"org_id\", \"opaque_id\", \"source\"], usecols=[\"aut\", \"org_id\", \"source\", \"changed\"])\n",
    "    org_df = pd.read_csv(org_buffer, sep=\"|\",\n",
    "                        names=[\"org_id\", \"changed\", \"org_name\", \"country\", \"source\"], usecols=[\"org_id\", \"org_name\", \"country\"])\n",
    "\n",
    "    # Join the DataFrames\n",
    "    joined_df = pd.merge(aut_df, org_df, on=\"org_id\", how=\"left\")\n",
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73e5047",
   "metadata": {},
   "source": [
    "## Join both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6de8ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asn</th>\n",
       "      <th>org_name</th>\n",
       "      <th>country</th>\n",
       "      <th>source</th>\n",
       "      <th>info_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4436</td>\n",
       "      <td>GTT Americas, LLC</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>NSP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20940</td>\n",
       "      <td>Akamai International B.V.</td>\n",
       "      <td>NL</td>\n",
       "      <td>RIPE</td>\n",
       "      <td>Content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31800</td>\n",
       "      <td>DALnet</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Non-Profit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3303</td>\n",
       "      <td>Swisscom (Schweiz) AG</td>\n",
       "      <td>CH</td>\n",
       "      <td>RIPE</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22773</td>\n",
       "      <td>Cox Communications Inc.</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     asn                   org_name country source      info_type\n",
       "0   4436          GTT Americas, LLC      US   ARIN            NSP\n",
       "1  20940  Akamai International B.V.      NL   RIPE        Content\n",
       "2  31800                     DALnet      US   ARIN     Non-Profit\n",
       "3   3303      Swisscom (Schweiz) AG      CH   RIPE  Cable/DSL/ISP\n",
       "4  22773    Cox Communications Inc.      US   ARIN  Cable/DSL/ISP"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peering_df_joined = pd.merge(net_df, joined_df, left_on='asn', right_on='aut', how='left')\n",
    "peering_df_joined = peering_df_joined[['asn', 'org_name', 'country', 'source', 'info_type']]\n",
    "peering_df_joined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cf69bc",
   "metadata": {},
   "source": [
    "## Caida AS Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd17ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching AS Rank data: first=5000, offset=0\n",
      "Fetching AS Rank data: first=5000, offset=5000\n",
      "Fetching AS Rank data: first=5000, offset=10000\n",
      "Fetching AS Rank data: first=5000, offset=15000\n",
      "Fetching AS Rank data: first=5000, offset=20000\n",
      "Fetching AS Rank data: first=5000, offset=25000\n",
      "Fetching AS Rank data: first=5000, offset=30000\n",
      "Fetching AS Rank data: first=5000, offset=35000\n",
      "Fetching AS Rank data: first=5000, offset=40000\n",
      "Fetching AS Rank data: first=5000, offset=45000\n",
      "Fetching AS Rank data: first=5000, offset=50000\n",
      "Fetching AS Rank data: first=5000, offset=55000\n",
      "Fetching AS Rank data: first=5000, offset=60000\n",
      "Fetching AS Rank data: first=5000, offset=65000\n",
      "Fetching AS Rank data: first=5000, offset=70000\n",
      "Fetching AS Rank data: first=5000, offset=75000\n",
      "Fetching AS Rank data: first=5000, offset=80000\n",
      "Fetching AS Rank data: first=5000, offset=85000\n",
      "Fetching AS Rank data: first=5000, offset=90000\n",
      "Fetching AS Rank data: first=5000, offset=95000\n",
      "Fetching AS Rank data: first=5000, offset=100000\n",
      "Fetching AS Rank data: first=5000, offset=105000\n",
      "Fetching AS Rank data: first=5000, offset=110000\n",
      "Fetching AS Rank data: first=5000, offset=115000\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'asnDegree'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'asnDegree'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m as_rank_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(nodes)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 1️⃣ asnDegree (dict) in eigene Spalten auflösen\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m asnDegree_df \u001b[38;5;241m=\u001b[39m \u001b[43mas_rank_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43masnDegree\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(pd\u001b[38;5;241m.\u001b[39mSeries)\n\u001b[1;32m     24\u001b[0m asnDegree_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masnDegree_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m asnDegree_df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 2️⃣ wieder an den Haupt-DataFrame anhängen\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3805\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3807\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'asnDegree'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "next_page = True\n",
    "nodes = []\n",
    "first=5000\n",
    "offset=0\n",
    "while(next_page):\n",
    "    print(f\"Fetching AS Rank data: first={first}, offset={offset}\")\n",
    "    response = requests.get(f\"https://api.asrank.caida.org/v2/restful/asns/?first={first}&offset={offset}\")\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(f\"Request failed with status code {response.status_code}\")\n",
    "\n",
    "    as_rank_dump = response.json()\n",
    "    nodes.extend(as_rank_dump['data']['asns']['edges'])\n",
    "    if not as_rank_dump['data']['asns']['pageInfo']['hasNextPage']:\n",
    "        next_page = False\n",
    "    offset += first\n",
    "\n",
    "edges = [e['node'] for e in nodes]\n",
    "as_rank_df = pd.DataFrame(edges)\n",
    "# 1️⃣ asnDegree (dict) in eigene Spalten auflösen\n",
    "asnDegree_df = as_rank_df[\"asnDegree\"].apply(pd.Series)\n",
    "asnDegree_df.columns = [f\"asnDegree_{c}\" for c in asnDegree_df.columns]\n",
    "\n",
    "# 2️⃣ wieder an den Haupt-DataFrame anhängen\n",
    "as_rank_df = pd.concat([as_rank_df.drop(columns=[\"asnDegree\"]), asnDegree_df], axis=1)\n",
    "\n",
    "# 1️⃣ asnDegree (dict) in eigene Spalten auflösen\n",
    "asnCone_df = as_rank_df[\"cone\"].apply(pd.Series)\n",
    "asnCone_df.columns = [f\"cone_{c}\" for c in asnCone_df.columns]\n",
    "\n",
    "# 2️⃣ wieder an den Haupt-DataFrame anhängen\n",
    "as_rank_df = pd.concat([as_rank_df.drop(columns=[\"cone\"]), asnCone_df], axis=1)\n",
    "\n",
    "# show a quick preview\n",
    "as_rank_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f80590c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asn</th>\n",
       "      <th>rank</th>\n",
       "      <th>asnDegree_total</th>\n",
       "      <th>asnDegree_customer</th>\n",
       "      <th>asnDegree_peer</th>\n",
       "      <th>asnDegree_provider</th>\n",
       "      <th>cone_numberAsns</th>\n",
       "      <th>cone_numberPrefixes</th>\n",
       "      <th>cone_numberAddresses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3356</td>\n",
       "      <td>1</td>\n",
       "      <td>6613</td>\n",
       "      <td>6545</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>53986</td>\n",
       "      <td>873410</td>\n",
       "      <td>3468642119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1299</td>\n",
       "      <td>2</td>\n",
       "      <td>2567</td>\n",
       "      <td>2509</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>41193</td>\n",
       "      <td>776707</td>\n",
       "      <td>3219679484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>174</td>\n",
       "      <td>3</td>\n",
       "      <td>6723</td>\n",
       "      <td>6626</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>38887</td>\n",
       "      <td>730166</td>\n",
       "      <td>3034352967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3257</td>\n",
       "      <td>4</td>\n",
       "      <td>1853</td>\n",
       "      <td>1816</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>36040</td>\n",
       "      <td>612491</td>\n",
       "      <td>2791999209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2914</td>\n",
       "      <td>5</td>\n",
       "      <td>1541</td>\n",
       "      <td>1483</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>25179</td>\n",
       "      <td>576134</td>\n",
       "      <td>2918763154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119390</th>\n",
       "      <td>56279</td>\n",
       "      <td>78320</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119391</th>\n",
       "      <td>215758</td>\n",
       "      <td>78320</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119392</th>\n",
       "      <td>144817</td>\n",
       "      <td>78320</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119393</th>\n",
       "      <td>144068</td>\n",
       "      <td>78320</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119394</th>\n",
       "      <td>2303</td>\n",
       "      <td>78320</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119395 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           asn   rank  asnDegree_total  asnDegree_customer  asnDegree_peer  \\\n",
       "0         3356      1             6613                6545              68   \n",
       "1         1299      2             2567                2509              58   \n",
       "2          174      3             6723                6626              97   \n",
       "3         3257      4             1853                1816              37   \n",
       "4         2914      5             1541                1483              58   \n",
       "...        ...    ...              ...                 ...             ...   \n",
       "119390   56279  78320                0                   0               0   \n",
       "119391  215758  78320                0                   0               0   \n",
       "119392  144817  78320                0                   0               0   \n",
       "119393  144068  78320                0                   0               0   \n",
       "119394    2303  78320                0                   0               0   \n",
       "\n",
       "        asnDegree_provider  cone_numberAsns  cone_numberPrefixes  \\\n",
       "0                        0            53986               873410   \n",
       "1                        0            41193               776707   \n",
       "2                        0            38887               730166   \n",
       "3                        0            36040               612491   \n",
       "4                        0            25179               576134   \n",
       "...                    ...              ...                  ...   \n",
       "119390                   0                1                    0   \n",
       "119391                   0                1                    0   \n",
       "119392                   0                1                    0   \n",
       "119393                   0                1                    0   \n",
       "119394                   0                1                    0   \n",
       "\n",
       "        cone_numberAddresses  \n",
       "0                 3468642119  \n",
       "1                 3219679484  \n",
       "2                 3034352967  \n",
       "3                 2791999209  \n",
       "4                 2918763154  \n",
       "...                      ...  \n",
       "119390                     0  \n",
       "119391                     0  \n",
       "119392                     0  \n",
       "119393                     0  \n",
       "119394                     0  \n",
       "\n",
       "[119395 rows x 9 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges = [e['node'] for e in nodes]\n",
    "as_rank_df = pd.DataFrame(edges)\n",
    "asnDegree_df = as_rank_df[\"asnDegree\"].apply(pd.Series)\n",
    "asnDegree_df.columns = [f\"asnDegree_{c}\" for c in asnDegree_df.columns]\n",
    "\n",
    "# 2️⃣ wieder an den Haupt-DataFrame anhängen\n",
    "as_rank_df = pd.concat([as_rank_df.drop(columns=[\"asnDegree\"]), asnDegree_df], axis=1)\n",
    "\n",
    "# 1️⃣ asnDegree (dict) in eigene Spalten auflösen\n",
    "asnCone_df = as_rank_df[\"cone\"].apply(pd.Series)\n",
    "asnCone_df.columns = [f\"cone_{c}\" for c in asnCone_df.columns]\n",
    "\n",
    "# 2️⃣ wieder an den Haupt-DataFrame anhängen\n",
    "as_rank_df = pd.concat([as_rank_df.drop(columns=[\"cone\"]), asnCone_df], axis=1)\n",
    "\n",
    "as_rank_df['asn'] = as_rank_df['asn'].astype(int)\n",
    "\n",
    "\n",
    "as_rank_df = as_rank_df[['asn', 'rank', 'asnDegree_total', 'asnDegree_customer', 'asnDegree_peer', 'asnDegree_provider', 'cone_numberAsns', 'cone_numberPrefixes', 'cone_numberAddresses']]\n",
    "\n",
    "# show a quick preview\n",
    "as_rank_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a9da4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asn</th>\n",
       "      <th>org_name</th>\n",
       "      <th>country</th>\n",
       "      <th>source</th>\n",
       "      <th>info_type</th>\n",
       "      <th>rank</th>\n",
       "      <th>asnDegree_total</th>\n",
       "      <th>asnDegree_customer</th>\n",
       "      <th>asnDegree_peer</th>\n",
       "      <th>asnDegree_provider</th>\n",
       "      <th>cone_numberAsns</th>\n",
       "      <th>cone_numberPrefixes</th>\n",
       "      <th>cone_numberAddresses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4436</td>\n",
       "      <td>GTT Americas, LLC</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>NSP</td>\n",
       "      <td>78320.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20940</td>\n",
       "      <td>Akamai International B.V.</td>\n",
       "      <td>NL</td>\n",
       "      <td>RIPE</td>\n",
       "      <td>Content</td>\n",
       "      <td>1894.0</td>\n",
       "      <td>485.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8945.0</td>\n",
       "      <td>14612752.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31800</td>\n",
       "      <td>DALnet</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Non-Profit</td>\n",
       "      <td>47745.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3303</td>\n",
       "      <td>Swisscom (Schweiz) AG</td>\n",
       "      <td>CH</td>\n",
       "      <td>RIPE</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "      <td>81.0</td>\n",
       "      <td>1273.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>733.0</td>\n",
       "      <td>22131.0</td>\n",
       "      <td>42899794.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22773</td>\n",
       "      <td>Cox Communications Inc.</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "      <td>110.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>505.0</td>\n",
       "      <td>11982.0</td>\n",
       "      <td>31992440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23625</th>\n",
       "      <td>154232</td>\n",
       "      <td>MAX TECHNOLOGY &amp; SUPPORT SERVICES PRIVATE LIMITED</td>\n",
       "      <td>IN</td>\n",
       "      <td>APNIC</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23626</th>\n",
       "      <td>204856</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Educational/Research</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23627</th>\n",
       "      <td>204917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23628</th>\n",
       "      <td>210796</td>\n",
       "      <td>Bjoern Schleyer</td>\n",
       "      <td>DE</td>\n",
       "      <td>RIPE</td>\n",
       "      <td>NSP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23629</th>\n",
       "      <td>400926</td>\n",
       "      <td>KIWI TELECOM</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>NSP</td>\n",
       "      <td>78320.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23630 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          asn                                           org_name country  \\\n",
       "0        4436                                  GTT Americas, LLC      US   \n",
       "1       20940                          Akamai International B.V.      NL   \n",
       "2       31800                                             DALnet      US   \n",
       "3        3303                              Swisscom (Schweiz) AG      CH   \n",
       "4       22773                            Cox Communications Inc.      US   \n",
       "...       ...                                                ...     ...   \n",
       "23625  154232  MAX TECHNOLOGY & SUPPORT SERVICES PRIVATE LIMITED      IN   \n",
       "23626  204856                                                NaN     NaN   \n",
       "23627  204917                                                NaN     NaN   \n",
       "23628  210796                                    Bjoern Schleyer      DE   \n",
       "23629  400926                                       KIWI TELECOM      US   \n",
       "\n",
       "      source             info_type     rank  asnDegree_total  \\\n",
       "0       ARIN                   NSP  78320.0              0.0   \n",
       "1       RIPE               Content   1894.0            485.0   \n",
       "2       ARIN            Non-Profit  47745.0             78.0   \n",
       "3       RIPE         Cable/DSL/ISP     81.0           1273.0   \n",
       "4       ARIN         Cable/DSL/ISP    110.0            499.0   \n",
       "...      ...                   ...      ...              ...   \n",
       "23625  APNIC         Cable/DSL/ISP      NaN              NaN   \n",
       "23626    NaN  Educational/Research      NaN              NaN   \n",
       "23627    NaN         Cable/DSL/ISP      NaN              NaN   \n",
       "23628   RIPE                   NSP      NaN              NaN   \n",
       "23629   ARIN                   NSP  78320.0              0.0   \n",
       "\n",
       "       asnDegree_customer  asnDegree_peer  asnDegree_provider  \\\n",
       "0                     0.0             0.0                 0.0   \n",
       "1                    14.0           366.0               105.0   \n",
       "2                     0.0            74.0                 4.0   \n",
       "3                   166.0          1101.0                 6.0   \n",
       "4                   489.0             8.0                 2.0   \n",
       "...                   ...             ...                 ...   \n",
       "23625                 NaN             NaN                 NaN   \n",
       "23626                 NaN             NaN                 NaN   \n",
       "23627                 NaN             NaN                 NaN   \n",
       "23628                 NaN             NaN                 NaN   \n",
       "23629                 0.0             0.0                 0.0   \n",
       "\n",
       "       cone_numberAsns  cone_numberPrefixes  cone_numberAddresses  \n",
       "0                  1.0                  0.0                   0.0  \n",
       "1                 15.0               8945.0            14612752.0  \n",
       "2                  1.0                  2.0                 512.0  \n",
       "3                733.0              22131.0            42899794.0  \n",
       "4                505.0              11982.0            31992440.0  \n",
       "...                ...                  ...                   ...  \n",
       "23625              NaN                  NaN                   NaN  \n",
       "23626              NaN                  NaN                   NaN  \n",
       "23627              NaN                  NaN                   NaN  \n",
       "23628              NaN                  NaN                   NaN  \n",
       "23629              1.0                  0.0                   0.0  \n",
       "\n",
       "[23630 rows x 13 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peering_df_joined_with_asrank = pd.merge(\n",
    "    peering_df_joined,\n",
    "    as_rank_df,\n",
    "    left_on='asn',\n",
    "    right_on='asn',\n",
    "    how='left'\n",
    ")\n",
    "peering_df_joined_with_asrank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfbbfeb",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f565817d",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fce41b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          asn                                           org_name country  \\\n",
      "0        4436                                  gtt americas, llc      US   \n",
      "1       20940                          akamai international b.v.      NL   \n",
      "2       31800                                             dalnet      US   \n",
      "3        3303                              swisscom (schweiz) ag      CH   \n",
      "4       22773                            cox communications inc.      US   \n",
      "...       ...                                                ...     ...   \n",
      "23625  154232  max technology & support services private limited      IN   \n",
      "23626  204856                                            unknown     NaN   \n",
      "23627  204917                                            unknown     NaN   \n",
      "23628  210796                                    bjoern schleyer      DE   \n",
      "23629  400926                                       kiwi telecom      US   \n",
      "\n",
      "      source             info_type  \n",
      "0       ARIN                   NSP  \n",
      "1       RIPE               Content  \n",
      "2       ARIN            Non-Profit  \n",
      "3       RIPE         Cable/DSL/ISP  \n",
      "4       ARIN         Cable/DSL/ISP  \n",
      "...      ...                   ...  \n",
      "23625  APNIC         Cable/DSL/ISP  \n",
      "23626    NaN  Educational/Research  \n",
      "23627    NaN         Cable/DSL/ISP  \n",
      "23628   RIPE                   NSP  \n",
      "23629   ARIN                   NSP  \n",
      "\n",
      "[23630 rows x 5 columns]\n",
      "Verwendete Klassen: ['Cable/DSL/ISP', 'NSP', 'Content', 'Enterprise', 'Educational/Research', 'Network Services', 'Route Server', 'Non-Profit', 'Government', 'Route Collector']\n",
      "DataFrame nach Filterung: 23630 Zeilen\n",
      "\n",
      "=== TF-IDF-Modell ===\n",
      "TF-IDF Accuracy: 0.5465494791666666\n",
      "TF-IDF Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       Cable/DSL/ISP       0.67      0.78      0.72      1532\n",
      "             Content       0.35      0.41      0.37       323\n",
      "Educational/Research       0.53      0.50      0.52       189\n",
      "          Enterprise       0.26      0.19      0.22       224\n",
      "          Government       0.33      0.25      0.29        16\n",
      "                 NSP       0.38      0.28      0.32       518\n",
      "    Network Services       0.16      0.07      0.09       105\n",
      "          Non-Profit       0.65      0.28      0.39        80\n",
      "     Route Collector       0.18      0.50      0.27         4\n",
      "        Route Server       0.47      0.49      0.48        81\n",
      "\n",
      "            accuracy                           0.55      3072\n",
      "           macro avg       0.40      0.37      0.37      3072\n",
      "        weighted avg       0.52      0.55      0.53      3072\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# DataFrame (dein echter Datensatz, hier Beispiel\n",
    "df = peering_df_joined\n",
    "# Preprocessing\n",
    "df['org_name'] = df['org_name'].fillna('Unknown').str.lower()\n",
    "\n",
    "# Filtere Klassen mit <2 Einträgen\n",
    "class_counts = df['info_type'].value_counts()\n",
    "valid_classes = class_counts[class_counts >= 2].index\n",
    "df = df[df['info_type'].isin(valid_classes)]\n",
    "print(df)\n",
    "print(f\"Verwendete Klassen: {valid_classes.tolist()}\")\n",
    "print(f\"DataFrame nach Filterung: {len(df)} Zeilen\")\n",
    "\n",
    "# --- TF-IDF-Modell ---\n",
    "print(\"\\n=== TF-IDF-Modell ===\")\n",
    "# TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), lowercase=True)\n",
    "X_tfidf = vectorizer.fit_transform(df['org_name'])\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, df['info_type'], test_size=0.13, random_state=42, stratify=df['info_type'])\n",
    "\n",
    "# Balancing\n",
    "smote = SMOTE(random_state=42)\n",
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_tfidf, y_train)\n",
    "X_train_bal, y_train_bal = rus.fit_resample(X_train_res, y_train_res)\n",
    "\n",
    "# Classifier\n",
    "classifier_tfidf = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs', class_weight='balanced')\n",
    "classifier_tfidf.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Evaluation\n",
    "y_pred_tfidf = classifier_tfidf.predict(X_test_tfidf)\n",
    "print(\"TF-IDF Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
    "print(\"TF-IDF Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a706a136",
   "metadata": {},
   "source": [
    "## Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa48be66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU-Name: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "          asn                                           org_name country  \\\n",
      "0        4436                                  gtt americas, llc      US   \n",
      "1       20940                          akamai international b.v.      NL   \n",
      "2       31800                                             dalnet      US   \n",
      "3        3303                              swisscom (schweiz) ag      CH   \n",
      "4       22773                            cox communications inc.      US   \n",
      "...       ...                                                ...     ...   \n",
      "23625  154232  max technology & support services private limited      IN   \n",
      "23626  204856                                            unknown     NaN   \n",
      "23627  204917                                            unknown     NaN   \n",
      "23628  210796                                    bjoern schleyer      DE   \n",
      "23629  400926                                       kiwi telecom      US   \n",
      "\n",
      "      source             info_type  \n",
      "0       ARIN                   NSP  \n",
      "1       RIPE               Content  \n",
      "2       ARIN            Non-Profit  \n",
      "3       RIPE         Cable/DSL/ISP  \n",
      "4       ARIN         Cable/DSL/ISP  \n",
      "...      ...                   ...  \n",
      "23625  APNIC         Cable/DSL/ISP  \n",
      "23626    NaN  Educational/Research  \n",
      "23627    NaN         Cable/DSL/ISP  \n",
      "23628   RIPE                   NSP  \n",
      "23629   ARIN                   NSP  \n",
      "\n",
      "[23630 rows x 5 columns]\n",
      "Verwendete Klassen: ['Cable/DSL/ISP', 'NSP', 'Content', 'Enterprise', 'Educational/Research', 'Network Services', 'Route Server', 'Non-Profit', 'Government', 'Route Collector']\n",
      "DataFrame nach Filterung: 23630 Zeilen\n",
      "Eindeutige org_name: 20832\n",
      "\n",
      "=== BERT-Modell ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "BERT-Embeddings: 100%|██████████| 326/326 [00:09<00:00, 34.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Accuracy: 0.34403839055001845\n",
      "BERT Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       Cable/DSL/ISP       0.77      0.41      0.53      1418\n",
      "             Content       0.21      0.29      0.24       277\n",
      "Educational/Research       0.38      0.47      0.42       170\n",
      "          Enterprise       0.15      0.23      0.18       201\n",
      "          Government       0.38      0.50      0.43        16\n",
      "                 NSP       0.24      0.23      0.23       436\n",
      "    Network Services       0.03      0.12      0.05        91\n",
      "          Non-Profit       0.11      0.31      0.16        61\n",
      "     Route Collector       0.00      0.00      0.00         2\n",
      "        Route Server       0.10      0.32      0.15        37\n",
      "\n",
      "            accuracy                           0.34      2709\n",
      "           macro avg       0.24      0.29      0.24      2709\n",
      "        weighted avg       0.51      0.34      0.39      2709\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "# Initialize parallel_pandas\n",
    "\n",
    "# Prüfe GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU-Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Warnung: Keine GPU verfügbar, CPU wird verwendet.\")\n",
    "\n",
    "# DataFrame (dein echter Datensatz, hier Beispiel\n",
    "df = peering_df_joined\n",
    "# Preprocessing\n",
    "df['org_name'] = df['org_name'].fillna('Unknown').str.lower()\n",
    "\n",
    "# Filtere Klassen mit <2 Einträgen\n",
    "class_counts = df['info_type'].value_counts()\n",
    "valid_classes = class_counts[class_counts >= 2].index\n",
    "df = df[df['info_type'].isin(valid_classes)]\n",
    "print(df)\n",
    "print(f\"Verwendete Klassen: {valid_classes.tolist()}\")\n",
    "print(f\"DataFrame nach Filterung: {len(df)} Zeilen\")\n",
    "\n",
    "# Deduplizierung\n",
    "unique_df = df.drop_duplicates(subset=['org_name'])\n",
    "print(f\"Eindeutige org_name: {len(unique_df)}\")\n",
    "\n",
    "\n",
    "# --- BERT-Modell ---\n",
    "print(\"\\n=== BERT-Modell ===\")\n",
    "# BERT-Tokenizer und Model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "model.eval()\n",
    "\n",
    "# BERT Embeddings\n",
    "def get_bert_embedding(text, batch_size=64):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(text), batch_size), desc=\"BERT-Embeddings\"):\n",
    "        batch = text[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            embeddings.append(cls_embeddings.cpu().numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Training\n",
    "X_bert = get_bert_embedding(unique_df['org_name'].tolist())\n",
    "y = unique_df['info_type']\n",
    "X_train_bert, X_test_bert, y_train, y_test = train_test_split(X_bert, y, test_size=0.13, random_state=42, stratify=y)\n",
    "\n",
    "# Balancing\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_bert, y_train)\n",
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_train_bal, y_train_bal = rus.fit_resample(X_train_res, y_train_res)\n",
    "\n",
    "# Classifier\n",
    "classifier_bert = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs', class_weight='balanced')\n",
    "classifier_bert.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Evaluation\n",
    "y_pred_bert = classifier_bert.predict(X_test_bert)\n",
    "print(\"BERT Accuracy:\", accuracy_score(y_test, y_pred_bert))\n",
    "print(\"BERT Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd11dd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: 10 Klassen -> ['Cable/DSL/ISP', 'Content', 'Educational/Research', 'Enterprise', 'Government', 'NSP', 'Network Services', 'Non-Profit', 'Route Collector', 'Route Server']\n",
      "Device: cuda\n",
      "GPU-Name: NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [0.018 0.086 0.147 0.125 1.697 0.054 0.267 0.35  6.912 0.344]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5144' max='12860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5144/12860 22:39 < 34:00, 3.78 it/s, Epoch 8/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.172000</td>\n",
       "      <td>2.033163</td>\n",
       "      <td>0.435872</td>\n",
       "      <td>0.218902</td>\n",
       "      <td>0.254875</td>\n",
       "      <td>0.232359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.830100</td>\n",
       "      <td>1.796867</td>\n",
       "      <td>0.457031</td>\n",
       "      <td>0.296792</td>\n",
       "      <td>0.312804</td>\n",
       "      <td>0.352578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.734500</td>\n",
       "      <td>1.769686</td>\n",
       "      <td>0.430339</td>\n",
       "      <td>0.292570</td>\n",
       "      <td>0.287761</td>\n",
       "      <td>0.372117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.668100</td>\n",
       "      <td>1.733563</td>\n",
       "      <td>0.511068</td>\n",
       "      <td>0.329118</td>\n",
       "      <td>0.325248</td>\n",
       "      <td>0.377533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.642100</td>\n",
       "      <td>1.770867</td>\n",
       "      <td>0.475260</td>\n",
       "      <td>0.327483</td>\n",
       "      <td>0.306370</td>\n",
       "      <td>0.384421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.484900</td>\n",
       "      <td>1.820235</td>\n",
       "      <td>0.504232</td>\n",
       "      <td>0.345382</td>\n",
       "      <td>0.352610</td>\n",
       "      <td>0.376024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.475100</td>\n",
       "      <td>1.793263</td>\n",
       "      <td>0.508789</td>\n",
       "      <td>0.340168</td>\n",
       "      <td>0.326939</td>\n",
       "      <td>0.381363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.368200</td>\n",
       "      <td>1.797449</td>\n",
       "      <td>0.455729</td>\n",
       "      <td>0.330194</td>\n",
       "      <td>0.311678</td>\n",
       "      <td>0.372623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval: {'eval_loss': 1.8202348947525024, 'eval_accuracy': 0.5042317708333334, 'eval_f1_macro': 0.34538187490101324, 'eval_precision': 0.3526095238775851, 'eval_recall': 0.3760243040019605, 'eval_runtime': 1.2399, 'eval_samples_per_second': 2477.658, 'eval_steps_per_second': 77.427, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('xlmr_org_trainer_out/model/tokenizer_config.json',\n",
       " 'xlmr_org_trainer_out/model/special_tokens_map.json',\n",
       " 'xlmr_org_trainer_out/model/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Ersatz für den HF-Datasets-Teil (kein pyarrow/datasets nötig) ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding, TrainingArguments, Trainer,\n",
    "                          EarlyStoppingCallback, TextClassificationPipeline)\n",
    "\n",
    "# --------- Konfig ---------\n",
    "MODEL_NAME   = \"xlm-roberta-base\"   # multilingual, starkes Baseline-Modell\n",
    "MAX_LENGTH   = 256                   # Org-Namen sind kurz -> 256 reicht\n",
    "LR           = 1e-5\n",
    "EPOCHS       = 20\n",
    "BATCH_SIZE   = 32\n",
    "WARMUP_RATIO = 0.06\n",
    "SEED         = 42\n",
    "OUT_DIR      = \"xlmr_org_trainer_out\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "le = LabelEncoder()\n",
    "\n",
    "le = LabelEncoder()\n",
    "df = peering_df_joined\n",
    "df[\"label_id\"] = le.fit_transform(df[\"info_type\"])\n",
    "num_labels = len(le.classes_)\n",
    "print(f\"Labels: {num_labels} Klassen ->\", list(le.classes_))\n",
    "\n",
    "df[\"label_id\"] = le.fit_transform(df[\"info_type\"])\n",
    "\n",
    "\n",
    "df.fillna('Unknown', inplace=True)\n",
    "\n",
    "# Prüfe GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU-Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Warnung: Keine GPU verfügbar, CPU wird verwendet.\")\n",
    "\n",
    "# Train/Validation Split (stratifiziert)\n",
    "train_df, eval_df = train_test_split(\n",
    "    df[[\"org_name\", \"label_id\"]],\n",
    "    test_size=0.13,\n",
    "    random_state=SEED,\n",
    "    stratify=df[\"label_id\"]\n",
    ")\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "eval_df  = eval_df.reset_index(drop=True)\n",
    "\n",
    "# Texte & Labels aus den bereits vorbereiteten DataFrames (train_df, eval_df)\n",
    "train_texts = train_df[\"org_name\"].tolist()\n",
    "eval_texts  = eval_df[\"org_name\"].tolist()\n",
    "y_train_np  = train_df[\"label_id\"].to_numpy()\n",
    "y_eval_np   = eval_df[\"label_id\"].to_numpy()\n",
    "num_labels  = df[\"label_id\"].nunique()\n",
    "\n",
    "\n",
    "\n",
    "# Tokenisierung OHNE Padding (Padding macht später der DataCollator)\n",
    "train_enc = tok(train_texts, truncation=True, max_length=MAX_LENGTH)\n",
    "eval_enc  = tok(eval_texts,  truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "class SimpleHFLikeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.enc = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(int(self.labels[idx]))\n",
    "        return item\n",
    "\n",
    "ds_train = SimpleHFLikeDataset(train_enc, y_train_np)\n",
    "ds_eval  = SimpleHFLikeDataset(eval_enc,  y_eval_np)\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tok)\n",
    "\n",
    "valid_classes = sorted(df[\"info_type\"].unique())\n",
    "\n",
    "# ---- Modell + Class Weights wie gehabt ----\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label={int(i): c for i, c in enumerate(valid_classes)},\n",
    "    label2id={c: int(i) for i, c in enumerate(valid_classes)}\n",
    ").to(device)\n",
    "\n",
    "# Class-Weights aus dem Trainingssplit\n",
    "class_counts = np.bincount(y_train_np, minlength=num_labels)\n",
    "weights = class_counts.sum() / np.maximum(class_counts, 1)\n",
    "weights = weights / weights.mean()\n",
    "class_weights = torch.tensor(weights, dtype=torch.float, device=device)\n",
    "print(\"Class weights:\", np.round(weights, 3))\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**{k:v for k,v in inputs.items() if k != \"labels\"})\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fct(logits.view(-1, num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUT_DIR + \"/checkpoints\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    fp16=(device.type==\"cuda\"),\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    seed=SEED,\n",
    "    report_to=[\"none\"],\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "    return {\n",
    "        \"accuracy\":  float(accuracy_score(labels, preds)),\n",
    "        \"f1_macro\":  float(f1_score(labels, preds, average=\"macro\")),\n",
    "        \"precision\": float(precision_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"recall\":    float(recall_score(labels, preds, average=\"macro\")),\n",
    "    }\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_eval,\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Eval:\", metrics)\n",
    "\n",
    "trainer.save_model(OUT_DIR + \"/model\")\n",
    "tok.save_pretrained(OUT_DIR + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1c33e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biggest length of org_name is: 203\n"
     ]
    }
   ],
   "source": [
    "# Calculate the maximum length of org_name strings in peering_df_joined, ignoring NaN values\n",
    "max_org_name_length = joined_df['org_name'].dropna().str.len().max()\n",
    "print(f\"The biggest length of org_name is: {max_org_name_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f335e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Class weights: [0.018 0.086 0.147 0.125 1.697 0.054 0.267 0.35  6.912 0.344]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/tmp/ipykernel_18848/1218507509.py:105: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  feats = torch.tensor([b[\"features\"] for b in batch], dtype=torch.float)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2290' max='16075' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2290/16075 10:18 < 1:02:07, 3.70 it/s, Epoch 3.56/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.165100</td>\n",
       "      <td>3.971931</td>\n",
       "      <td>0.032552</td>\n",
       "      <td>0.084598</td>\n",
       "      <td>0.090505</td>\n",
       "      <td>0.213724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.053700</td>\n",
       "      <td>3.797562</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.171292</td>\n",
       "      <td>0.236636</td>\n",
       "      <td>0.251107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.831100</td>\n",
       "      <td>3.804070</td>\n",
       "      <td>0.058268</td>\n",
       "      <td>0.181972</td>\n",
       "      <td>0.205395</td>\n",
       "      <td>0.279771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==== Text + numerische AS-Rank-Features in einem HF-Trainer ====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# --------- Konfig ---------\n",
    "MODEL_NAME   = \"xlm-roberta-base\"\n",
    "MAX_LENGTH   = 64           # 64 reicht für Org-Namen\n",
    "LR           = 2e-5\n",
    "EPOCHS       = 25\n",
    "BATCH_SIZE   = 32\n",
    "WARMUP_RATIO = 0.06\n",
    "SEED         = 100\n",
    "OUT_DIR      = \"xlmr_org_trainer_out_mixed\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# --------- Daten ---------\n",
    "df = peering_df_joined_with_asrank.copy()\n",
    "\n",
    "# Label-Encode\n",
    "le = LabelEncoder()\n",
    "df[\"label_id\"] = le.fit_transform(df[\"info_type\"].astype(str))\n",
    "num_labels = len(le.classes_)\n",
    "id2label = {i: c for i,c in enumerate(le.classes_)}\n",
    "label2id = {c: i for i,c in enumerate(le.classes_)}\n",
    "\n",
    "# Text\n",
    "df[\"org_name\"] = df[\"org_name\"].fillna(\"Unknown\").astype(str).str.strip()\n",
    "\n",
    "# Numerische Featureliste (deine Spalten)\n",
    "FEAT_COLS = [\n",
    "    \"rank\",\n",
    "    \"asnDegree_total\", \"asnDegree_customer\", \"asnDegree_peer\", \"asnDegree_provider\",\n",
    "    \"cone_numberAsns\", \"cone_numberPrefixes\", \"cone_numberAddresses\",\n",
    "]\n",
    "num_feats = scaler.fit_transform(df[FEAT_COLS].fillna(0))\n",
    "# NaNs -> 0, sinnvolle Skalen (log1p für stark schiefe Größen)\n",
    "num_df = df[FEAT_COLS].copy()\n",
    "for c in FEAT_COLS:\n",
    "    if c in (\"cone_numberPrefixes\",\"cone_numberAddresses\",\"cone_numberAsns\"):\n",
    "        num_df[c] = np.log1p(pd.to_numeric(num_df[c], errors=\"coerce\").fillna(0))\n",
    "    else:\n",
    "        num_df[c] = pd.to_numeric(num_df[c], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# Split\n",
    "train_idx, eval_idx = train_test_split(\n",
    "    np.arange(len(df)),\n",
    "    test_size=0.13,\n",
    "    random_state=SEED,\n",
    "    stratify=df[\"label_id\"]\n",
    ")\n",
    "train_texts = df.loc[train_idx, \"org_name\"].tolist()\n",
    "eval_texts  = df.loc[eval_idx,  \"org_name\"].tolist()\n",
    "y_train_np  = df.loc[train_idx, \"label_id\"].to_numpy()\n",
    "y_eval_np   = df.loc[eval_idx,  \"label_id\"].to_numpy()\n",
    "\n",
    "Xnum_train = num_df.loc[train_idx].to_numpy(dtype=np.float32)\n",
    "Xnum_eval  = num_df.loc[eval_idx].to_numpy(dtype=np.float32)\n",
    "\n",
    "# Standardisieren (nur auf Train fitten!)\n",
    "scaler = StandardScaler()\n",
    "Xnum_train = scaler.fit_transform(Xnum_train)\n",
    "Xnum_eval  = scaler.transform(Xnum_eval)\n",
    "\n",
    "# Tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Tokenisierung (ohne Padding – das macht der Collator)\n",
    "enc_train = tok(train_texts, truncation=True, max_length=MAX_LENGTH)\n",
    "enc_eval  = tok(eval_texts,  truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "# Dataset: gibt Listen (keine Tensors) zurück -> Collator kümmert sich ums Padding/Stacking\n",
    "class TextNumDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, num_feats, labels):\n",
    "        self.enc = encodings\n",
    "        self.num = num_feats\n",
    "        self.y   = labels\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: self.enc[k][idx] for k in self.enc}  # ids/attn as plain lists\n",
    "        item[\"features\"] = self.num[idx]\n",
    "        item[\"labels\"]   = int(self.y[idx])\n",
    "        return item\n",
    "\n",
    "ds_train = TextNumDataset(enc_train, Xnum_train, y_train_np)\n",
    "ds_eval  = TextNumDataset(enc_eval,  Xnum_eval,  y_eval_np)\n",
    "\n",
    "# Custom Collator: pad Text + stapel numerische Features\n",
    "class MixedCollator:\n",
    "    def __init__(self, tokenizer): self.tok = tokenizer\n",
    "    def __call__(self, batch):\n",
    "        text = {k: [b[k] for b in batch] for k in [\"input_ids\",\"attention_mask\"]}\n",
    "        text = self.tok.pad(text, return_tensors=\"pt\")\n",
    "        feats = torch.tensor([b[\"features\"] for b in batch], dtype=torch.float)\n",
    "        labels = torch.tensor([b[\"labels\"] for b in batch], dtype=torch.long)\n",
    "        text[\"features\"] = feats\n",
    "        text[\"labels\"]   = labels\n",
    "        return text\n",
    "\n",
    "collator = MixedCollator(tok)\n",
    "\n",
    "# Class-Weights (aus Train)\n",
    "class_counts = np.bincount(y_train_np, minlength=num_labels)\n",
    "w = class_counts.sum() / np.maximum(class_counts, 1)\n",
    "w = w / w.mean()\n",
    "class_weights = torch.tensor(w, dtype=torch.float, device=device)\n",
    "print(\"Class weights:\", np.round(w, 3))\n",
    "\n",
    "# Modell: XLM-R Encoder + mean pooling + numerische Features -> MLP-Classifier\n",
    "class TextPlusNumClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, num_num_feats, dropout=0.2, use_attn_pool=False):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        hidden = self.backbone.config.hidden_size  # z.B. 768\n",
    "\n",
    "        # 🔸 numerische Features erst in einen eigenen Raum projizieren\n",
    "        self.num_proj = nn.Sequential(\n",
    "            nn.Linear(num_num_feats, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(128),\n",
    "        )\n",
    "\n",
    "        # (optional) Attention-Pooling statt Mean-Pooling\n",
    "        self.use_attn_pool = use_attn_pool\n",
    "        if self.use_attn_pool:\n",
    "            self.attn = nn.Linear(hidden, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden + 128, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, num_labels),\n",
    "        )\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def _pool(self, last_hidden, attn_mask):\n",
    "        if not self.use_attn_pool:\n",
    "            # Mean-Pooling\n",
    "            mask = attn_mask.unsqueeze(-1)                   # [B,T,1]\n",
    "            return (last_hidden * mask).sum(1) / mask.sum(1).clamp(min=1e-9)\n",
    "        # Attention-Pooling\n",
    "        scores = self.attn(last_hidden).squeeze(-1)          # [B,T]\n",
    "        scores = scores.masked_fill(attn_mask == 0, -1e9)\n",
    "        weights = scores.softmax(dim=-1).unsqueeze(-1)       # [B,T,1]\n",
    "        return (last_hidden * weights).sum(1)                # [B,H]\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, features=None, labels=None):\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = self._pool(out.last_hidden_state, attention_mask)      # [B,H]\n",
    "\n",
    "        # 🔸 numerische Features projizieren & mit Text-Embedding konkatenieren\n",
    "        num_emb = self.num_proj(features)                                # [B,128]\n",
    "        z = torch.cat([pooled, num_emb], dim=1)                          # [B,H+128]\n",
    "\n",
    "        logits = self.classifier(self.dropout(z))\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Label smoothing hilft oft bei Imbalance leicht\n",
    "            loss = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)(logits, labels)\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "model = TextPlusNumClassifier(MODEL_NAME, num_labels=num_labels, num_num_feats=Xnum_train.shape[1]).to(device)\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\":  float(accuracy_score(labels, preds)),\n",
    "        \"f1_macro\":  float(f1_score(labels, preds, average=\"macro\")),\n",
    "        \"precision\": float(precision_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"recall\":    float(recall_score(labels, preds, average=\"macro\")),\n",
    "    }\n",
    "\n",
    "# TrainingArguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUT_DIR + \"/checkpoints\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=(device.type==\"cuda\"),\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    seed=SEED,\n",
    "    report_to=[\"none\"],\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_eval,\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Eval:\", trainer.evaluate())\n",
    "\n",
    "# Modelle speichern (Backbone+Head + LabelEncoder & Scaler separat speichern, falls gewünscht)\n",
    "trainer.save_model(OUT_DIR + \"/model\")\n",
    "tok.save_pretrained(OUT_DIR + \"/model\")\n",
    "\n",
    "# Tipp: Speichere auch den StandardScaler, damit du bei Inferenz die numerischen Features identisch transformierst\n",
    "import joblib, os\n",
    "os.makedirs(OUT_DIR + \"/model\", exist_ok=True)\n",
    "joblib.dump({\"scaler\": scaler, \"feat_cols\": FEAT_COLS, \"label_encoder\": le}, OUT_DIR + \"/model/aux.pkl\")\n",
    "print(\"Aux artefacts saved to\", OUT_DIR + \"/model/aux.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a3de1e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "as_rank_df.to_csv('as_rank_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d5563b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "as_rank_df = pd.read_csv('as_rank_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
