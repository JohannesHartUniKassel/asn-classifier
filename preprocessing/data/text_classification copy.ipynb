{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a33344",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "## Peeringdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1c32292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>org_id</th>\n",
       "      <th>name</th>\n",
       "      <th>aka</th>\n",
       "      <th>name_long</th>\n",
       "      <th>website</th>\n",
       "      <th>social_media</th>\n",
       "      <th>asn</th>\n",
       "      <th>looking_glass</th>\n",
       "      <th>route_server</th>\n",
       "      <th>...</th>\n",
       "      <th>policy_ratio</th>\n",
       "      <th>policy_contracts</th>\n",
       "      <th>allow_ixp_update</th>\n",
       "      <th>status_dashboard</th>\n",
       "      <th>rir_status</th>\n",
       "      <th>rir_status_updated</th>\n",
       "      <th>logo</th>\n",
       "      <th>created</th>\n",
       "      <th>updated</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8897</td>\n",
       "      <td>GTT Communications (AS4436)</td>\n",
       "      <td>Formerly known as nLayer Communications</td>\n",
       "      <td></td>\n",
       "      <td>http://www.gtt.net</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'http://...</td>\n",
       "      <td>4436</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>Required</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2022-07-27T05:33:22Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>Akamai Technologies</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://www.akamai.com/</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'https:/...</td>\n",
       "      <td>20940</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Not Required</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.akamaistatus.com/</td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2025-10-20T12:16:12Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>DALnet IRC Network</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>http://www.dal.net</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'http://...</td>\n",
       "      <td>31800</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Not Required</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2025-01-09T13:42:07Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>9350</td>\n",
       "      <td>Swisscom</td>\n",
       "      <td>IP-Plus</td>\n",
       "      <td></td>\n",
       "      <td>http://www.swisscom.com</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'http://...</td>\n",
       "      <td>3303</td>\n",
       "      <td></td>\n",
       "      <td>telnet://route-server.ip-plus.net</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>Required</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2025-08-12T06:33:30Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>Cox Communications</td>\n",
       "      <td>Cox Communications</td>\n",
       "      <td></td>\n",
       "      <td>http://www.cox.com/peering</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'http://...</td>\n",
       "      <td>22773</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Required</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2022-11-28T22:55:17Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  org_id                         name  \\\n",
       "0   1    8897  GTT Communications (AS4436)   \n",
       "1   2      14          Akamai Technologies   \n",
       "2   3      17           DALnet IRC Network   \n",
       "3   5    9350                     Swisscom   \n",
       "4   6      23           Cox Communications   \n",
       "\n",
       "                                       aka name_long  \\\n",
       "0  Formerly known as nLayer Communications             \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                  IP-Plus             \n",
       "4                       Cox Communications             \n",
       "\n",
       "                      website  \\\n",
       "0          http://www.gtt.net   \n",
       "1     https://www.akamai.com/   \n",
       "2          http://www.dal.net   \n",
       "3     http://www.swisscom.com   \n",
       "4  http://www.cox.com/peering   \n",
       "\n",
       "                                        social_media    asn looking_glass  \\\n",
       "0  [{'service': 'website', 'identifier': 'http://...   4436                 \n",
       "1  [{'service': 'website', 'identifier': 'https:/...  20940                 \n",
       "2  [{'service': 'website', 'identifier': 'http://...  31800                 \n",
       "3  [{'service': 'website', 'identifier': 'http://...   3303                 \n",
       "4  [{'service': 'website', 'identifier': 'http://...  22773                 \n",
       "\n",
       "                        route_server  ... policy_ratio policy_contracts  \\\n",
       "0                                     ...         True         Required   \n",
       "1                                     ...        False     Not Required   \n",
       "2                                     ...        False     Not Required   \n",
       "3  telnet://route-server.ip-plus.net  ...         True         Required   \n",
       "4                                     ...        False         Required   \n",
       "\n",
       "  allow_ixp_update               status_dashboard  rir_status  \\\n",
       "0            False                           None          ok   \n",
       "1            False  https://www.akamaistatus.com/          ok   \n",
       "2            False                                         ok   \n",
       "3            False                                         ok   \n",
       "4            False                                         ok   \n",
       "\n",
       "     rir_status_updated  logo               created               updated  \\\n",
       "0  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2022-07-27T05:33:22Z   \n",
       "1  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2025-10-20T12:16:12Z   \n",
       "2  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2025-01-09T13:42:07Z   \n",
       "3  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2025-08-12T06:33:30Z   \n",
       "4  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2022-11-28T22:55:17Z   \n",
       "\n",
       "   status  \n",
       "0      ok  \n",
       "1      ok  \n",
       "2      ok  \n",
       "3      ok  \n",
       "4      ok  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "filepath = Path('peeringdb/peeringdb_2_dump_2025_10_21.json')\n",
    "\n",
    "with filepath.open('r', encoding='utf-8') as f:\n",
    "    dump = json.load(f)\n",
    "\n",
    "# extract the net.data section and load into a DataFrame\n",
    "net_data = dump.get('net', {}).get('data')\n",
    "if net_data is None:\n",
    "    raise KeyError(\"JSON does not contain 'net' -> 'data' structure\")\n",
    "\n",
    "net_df = pd.DataFrame(net_data)\n",
    "net_df['asn'] = net_df['asn'].astype(int)\n",
    "net_df = net_df[net_df['info_type'] != '']\n",
    "\n",
    "# show a quick preview\n",
    "net_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2f20d5",
   "metadata": {},
   "source": [
    "# Caida AS Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c190e0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aut</th>\n",
       "      <th>changed</th>\n",
       "      <th>org_id</th>\n",
       "      <th>source</th>\n",
       "      <th>org_name</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20240618.0</td>\n",
       "      <td>LPL-141-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Level 3 Parent, LLC</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20231108.0</td>\n",
       "      <td>UNIVER-19-Z-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>University of Delaware</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>20100927.0</td>\n",
       "      <td>MIT-2-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Massachusetts Institute of Technology</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>20230929.0</td>\n",
       "      <td>USC-32-Z-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>University of Southern California</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>20200723.0</td>\n",
       "      <td>WGL-117-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>WFA Group LLC</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   aut     changed            org_id source  \\\n",
       "0    1  20240618.0      LPL-141-ARIN   ARIN   \n",
       "1    2  20231108.0  UNIVER-19-Z-ARIN   ARIN   \n",
       "2    3  20100927.0        MIT-2-ARIN   ARIN   \n",
       "3    4  20230929.0     USC-32-Z-ARIN   ARIN   \n",
       "4    5  20200723.0      WGL-117-ARIN   ARIN   \n",
       "\n",
       "                                org_name country  \n",
       "0                    Level 3 Parent, LLC      US  \n",
       "1                 University of Delaware      US  \n",
       "2  Massachusetts Institute of Technology      US  \n",
       "3      University of Southern California      US  \n",
       "4                          WFA Group LLC      US  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "with open('/workspaces/pytorch-gpu-2/preprocessing/data/caida/20251001.as-org2info.txt', 'r', newline='', encoding='utf-8') as input_file:\n",
    "    lines = input_file.readlines()   \n",
    "    # Buffers initialisieren\n",
    "    aut_lines = []\n",
    "    org_lines = []\n",
    "    mode = None\n",
    "    total_lines = len(lines)\n",
    "    aut_count = 0\n",
    "    org_count = 0 \n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"# format:aut\"):\n",
    "            mode = \"aut\"\n",
    "            continue\n",
    "        elif line.startswith(\"# format:org_id\"):\n",
    "            mode = \"org\"\n",
    "            continue\n",
    "        elif line.startswith(\"#\") or not line:\n",
    "            # Andere Kommentar- oder Leerzeilen überspringen\n",
    "            continue      \n",
    "        if mode == \"aut\":\n",
    "            aut_lines.append(line)\n",
    "            aut_count += 1\n",
    "        elif mode == \"org\":\n",
    "            org_lines.append(line)\n",
    "            org_count += 1\n",
    "    # StringIO-Objekte aus den gesammelten Zeilen bauen\n",
    "    aut_buffer = io.StringIO(\"\\n\".join(aut_lines))\n",
    "    org_buffer = io.StringIO(\"\\n\".join(org_lines))\n",
    "    # DataFrames einlesen\n",
    "    aut_df = pd.read_csv(aut_buffer, sep=\"|\",\n",
    "                        names=[\"aut\", \"changed\", \"aut_name\", \"org_id\", \"opaque_id\", \"source\"], usecols=[\"aut\", \"org_id\", \"source\", \"changed\"])\n",
    "    org_df = pd.read_csv(org_buffer, sep=\"|\",\n",
    "                        names=[\"org_id\", \"changed\", \"org_name\", \"country\", \"source\"], usecols=[\"org_id\", \"org_name\", \"country\"])\n",
    "\n",
    "    # Join the DataFrames\n",
    "    joined_df = pd.merge(aut_df, org_df, on=\"org_id\", how=\"left\")\n",
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73e5047",
   "metadata": {},
   "source": [
    "## Join both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6de8ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asn</th>\n",
       "      <th>org_name</th>\n",
       "      <th>country</th>\n",
       "      <th>source</th>\n",
       "      <th>info_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4436</td>\n",
       "      <td>GTT Americas, LLC</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>NSP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20940</td>\n",
       "      <td>Akamai International B.V.</td>\n",
       "      <td>NL</td>\n",
       "      <td>RIPE</td>\n",
       "      <td>Content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31800</td>\n",
       "      <td>DALnet</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Non-Profit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3303</td>\n",
       "      <td>Swisscom (Schweiz) AG</td>\n",
       "      <td>CH</td>\n",
       "      <td>RIPE</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22773</td>\n",
       "      <td>Cox Communications Inc.</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     asn                   org_name country source      info_type\n",
       "0   4436          GTT Americas, LLC      US   ARIN            NSP\n",
       "1  20940  Akamai International B.V.      NL   RIPE        Content\n",
       "2  31800                     DALnet      US   ARIN     Non-Profit\n",
       "3   3303      Swisscom (Schweiz) AG      CH   RIPE  Cable/DSL/ISP\n",
       "4  22773    Cox Communications Inc.      US   ARIN  Cable/DSL/ISP"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peering_df_joined = pd.merge(net_df, joined_df, left_on='asn', right_on='aut', how='left')\n",
    "peering_df_joined = peering_df_joined[['asn', 'org_name', 'country', 'source', 'info_type']]\n",
    "peering_df_joined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfbbfeb",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f565817d",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fce41b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          asn                                           org_name country  \\\n",
      "0        4436                                  gtt americas, llc      US   \n",
      "1       20940                          akamai international b.v.      NL   \n",
      "2       31800                                             dalnet      US   \n",
      "3        3303                              swisscom (schweiz) ag      CH   \n",
      "4       22773                            cox communications inc.      US   \n",
      "...       ...                                                ...     ...   \n",
      "23625  154232  max technology & support services private limited      IN   \n",
      "23626  204856                                            unknown     NaN   \n",
      "23627  204917                                            unknown     NaN   \n",
      "23628  210796                                    bjoern schleyer      DE   \n",
      "23629  400926                                       kiwi telecom      US   \n",
      "\n",
      "      source             info_type  \n",
      "0       ARIN                   NSP  \n",
      "1       RIPE               Content  \n",
      "2       ARIN            Non-Profit  \n",
      "3       RIPE         Cable/DSL/ISP  \n",
      "4       ARIN         Cable/DSL/ISP  \n",
      "...      ...                   ...  \n",
      "23625  APNIC         Cable/DSL/ISP  \n",
      "23626    NaN  Educational/Research  \n",
      "23627    NaN         Cable/DSL/ISP  \n",
      "23628   RIPE                   NSP  \n",
      "23629   ARIN                   NSP  \n",
      "\n",
      "[23630 rows x 5 columns]\n",
      "Verwendete Klassen: ['Cable/DSL/ISP', 'NSP', 'Content', 'Enterprise', 'Educational/Research', 'Network Services', 'Route Server', 'Non-Profit', 'Government', 'Route Collector']\n",
      "DataFrame nach Filterung: 23630 Zeilen\n",
      "\n",
      "=== TF-IDF-Modell ===\n",
      "TF-IDF Accuracy: 0.5465494791666666\n",
      "TF-IDF Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       Cable/DSL/ISP       0.67      0.78      0.72      1532\n",
      "             Content       0.35      0.41      0.37       323\n",
      "Educational/Research       0.53      0.50      0.52       189\n",
      "          Enterprise       0.26      0.19      0.22       224\n",
      "          Government       0.33      0.25      0.29        16\n",
      "                 NSP       0.38      0.28      0.32       518\n",
      "    Network Services       0.16      0.07      0.09       105\n",
      "          Non-Profit       0.65      0.28      0.39        80\n",
      "     Route Collector       0.18      0.50      0.27         4\n",
      "        Route Server       0.47      0.49      0.48        81\n",
      "\n",
      "            accuracy                           0.55      3072\n",
      "           macro avg       0.40      0.37      0.37      3072\n",
      "        weighted avg       0.52      0.55      0.53      3072\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# DataFrame (dein echter Datensatz, hier Beispiel\n",
    "df = peering_df_joined\n",
    "# Preprocessing\n",
    "df['org_name'] = df['org_name'].fillna('Unknown').str.lower()\n",
    "\n",
    "# Filtere Klassen mit <2 Einträgen\n",
    "class_counts = df['info_type'].value_counts()\n",
    "valid_classes = class_counts[class_counts >= 2].index\n",
    "df = df[df['info_type'].isin(valid_classes)]\n",
    "print(df)\n",
    "print(f\"Verwendete Klassen: {valid_classes.tolist()}\")\n",
    "print(f\"DataFrame nach Filterung: {len(df)} Zeilen\")\n",
    "\n",
    "# --- TF-IDF-Modell ---\n",
    "print(\"\\n=== TF-IDF-Modell ===\")\n",
    "# TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), lowercase=True)\n",
    "X_tfidf = vectorizer.fit_transform(df['org_name'])\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, df['info_type'], test_size=0.13, random_state=42, stratify=df['info_type'])\n",
    "\n",
    "# Balancing\n",
    "smote = SMOTE(random_state=42)\n",
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_tfidf, y_train)\n",
    "X_train_bal, y_train_bal = rus.fit_resample(X_train_res, y_train_res)\n",
    "\n",
    "# Classifier\n",
    "classifier_tfidf = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs', class_weight='balanced')\n",
    "classifier_tfidf.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Evaluation\n",
    "y_pred_tfidf = classifier_tfidf.predict(X_test_tfidf)\n",
    "print(\"TF-IDF Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
    "print(\"TF-IDF Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a706a136",
   "metadata": {},
   "source": [
    "## Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa48be66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU-Name: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "          asn                                           org_name country  \\\n",
      "0        4436                                  gtt americas, llc      US   \n",
      "1       20940                          akamai international b.v.      NL   \n",
      "2       31800                                             dalnet      US   \n",
      "3        3303                              swisscom (schweiz) ag      CH   \n",
      "4       22773                            cox communications inc.      US   \n",
      "...       ...                                                ...     ...   \n",
      "23625  154232  max technology & support services private limited      IN   \n",
      "23626  204856                                            unknown     NaN   \n",
      "23627  204917                                            unknown     NaN   \n",
      "23628  210796                                    bjoern schleyer      DE   \n",
      "23629  400926                                       kiwi telecom      US   \n",
      "\n",
      "      source             info_type  \n",
      "0       ARIN                   NSP  \n",
      "1       RIPE               Content  \n",
      "2       ARIN            Non-Profit  \n",
      "3       RIPE         Cable/DSL/ISP  \n",
      "4       ARIN         Cable/DSL/ISP  \n",
      "...      ...                   ...  \n",
      "23625  APNIC         Cable/DSL/ISP  \n",
      "23626    NaN  Educational/Research  \n",
      "23627    NaN         Cable/DSL/ISP  \n",
      "23628   RIPE                   NSP  \n",
      "23629   ARIN                   NSP  \n",
      "\n",
      "[23630 rows x 5 columns]\n",
      "Verwendete Klassen: ['Cable/DSL/ISP', 'NSP', 'Content', 'Enterprise', 'Educational/Research', 'Network Services', 'Route Server', 'Non-Profit', 'Government', 'Route Collector']\n",
      "DataFrame nach Filterung: 23630 Zeilen\n",
      "Eindeutige org_name: 20832\n",
      "\n",
      "=== BERT-Modell ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "BERT-Embeddings: 100%|██████████| 326/326 [00:09<00:00, 34.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Accuracy: 0.34403839055001845\n",
      "BERT Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       Cable/DSL/ISP       0.77      0.41      0.53      1418\n",
      "             Content       0.21      0.29      0.24       277\n",
      "Educational/Research       0.38      0.47      0.42       170\n",
      "          Enterprise       0.15      0.23      0.18       201\n",
      "          Government       0.38      0.50      0.43        16\n",
      "                 NSP       0.24      0.23      0.23       436\n",
      "    Network Services       0.03      0.12      0.05        91\n",
      "          Non-Profit       0.11      0.31      0.16        61\n",
      "     Route Collector       0.00      0.00      0.00         2\n",
      "        Route Server       0.10      0.32      0.15        37\n",
      "\n",
      "            accuracy                           0.34      2709\n",
      "           macro avg       0.24      0.29      0.24      2709\n",
      "        weighted avg       0.51      0.34      0.39      2709\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "# Initialize parallel_pandas\n",
    "\n",
    "# Prüfe GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU-Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Warnung: Keine GPU verfügbar, CPU wird verwendet.\")\n",
    "\n",
    "# DataFrame (dein echter Datensatz, hier Beispiel\n",
    "df = peering_df_joined\n",
    "# Preprocessing\n",
    "df['org_name'] = df['org_name'].fillna('Unknown').str.lower()\n",
    "\n",
    "# Filtere Klassen mit <2 Einträgen\n",
    "class_counts = df['info_type'].value_counts()\n",
    "valid_classes = class_counts[class_counts >= 2].index\n",
    "df = df[df['info_type'].isin(valid_classes)]\n",
    "print(df)\n",
    "print(f\"Verwendete Klassen: {valid_classes.tolist()}\")\n",
    "print(f\"DataFrame nach Filterung: {len(df)} Zeilen\")\n",
    "\n",
    "# Deduplizierung\n",
    "unique_df = df.drop_duplicates(subset=['org_name'])\n",
    "print(f\"Eindeutige org_name: {len(unique_df)}\")\n",
    "\n",
    "\n",
    "# --- BERT-Modell ---\n",
    "print(\"\\n=== BERT-Modell ===\")\n",
    "# BERT-Tokenizer und Model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "model.eval()\n",
    "\n",
    "# BERT Embeddings\n",
    "def get_bert_embedding(text, batch_size=64):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(text), batch_size), desc=\"BERT-Embeddings\"):\n",
    "        batch = text[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            embeddings.append(cls_embeddings.cpu().numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Training\n",
    "X_bert = get_bert_embedding(unique_df['org_name'].tolist())\n",
    "y = unique_df['info_type']\n",
    "X_train_bert, X_test_bert, y_train, y_test = train_test_split(X_bert, y, test_size=0.13, random_state=42, stratify=y)\n",
    "\n",
    "# Balancing\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_bert, y_train)\n",
    "rus = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_train_bal, y_train_bal = rus.fit_resample(X_train_res, y_train_res)\n",
    "\n",
    "# Classifier\n",
    "classifier_bert = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs', class_weight='balanced')\n",
    "classifier_bert.fit(X_train_bal, y_train_bal)\n",
    "\n",
    "# Evaluation\n",
    "y_pred_bert = classifier_bert.predict(X_test_bert)\n",
    "print(\"BERT Accuracy:\", accuracy_score(y_test, y_pred_bert))\n",
    "print(\"BERT Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd11dd4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DatasetDict\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py:1414\u001b[0m\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;66;03m# nn.quant* depends on ao -- so should be after those.\u001b[39;00m\n\u001b[1;32m   1413\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantizable\u001b[39;00m\n\u001b[0;32m-> 1414\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantized\u001b[39;00m\n\u001b[1;32m   1415\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqat\u001b[39;00m\n\u001b[1;32m   1416\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintrinsic\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/quantized/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dynamic  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m modules  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/quantized/dynamic/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantized\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/ao/nn/quantized/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MaxPool2d\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/ao/nn/quantized/functional.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pair, _triple\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mannotations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BroadcastingList2\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pair_from_first\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Although some of the functions and docstrings are mirrored from the torch.nn,\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# we want to have them here for future changes.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_pool2d\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_pool3d\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupsample_nearest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m ]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/ao/nn/quantized/modules/__init__.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatchnorm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchNorm2d, BatchNorm3d\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnormalization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LayerNorm, GroupNorm, InstanceNorm1d, \\\n\u001b[1;32m     16\u001b[0m     InstanceNorm2d, InstanceNorm3d\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv1d, Conv2d, Conv3d\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvTranspose1d, ConvTranspose2d, ConvTranspose3d\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Linear\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/ao/nn/quantized/modules/conv.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintrinsic\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnni\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mao\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintrinsic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqat\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnniqat\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _size_1_t\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/ao/nn/intrinsic/qat/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/ao/nn/intrinsic/qat/modules/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_relu\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearReLU\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_fused\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearBn1d\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconv_fused\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     ConvBn1d,\n\u001b[1;32m      5\u001b[0m     ConvBn2d,\n\u001b[1;32m      6\u001b[0m     ConvBn3d,\n\u001b[1;32m      7\u001b[0m     ConvBnReLU1d,\n\u001b[1;32m      8\u001b[0m     ConvBnReLU2d,\n\u001b[1;32m      9\u001b[0m     ConvBnReLU3d,\n\u001b[1;32m     10\u001b[0m     ConvReLU1d,\n\u001b[1;32m     11\u001b[0m     ConvReLU2d,\n\u001b[1;32m     12\u001b[0m     ConvReLU3d,\n\u001b[1;32m     13\u001b[0m     update_bn_stats,\n\u001b[1;32m     14\u001b[0m     freeze_bn_stats,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinearReLU\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinearBn1d\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfreeze_bn_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     31\u001b[0m ]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/ao/nn/intrinsic/qat/modules/conv_fused.py:25\u001b[0m\n\u001b[1;32m     15\u001b[0m _BN_CLASS_MAP \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;241m1\u001b[39m: nn\u001b[38;5;241m.\u001b[39mBatchNorm1d,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;241m2\u001b[39m: nn\u001b[38;5;241m.\u001b[39mBatchNorm2d,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;241m3\u001b[39m: nn\u001b[38;5;241m.\u001b[39mBatchNorm3d,\n\u001b[1;32m     19\u001b[0m }\n\u001b[1;32m     22\u001b[0m MOD \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMOD\u001b[39m\u001b[38;5;124m'\u001b[39m, bound\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mconv\u001b[38;5;241m.\u001b[39m_ConvNd)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_ConvBnNd\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mconv\u001b[38;5;241m.\u001b[39m_ConvNd, nni\u001b[38;5;241m.\u001b[39m_FusedModule):\n\u001b[1;32m     27\u001b[0m     _version \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     28\u001b[0m     _FLOAT_MODULE \u001b[38;5;241m=\u001b[39m MOD\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# === Ersatz für den HF-Datasets-Teil (kein pyarrow/datasets nötig) ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding, TrainingArguments, Trainer,\n",
    "                          EarlyStoppingCallback, TextClassificationPipeline)\n",
    "\n",
    "# --------- Konfig ---------\n",
    "MODEL_NAME   = \"xlm-roberta-base\"   # multilingual, starkes Baseline-Modell\n",
    "MAX_LENGTH   = 64                   # Org-Namen sind kurz -> 256 reicht\n",
    "LR           = 1e-5\n",
    "EPOCHS       = 20\n",
    "BATCH_SIZE   = 32\n",
    "WARMUP_RATIO = 0.06\n",
    "SEED         = 42\n",
    "OUT_DIR      = \"xlmr_org_trainer_out\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "le = LabelEncoder()\n",
    "\n",
    "le = LabelEncoder()\n",
    "df = peering_df_joined\n",
    "df[\"label_id\"] = le.fit_transform(df[\"info_type\"])\n",
    "num_labels = len(le.classes_)\n",
    "print(f\"Labels: {num_labels} Klassen ->\", list(le.classes_))\n",
    "\n",
    "df[\"label_id\"] = le.fit_transform(df[\"info_type\"])\n",
    "\n",
    "\n",
    "df.fillna('Unknown', inplace=True)\n",
    "\n",
    "# Prüfe GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU-Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Warnung: Keine GPU verfügbar, CPU wird verwendet.\")\n",
    "\n",
    "# Train/Validation Split (stratifiziert)\n",
    "train_df, eval_df = train_test_split(\n",
    "    df[[\"org_name\", \"label_id\"]],\n",
    "    test_size=0.13,\n",
    "    random_state=SEED,\n",
    "    stratify=df[\"label_id\"]\n",
    ")\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "eval_df  = eval_df.reset_index(drop=True)\n",
    "\n",
    "# Texte & Labels aus den bereits vorbereiteten DataFrames (train_df, eval_df)\n",
    "train_texts = train_df[\"org_name\"].tolist()\n",
    "eval_texts  = eval_df[\"org_name\"].tolist()\n",
    "y_train_np  = train_df[\"label_id\"].to_numpy()\n",
    "y_eval_np   = eval_df[\"label_id\"].to_numpy()\n",
    "num_labels  = df[\"label_id\"].nunique()\n",
    "\n",
    "\n",
    "\n",
    "# Tokenisierung OHNE Padding (Padding macht später der DataCollator)\n",
    "train_enc = tok(train_texts, truncation=True, max_length=MAX_LENGTH)\n",
    "eval_enc  = tok(eval_texts,  truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "class SimpleHFLikeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.enc = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(int(self.labels[idx]))\n",
    "        return item\n",
    "\n",
    "ds_train = SimpleHFLikeDataset(train_enc, y_train_np)\n",
    "ds_eval  = SimpleHFLikeDataset(eval_enc,  y_eval_np)\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tok)\n",
    "\n",
    "valid_classes = sorted(df[\"info_type\"].unique())\n",
    "\n",
    "# ---- Modell + Class Weights wie gehabt ----\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label={int(i): c for i, c in enumerate(valid_classes)},\n",
    "    label2id={c: int(i) for i, c in enumerate(valid_classes)}\n",
    ").to(device)\n",
    "\n",
    "# Class-Weights aus dem Trainingssplit\n",
    "class_counts = np.bincount(y_train_np, minlength=num_labels)\n",
    "weights = class_counts.sum() / np.maximum(class_counts, 1)\n",
    "weights = weights / weights.mean()\n",
    "class_weights = torch.tensor(weights, dtype=torch.float, device=device)\n",
    "print(\"Class weights:\", np.round(weights, 3))\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**{k:v for k,v in inputs.items() if k != \"labels\"})\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fct(logits.view(-1, num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUT_DIR + \"/checkpoints\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    fp16=(device.type==\"cuda\"),\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    seed=SEED,\n",
    "    report_to=[\"none\"],\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "    return {\n",
    "        \"accuracy\":  float(accuracy_score(labels, preds)),\n",
    "        \"f1_macro\":  float(f1_score(labels, preds, average=\"macro\")),\n",
    "        \"precision\": float(precision_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"recall\":    float(recall_score(labels, preds, average=\"macro\")),\n",
    "    }\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_eval,\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Eval:\", metrics)\n",
    "\n",
    "\n",
    "trainer.save_model(OUT_DIR + \"/model\")\n",
    "tok.save_pretrained(OUT_DIR + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1c33e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biggest length of org_name is: 203\n"
     ]
    }
   ],
   "source": [
    "# Calculate the maximum length of org_name strings in peering_df_joined, ignoring NaN values\n",
    "max_org_name_length = joined_df['org_name'].dropna().str.len().max()\n",
    "print(f\"The biggest length of org_name is: {max_org_name_length}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
