{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9014be38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>org_id</th>\n",
       "      <th>name</th>\n",
       "      <th>aka</th>\n",
       "      <th>name_long</th>\n",
       "      <th>website</th>\n",
       "      <th>social_media</th>\n",
       "      <th>asn</th>\n",
       "      <th>looking_glass</th>\n",
       "      <th>route_server</th>\n",
       "      <th>...</th>\n",
       "      <th>policy_ratio</th>\n",
       "      <th>policy_contracts</th>\n",
       "      <th>allow_ixp_update</th>\n",
       "      <th>status_dashboard</th>\n",
       "      <th>rir_status</th>\n",
       "      <th>rir_status_updated</th>\n",
       "      <th>logo</th>\n",
       "      <th>created</th>\n",
       "      <th>updated</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8897</td>\n",
       "      <td>GTT Communications (AS4436)</td>\n",
       "      <td>Formerly known as nLayer Communications</td>\n",
       "      <td></td>\n",
       "      <td>http://www.gtt.net</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'http://...</td>\n",
       "      <td>4436</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>Required</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2022-07-27T05:33:22Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>Akamai Technologies</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://www.akamai.com/</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'https:/...</td>\n",
       "      <td>20940</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Not Required</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.akamaistatus.com/</td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2025-10-20T12:16:12Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>DALnet IRC Network</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>http://www.dal.net</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'http://...</td>\n",
       "      <td>31800</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Not Required</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2025-01-09T13:42:07Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>9350</td>\n",
       "      <td>Swisscom</td>\n",
       "      <td>IP-Plus</td>\n",
       "      <td></td>\n",
       "      <td>http://www.swisscom.com</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'http://...</td>\n",
       "      <td>3303</td>\n",
       "      <td></td>\n",
       "      <td>telnet://route-server.ip-plus.net</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>Required</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2025-08-12T06:33:30Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>Cox Communications</td>\n",
       "      <td>Cox Communications</td>\n",
       "      <td></td>\n",
       "      <td>http://www.cox.com/peering</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'http://...</td>\n",
       "      <td>22773</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Required</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2022-11-28T22:55:17Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  org_id                         name  \\\n",
       "0   1    8897  GTT Communications (AS4436)   \n",
       "1   2      14          Akamai Technologies   \n",
       "2   3      17           DALnet IRC Network   \n",
       "3   5    9350                     Swisscom   \n",
       "4   6      23           Cox Communications   \n",
       "\n",
       "                                       aka name_long  \\\n",
       "0  Formerly known as nLayer Communications             \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                  IP-Plus             \n",
       "4                       Cox Communications             \n",
       "\n",
       "                      website  \\\n",
       "0          http://www.gtt.net   \n",
       "1     https://www.akamai.com/   \n",
       "2          http://www.dal.net   \n",
       "3     http://www.swisscom.com   \n",
       "4  http://www.cox.com/peering   \n",
       "\n",
       "                                        social_media    asn looking_glass  \\\n",
       "0  [{'service': 'website', 'identifier': 'http://...   4436                 \n",
       "1  [{'service': 'website', 'identifier': 'https:/...  20940                 \n",
       "2  [{'service': 'website', 'identifier': 'http://...  31800                 \n",
       "3  [{'service': 'website', 'identifier': 'http://...   3303                 \n",
       "4  [{'service': 'website', 'identifier': 'http://...  22773                 \n",
       "\n",
       "                        route_server  ... policy_ratio policy_contracts  \\\n",
       "0                                     ...         True         Required   \n",
       "1                                     ...        False     Not Required   \n",
       "2                                     ...        False     Not Required   \n",
       "3  telnet://route-server.ip-plus.net  ...         True         Required   \n",
       "4                                     ...        False         Required   \n",
       "\n",
       "  allow_ixp_update               status_dashboard  rir_status  \\\n",
       "0            False                           None          ok   \n",
       "1            False  https://www.akamaistatus.com/          ok   \n",
       "2            False                                         ok   \n",
       "3            False                                         ok   \n",
       "4            False                                         ok   \n",
       "\n",
       "     rir_status_updated  logo               created               updated  \\\n",
       "0  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2022-07-27T05:33:22Z   \n",
       "1  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2025-10-20T12:16:12Z   \n",
       "2  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2025-01-09T13:42:07Z   \n",
       "3  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2025-08-12T06:33:30Z   \n",
       "4  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2022-11-28T22:55:17Z   \n",
       "\n",
       "   status  \n",
       "0      ok  \n",
       "1      ok  \n",
       "2      ok  \n",
       "3      ok  \n",
       "4      ok  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "filepath = Path('peeringdb/peeringdb_2_dump_2025_10_21.json')\n",
    "\n",
    "with filepath.open('r', encoding='utf-8') as f:\n",
    "    dump = json.load(f)\n",
    "\n",
    "# extract the net.data section and load into a DataFrame\n",
    "net_data = dump.get('net', {}).get('data')\n",
    "if net_data is None:\n",
    "    raise KeyError(\"JSON does not contain 'net' -> 'data' structure\")\n",
    "\n",
    "net_df = pd.DataFrame(net_data)\n",
    "net_df['asn'] = net_df['asn'].astype(int)\n",
    "net_df = net_df[net_df['info_type'] != '']\n",
    "\n",
    "# show a quick preview\n",
    "net_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8738238d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aut</th>\n",
       "      <th>changed</th>\n",
       "      <th>org_id</th>\n",
       "      <th>source</th>\n",
       "      <th>org_name</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20240618.0</td>\n",
       "      <td>LPL-141-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Level 3 Parent, LLC</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20231108.0</td>\n",
       "      <td>UNIVER-19-Z-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>University of Delaware</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>20100927.0</td>\n",
       "      <td>MIT-2-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Massachusetts Institute of Technology</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>20230929.0</td>\n",
       "      <td>USC-32-Z-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>University of Southern California</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>20200723.0</td>\n",
       "      <td>WGL-117-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>WFA Group LLC</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   aut     changed            org_id source  \\\n",
       "0    1  20240618.0      LPL-141-ARIN   ARIN   \n",
       "1    2  20231108.0  UNIVER-19-Z-ARIN   ARIN   \n",
       "2    3  20100927.0        MIT-2-ARIN   ARIN   \n",
       "3    4  20230929.0     USC-32-Z-ARIN   ARIN   \n",
       "4    5  20200723.0      WGL-117-ARIN   ARIN   \n",
       "\n",
       "                                org_name country  \n",
       "0                    Level 3 Parent, LLC      US  \n",
       "1                 University of Delaware      US  \n",
       "2  Massachusetts Institute of Technology      US  \n",
       "3      University of Southern California      US  \n",
       "4                          WFA Group LLC      US  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "with open('/workspaces/pytorch-gpu-2/preprocessing/data/caida/20251001.as-org2info.txt', 'r', newline='', encoding='utf-8') as input_file:\n",
    "    lines = input_file.readlines()   \n",
    "    # Buffers initialisieren\n",
    "    aut_lines = []\n",
    "    org_lines = []\n",
    "    mode = None\n",
    "    total_lines = len(lines)\n",
    "    aut_count = 0\n",
    "    org_count = 0 \n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"# format:aut\"):\n",
    "            mode = \"aut\"\n",
    "            continue\n",
    "        elif line.startswith(\"# format:org_id\"):\n",
    "            mode = \"org\"\n",
    "            continue\n",
    "        elif line.startswith(\"#\") or not line:\n",
    "            # Andere Kommentar- oder Leerzeilen überspringen\n",
    "            continue      \n",
    "        if mode == \"aut\":\n",
    "            aut_lines.append(line)\n",
    "            aut_count += 1\n",
    "        elif mode == \"org\":\n",
    "            org_lines.append(line)\n",
    "            org_count += 1\n",
    "    # StringIO-Objekte aus den gesammelten Zeilen bauen\n",
    "    aut_buffer = io.StringIO(\"\\n\".join(aut_lines))\n",
    "    org_buffer = io.StringIO(\"\\n\".join(org_lines))\n",
    "    # DataFrames einlesen\n",
    "    aut_df = pd.read_csv(aut_buffer, sep=\"|\",\n",
    "                        names=[\"aut\", \"changed\", \"aut_name\", \"org_id\", \"opaque_id\", \"source\"], usecols=[\"aut\", \"org_id\", \"source\", \"changed\"])\n",
    "    org_df = pd.read_csv(org_buffer, sep=\"|\",\n",
    "                        names=[\"org_id\", \"changed\", \"org_name\", \"country\", \"source\"], usecols=[\"org_id\", \"org_name\", \"country\"])\n",
    "\n",
    "    # Join the DataFrames\n",
    "    joined_df = pd.merge(aut_df, org_df, on=\"org_id\", how=\"left\")\n",
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a47d1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asn</th>\n",
       "      <th>org_name</th>\n",
       "      <th>country</th>\n",
       "      <th>source</th>\n",
       "      <th>info_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4436</td>\n",
       "      <td>GTT Americas, LLC</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>NSP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20940</td>\n",
       "      <td>Akamai International B.V.</td>\n",
       "      <td>NL</td>\n",
       "      <td>RIPE</td>\n",
       "      <td>Content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31800</td>\n",
       "      <td>DALnet</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Non-Profit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3303</td>\n",
       "      <td>Swisscom (Schweiz) AG</td>\n",
       "      <td>CH</td>\n",
       "      <td>RIPE</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22773</td>\n",
       "      <td>Cox Communications Inc.</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     asn                   org_name country source      info_type\n",
       "0   4436          GTT Americas, LLC      US   ARIN            NSP\n",
       "1  20940  Akamai International B.V.      NL   RIPE        Content\n",
       "2  31800                     DALnet      US   ARIN     Non-Profit\n",
       "3   3303      Swisscom (Schweiz) AG      CH   RIPE  Cable/DSL/ISP\n",
       "4  22773    Cox Communications Inc.      US   ARIN  Cable/DSL/ISP"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peering_df_joined = pd.merge(net_df, joined_df, left_on='asn', right_on='aut', how='left')\n",
    "peering_df_joined = peering_df_joined[['asn', 'org_name', 'country', 'source', 'info_type']]\n",
    "peering_df_joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4eda075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching AS Rank data: first=5000, offset=0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(next_page):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching AS Rank data: first=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfirst\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, offset=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moffset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://api.asrank.caida.org/v2/restful/asns/?first=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfirst\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m&offset=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moffset\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest failed with status code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/adapters.py:644\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    641\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 644\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    461\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1303\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1301\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1302\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1159\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "next_page = True\n",
    "nodes = []\n",
    "first=5000\n",
    "offset=0\n",
    "while(next_page):\n",
    "    print(f\"Fetching AS Rank data: first={first}, offset={offset}\")\n",
    "    response = requests.get(f\"https://api.asrank.caida.org/v2/restful/asns/?first={first}&offset={offset}\")\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(f\"Request failed with status code {response.status_code}\")\n",
    "\n",
    "    as_rank_dump = response.json()\n",
    "    nodes.extend(as_rank_dump['data']['asns']['edges'])\n",
    "    if not as_rank_dump['data']['asns']['pageInfo']['hasNextPage']:\n",
    "        next_page = False\n",
    "    offset += first\n",
    "\n",
    "edges = [e['node'] for e in nodes]\n",
    "as_rank_df = pd.DataFrame(edges)\n",
    "# 1️⃣ asnDegree (dict) in eigene Spalten auflösen\n",
    "asnDegree_df = as_rank_df[\"asnDegree\"].apply(pd.Series)\n",
    "asnDegree_df.columns = [f\"asnDegree_{c}\" for c in asnDegree_df.columns]\n",
    "\n",
    "# 2️⃣ wieder an den Haupt-DataFrame anhängen\n",
    "as_rank_df = pd.concat([as_rank_df.drop(columns=[\"asnDegree\"]), asnDegree_df], axis=1)\n",
    "\n",
    "# 1️⃣ asnDegree (dict) in eigene Spalten auflösen\n",
    "asnCone_df = as_rank_df[\"cone\"].apply(pd.Series)\n",
    "asnCone_df.columns = [f\"cone_{c}\" for c in asnCone_df.columns]\n",
    "\n",
    "# 2️⃣ wieder an den Haupt-DataFrame anhängen\n",
    "as_rank_df = pd.concat([as_rank_df.drop(columns=[\"cone\"]), asnCone_df], axis=1)\n",
    "\n",
    "# show a quick preview\n",
    "as_rank_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a6eddf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asn</th>\n",
       "      <th>rank</th>\n",
       "      <th>asnDegree_total</th>\n",
       "      <th>asnDegree_customer</th>\n",
       "      <th>asnDegree_peer</th>\n",
       "      <th>asnDegree_provider</th>\n",
       "      <th>cone_numberAsns</th>\n",
       "      <th>cone_numberPrefixes</th>\n",
       "      <th>cone_numberAddresses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3356</td>\n",
       "      <td>1</td>\n",
       "      <td>6613</td>\n",
       "      <td>6545</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>53986</td>\n",
       "      <td>873410</td>\n",
       "      <td>3468642119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1299</td>\n",
       "      <td>2</td>\n",
       "      <td>2567</td>\n",
       "      <td>2509</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>41193</td>\n",
       "      <td>776707</td>\n",
       "      <td>3219679484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>174</td>\n",
       "      <td>3</td>\n",
       "      <td>6723</td>\n",
       "      <td>6626</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>38887</td>\n",
       "      <td>730166</td>\n",
       "      <td>3034352967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3257</td>\n",
       "      <td>4</td>\n",
       "      <td>1853</td>\n",
       "      <td>1816</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>36040</td>\n",
       "      <td>612491</td>\n",
       "      <td>2791999209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2914</td>\n",
       "      <td>5</td>\n",
       "      <td>1541</td>\n",
       "      <td>1483</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>25179</td>\n",
       "      <td>576134</td>\n",
       "      <td>2918763154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    asn  rank  asnDegree_total  asnDegree_customer  asnDegree_peer  \\\n",
       "0  3356     1             6613                6545              68   \n",
       "1  1299     2             2567                2509              58   \n",
       "2   174     3             6723                6626              97   \n",
       "3  3257     4             1853                1816              37   \n",
       "4  2914     5             1541                1483              58   \n",
       "\n",
       "   asnDegree_provider  cone_numberAsns  cone_numberPrefixes  \\\n",
       "0                   0            53986               873410   \n",
       "1                   0            41193               776707   \n",
       "2                   0            38887               730166   \n",
       "3                   0            36040               612491   \n",
       "4                   0            25179               576134   \n",
       "\n",
       "   cone_numberAddresses  \n",
       "0            3468642119  \n",
       "1            3219679484  \n",
       "2            3034352967  \n",
       "3            2791999209  \n",
       "4            2918763154  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "as_rank_df = pd.read_csv('/workspaces/pytorch-gpu-2/preprocessing/data/as_rank_df.csv')\n",
    "as_rank_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ae442df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asn</th>\n",
       "      <th>org_name</th>\n",
       "      <th>country</th>\n",
       "      <th>source</th>\n",
       "      <th>info_type</th>\n",
       "      <th>rank</th>\n",
       "      <th>asnDegree_total</th>\n",
       "      <th>asnDegree_customer</th>\n",
       "      <th>asnDegree_peer</th>\n",
       "      <th>asnDegree_provider</th>\n",
       "      <th>cone_numberAsns</th>\n",
       "      <th>cone_numberPrefixes</th>\n",
       "      <th>cone_numberAddresses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4436</td>\n",
       "      <td>GTT Americas, LLC</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>NSP</td>\n",
       "      <td>78320.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20940</td>\n",
       "      <td>Akamai International B.V.</td>\n",
       "      <td>NL</td>\n",
       "      <td>RIPE</td>\n",
       "      <td>Content</td>\n",
       "      <td>1894.0</td>\n",
       "      <td>485.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8945.0</td>\n",
       "      <td>14612752.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31800</td>\n",
       "      <td>DALnet</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Non-Profit</td>\n",
       "      <td>47745.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3303</td>\n",
       "      <td>Swisscom (Schweiz) AG</td>\n",
       "      <td>CH</td>\n",
       "      <td>RIPE</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "      <td>81.0</td>\n",
       "      <td>1273.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>733.0</td>\n",
       "      <td>22131.0</td>\n",
       "      <td>42899794.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22773</td>\n",
       "      <td>Cox Communications Inc.</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "      <td>110.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>505.0</td>\n",
       "      <td>11982.0</td>\n",
       "      <td>31992440.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23620</th>\n",
       "      <td>35359</td>\n",
       "      <td>ADR TEL s.p.a.</td>\n",
       "      <td>IT</td>\n",
       "      <td>RIPE</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "      <td>26962.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2048.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23621</th>\n",
       "      <td>26431</td>\n",
       "      <td>Happy Telecommunications LLC</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Content</td>\n",
       "      <td>78320.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23623</th>\n",
       "      <td>210856</td>\n",
       "      <td>Jesse Bakker trading as Skynode</td>\n",
       "      <td>NL</td>\n",
       "      <td>RIPE</td>\n",
       "      <td>NSP</td>\n",
       "      <td>66977.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23624</th>\n",
       "      <td>152033</td>\n",
       "      <td>PT Mandari Teknologi Nusantara</td>\n",
       "      <td>ID</td>\n",
       "      <td>APNIC</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "      <td>78320.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23629</th>\n",
       "      <td>400926</td>\n",
       "      <td>KIWI TELECOM</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>NSP</td>\n",
       "      <td>78320.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23392 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          asn                         org_name country source      info_type  \\\n",
       "0        4436                GTT Americas, LLC      US   ARIN            NSP   \n",
       "1       20940        Akamai International B.V.      NL   RIPE        Content   \n",
       "2       31800                           DALnet      US   ARIN     Non-Profit   \n",
       "3        3303            Swisscom (Schweiz) AG      CH   RIPE  Cable/DSL/ISP   \n",
       "4       22773          Cox Communications Inc.      US   ARIN  Cable/DSL/ISP   \n",
       "...       ...                              ...     ...    ...            ...   \n",
       "23620   35359                   ADR TEL s.p.a.      IT   RIPE  Cable/DSL/ISP   \n",
       "23621   26431     Happy Telecommunications LLC      US   ARIN        Content   \n",
       "23623  210856  Jesse Bakker trading as Skynode      NL   RIPE            NSP   \n",
       "23624  152033   PT Mandari Teknologi Nusantara      ID  APNIC  Cable/DSL/ISP   \n",
       "23629  400926                     KIWI TELECOM      US   ARIN            NSP   \n",
       "\n",
       "          rank  asnDegree_total  asnDegree_customer  asnDegree_peer  \\\n",
       "0      78320.0              0.0                 0.0             0.0   \n",
       "1       1894.0            485.0                14.0           366.0   \n",
       "2      47745.0             78.0                 0.0            74.0   \n",
       "3         81.0           1273.0               166.0          1101.0   \n",
       "4        110.0            499.0               489.0             8.0   \n",
       "...        ...              ...                 ...             ...   \n",
       "23620  26962.0              2.0                 0.0             0.0   \n",
       "23621  78320.0              0.0                 0.0             0.0   \n",
       "23623  66977.0              1.0                 0.0             0.0   \n",
       "23624  78320.0              0.0                 0.0             0.0   \n",
       "23629  78320.0              0.0                 0.0             0.0   \n",
       "\n",
       "       asnDegree_provider  cone_numberAsns  cone_numberPrefixes  \\\n",
       "0                     0.0              1.0                  0.0   \n",
       "1                   105.0             15.0               8945.0   \n",
       "2                     4.0              1.0                  2.0   \n",
       "3                     6.0            733.0              22131.0   \n",
       "4                     2.0            505.0              11982.0   \n",
       "...                   ...              ...                  ...   \n",
       "23620                 2.0              1.0                  8.0   \n",
       "23621                 0.0              1.0                  0.0   \n",
       "23623                 1.0              1.0                  1.0   \n",
       "23624                 0.0              1.0                  0.0   \n",
       "23629                 0.0              1.0                  0.0   \n",
       "\n",
       "       cone_numberAddresses  \n",
       "0                       0.0  \n",
       "1                14612752.0  \n",
       "2                     512.0  \n",
       "3                42899794.0  \n",
       "4                31992440.0  \n",
       "...                     ...  \n",
       "23620                2048.0  \n",
       "23621                   0.0  \n",
       "23623                 256.0  \n",
       "23624                   0.0  \n",
       "23629                   0.0  \n",
       "\n",
       "[23392 rows x 13 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peering_df_joined_with_asrank = pd.merge(\n",
    "    peering_df_joined,\n",
    "    as_rank_df,\n",
    "    left_on='asn',\n",
    "    right_on='asn',\n",
    "    how='left'\n",
    ")\n",
    "peering_df_joined_with_asrank = peering_df_joined_with_asrank.dropna()\n",
    "peering_df_joined_with_asrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "798c6637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Head-only Warmup ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2328/2740863333.py:107: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  text[\"features\"] = torch.tensor([b[\"features\"] for b in batch], dtype=torch.float)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1272' max='1272' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1272/1272 00:26, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.498000</td>\n",
       "      <td>1.421169</td>\n",
       "      <td>0.512989</td>\n",
       "      <td>0.099231</td>\n",
       "      <td>0.145458</td>\n",
       "      <td>0.117807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.430300</td>\n",
       "      <td>1.402779</td>\n",
       "      <td>0.518251</td>\n",
       "      <td>0.103131</td>\n",
       "      <td>0.154370</td>\n",
       "      <td>0.119113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup Eval: {'eval_loss': 1.4027788639068604, 'eval_accuracy': 0.5182505754685959, 'eval_f1_macro': 0.10313052987034874, 'eval_precision': 0.15437042039320942, 'eval_recall': 0.11911256766115803, 'eval_runtime': 1.0431, 'eval_samples_per_second': 2915.32, 'eval_steps_per_second': 92.032, 'epoch': 2.0}\n",
      "\n",
      "=== Unfreeze + LLRD ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/vscode/.local/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5088' max='5088' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5088/5088 15:44, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.341600</td>\n",
       "      <td>1.278851</td>\n",
       "      <td>0.559355</td>\n",
       "      <td>0.211321</td>\n",
       "      <td>0.270957</td>\n",
       "      <td>0.216799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.292900</td>\n",
       "      <td>1.224119</td>\n",
       "      <td>0.573167</td>\n",
       "      <td>0.276170</td>\n",
       "      <td>0.345887</td>\n",
       "      <td>0.270960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.229400</td>\n",
       "      <td>1.196620</td>\n",
       "      <td>0.587636</td>\n",
       "      <td>0.296089</td>\n",
       "      <td>0.328967</td>\n",
       "      <td>0.298595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.210500</td>\n",
       "      <td>1.184992</td>\n",
       "      <td>0.583690</td>\n",
       "      <td>0.345557</td>\n",
       "      <td>0.394055</td>\n",
       "      <td>0.335610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.170000</td>\n",
       "      <td>1.183088</td>\n",
       "      <td>0.588622</td>\n",
       "      <td>0.366560</td>\n",
       "      <td>0.394769</td>\n",
       "      <td>0.367987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.196200</td>\n",
       "      <td>1.172963</td>\n",
       "      <td>0.595199</td>\n",
       "      <td>0.374609</td>\n",
       "      <td>0.408676</td>\n",
       "      <td>0.368994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.175400</td>\n",
       "      <td>1.173576</td>\n",
       "      <td>0.597501</td>\n",
       "      <td>0.377181</td>\n",
       "      <td>0.404846</td>\n",
       "      <td>0.378122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.153700</td>\n",
       "      <td>1.173249</td>\n",
       "      <td>0.596514</td>\n",
       "      <td>0.376048</td>\n",
       "      <td>0.403539</td>\n",
       "      <td>0.376960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Eval: {'eval_loss': 1.1735758781433105, 'eval_accuracy': 0.597500822097994, 'eval_f1_macro': 0.3771814000111374, 'eval_precision': 0.4048462755629164, 'eval_recall': 0.37812229245071194, 'eval_runtime': 1.1219, 'eval_samples_per_second': 2710.694, 'eval_steps_per_second': 85.573, 'epoch': 8.0}\n",
      "Gespeichert nach: xlmr_org_trainer_out_mixed_fix/model\n"
     ]
    }
   ],
   "source": [
    "# ==== Text + numerische AS-Rank-Features: robustes, korrigiertes Training ====\n",
    "import os, numpy as np, pandas as pd, torch, torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, TrainingArguments, Trainer, EarlyStoppingCallback, AdamW\n",
    ")\n",
    "import joblib\n",
    "\n",
    "# --------- Konfig ---------\n",
    "MODEL_NAME   = \"xlm-roberta-base\"\n",
    "MAX_LENGTH   = 64\n",
    "LR_BACKBONE  = 5e-6          # Basis-LR für den Backbone nach Unfreeze\n",
    "LR_HEAD      = 2e-4          # höhere LR für Kopf/Num-Proj beim Warmup; nach Unfreeze kleiner\n",
    "EPOCHS_WARM  = 2             # 1–2 Epochen: nur Kopf trainieren\n",
    "EPOCHS_MAIN  = 8             # 6–10: gesamtes Modell mit LLRD\n",
    "BATCH_SIZE   = 32\n",
    "WARMUP_RATIO = 0.06\n",
    "SEED         = 100\n",
    "OUT_DIR      = \"xlmr_org_trainer_out_mixed_fix\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# --------- Daten ---------\n",
    "df = peering_df_joined_with_asrank.copy()\n",
    "df = df.reset_index(drop=True)   # Index zurücksetzen\n",
    "\n",
    "# Label\n",
    "le = LabelEncoder()\n",
    "df[\"label_id\"] = le.fit_transform(df[\"info_type\"].astype(str))\n",
    "num_labels = len(le.classes_)\n",
    "id2label = {i: c for i,c in enumerate(le.classes_)}\n",
    "label2id = {c: i for i,c in enumerate(le.classes_)}\n",
    "\n",
    "# Text\n",
    "def norm_name(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    s = \" \".join(s.split())\n",
    "    return s or \"unknown\"\n",
    "df[\"org_name\"] = df[\"org_name\"].fillna(\"unknown\").astype(str).map(norm_name)\n",
    "\n",
    "# Numerische Features\n",
    "FEAT_COLS = [\n",
    "    \"rank\",\n",
    "    \"asnDegree_total\",\"asnDegree_customer\",\"asnDegree_peer\",\"asnDegree_provider\",\n",
    "    \"cone_numberAsns\",\"cone_numberPrefixes\",\"cone_numberAddresses\",\n",
    "]\n",
    "num_df = df[FEAT_COLS].copy()\n",
    "for c in FEAT_COLS:\n",
    "    v = pd.to_numeric(num_df[c], errors=\"coerce\").fillna(0)\n",
    "    # starke Schiefe log1p-transformieren\n",
    "    if c.startswith(\"asnDegree\") or c.startswith(\"cone_number\") or c == \"rank\":\n",
    "        num_df[c] = np.log1p(v)\n",
    "    else:\n",
    "        num_df[c] = v\n",
    "\n",
    "# Split (stratifiziert)\n",
    "train_idx, eval_idx = train_test_split(\n",
    "    np.arange(len(df)),\n",
    "    test_size=0.13,\n",
    "    random_state=SEED,\n",
    "    stratify=df[\"label_id\"]\n",
    ")\n",
    "y_train_np  = df.loc[train_idx, \"label_id\"].to_numpy()\n",
    "y_eval_np   = df.loc[eval_idx,  \"label_id\"].to_numpy()\n",
    "\n",
    "texts_train = df.loc[train_idx, \"org_name\"].tolist()\n",
    "texts_eval  = df.loc[eval_idx,  \"org_name\"].tolist()\n",
    "\n",
    "Xnum_train_raw = num_df.loc[train_idx].to_numpy(dtype=np.float32)\n",
    "Xnum_eval_raw  = num_df.loc[eval_idx].to_numpy(dtype=np.float32)\n",
    "\n",
    "# Standardisieren: nur auf Train fitten!\n",
    "scaler = StandardScaler()\n",
    "Xnum_train = scaler.fit_transform(Xnum_train_raw)\n",
    "Xnum_eval  = scaler.transform(Xnum_eval_raw)\n",
    "\n",
    "# Tokenizer & Encodings (Padding übernimmt Collator)\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "enc_train = tok(texts_train, truncation=True, max_length=MAX_LENGTH)\n",
    "enc_eval  = tok(texts_eval,  truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "# Dataset\n",
    "class TextNumDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, num_feats, labels):\n",
    "        self.enc = encodings\n",
    "        self.num = num_feats\n",
    "        self.y   = labels\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: self.enc[k][idx] for k in self.enc}  # plain Python lists\n",
    "        item[\"features\"] = self.num[idx]                # np.ndarray row\n",
    "        item[\"labels\"]   = int(self.y[idx])\n",
    "        return item\n",
    "\n",
    "ds_train = TextNumDataset(enc_train, Xnum_train, y_train_np)\n",
    "ds_eval  = TextNumDataset(enc_eval,  Xnum_eval,  y_eval_np)\n",
    "\n",
    "# Collator\n",
    "class MixedCollator:\n",
    "    def __init__(self, tokenizer): self.tok = tokenizer\n",
    "    def __call__(self, batch):\n",
    "        text = {k: [b[k] for b in batch] for k in [\"input_ids\",\"attention_mask\"]}\n",
    "        text = self.tok.pad(text, return_tensors=\"pt\")\n",
    "        text[\"features\"] = torch.tensor([b[\"features\"] for b in batch], dtype=torch.float)\n",
    "        text[\"labels\"]   = torch.tensor([b[\"labels\"] for b in batch], dtype=torch.long)\n",
    "        return text\n",
    "\n",
    "collator = MixedCollator(tok)\n",
    "\n",
    "# Modell\n",
    "class TextPlusNumClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, num_num_feats, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        hidden = self.backbone.config.hidden_size  # 768 für XLM-R base\n",
    "\n",
    "        # Numerik-Projektion (gibt den Zahlen „Stimme“)\n",
    "        self.num_proj = nn.Sequential(\n",
    "            nn.Linear(num_num_feats, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden + 128, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, num_labels),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, features=None, labels=None):\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # CLS-Pooling (XLM-R: <s> am Index 0)\n",
    "        pooled = out.last_hidden_state[:, 0]                  # [B,H]\n",
    "        num_emb = self.num_proj(features)                     # [B,128]\n",
    "        z = torch.cat([pooled, num_emb], dim=1)               # [B,H+128]\n",
    "        logits = self.classifier(self.dropout(z))\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)      # erst mal „pur“\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "model = TextPlusNumClassifier(MODEL_NAME, num_labels=num_labels, num_num_feats=Xnum_train.shape[1]).to(device)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\":  float(accuracy_score(labels, preds)),\n",
    "        \"f1_macro\":  float(f1_score(labels, preds, average=\"macro\")),\n",
    "        \"precision\": float(precision_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"recall\":    float(recall_score(labels, preds, average=\"macro\")),\n",
    "    }\n",
    "\n",
    "# ---------------------------\n",
    "# 1) HEAD-ONLY WARMUP (Backbone gefroren)\n",
    "# ---------------------------\n",
    "for p in model.backbone.parameters():  # Backbone einfrieren\n",
    "    p.requires_grad = False\n",
    "\n",
    "args_warm = TrainingArguments(\n",
    "    output_dir=OUT_DIR + \"/warmup\",\n",
    "    num_train_epochs=EPOCHS_WARM,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LR_HEAD,                 # Kopf darf schneller lernen\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    fp16=False,                            # stabil\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    seed=SEED,\n",
    "    report_to=[\"none\"],\n",
    ")\n",
    "\n",
    "trainer_warm = Trainer(\n",
    "    model=model,\n",
    "    args=args_warm,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_eval,\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Head-only Warmup ===\")\n",
    "trainer_warm.train()\n",
    "print(\"Warmup Eval:\", trainer_warm.evaluate())\n",
    "\n",
    "# ---------------------------\n",
    "# 2) UNFREEZE + LLRD (Layer-wise LR Decay)\n",
    "# ---------------------------\n",
    "for p in model.backbone.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# LLRD: untere Layer kleinere LR, Head größere LR\n",
    "param_groups = []\n",
    "nL = model.backbone.config.num_hidden_layers\n",
    "# Encoder-Layer\n",
    "for i in range(nL):\n",
    "    params_i = [p for n,p in model.backbone.named_parameters() if f\"encoder.layer.{i}.\" in n]\n",
    "    if params_i:\n",
    "        param_groups.append({\"params\": params_i, \"lr\": LR_BACKBONE * (0.9 ** (nL - 1 - i))})\n",
    "# Embeddings\n",
    "emb_params = [p for n,p in model.backbone.named_parameters() if \"embeddings\" in n]\n",
    "if emb_params:\n",
    "    param_groups.append({\"params\": emb_params, \"lr\": LR_BACKBONE * (0.9 ** nL)})\n",
    "# Kopf (num_proj + classifier) mit höherer LR\n",
    "head_params = [p for n,p in model.named_parameters() if (\"num_proj\" in n) or (\"classifier\" in n)]\n",
    "param_groups.append({\"params\": head_params, \"lr\": LR_BACKBONE * 8})\n",
    "\n",
    "optimizer = AdamW(param_groups, weight_decay=0.01)\n",
    "\n",
    "args_main = TrainingArguments(\n",
    "    output_dir=OUT_DIR + \"/checkpoints\",\n",
    "    num_train_epochs=EPOCHS_MAIN,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LR_BACKBONE,            # wird von param_groups überschrieben\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    fp16=False,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    seed=SEED,\n",
    "    report_to=[\"none\"],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args_main,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_eval,\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    optimizers=(optimizer, None),          # <- unser LLRD-Optimizer\n",
    ")\n",
    "\n",
    "print(\"\\n=== Unfreeze + LLRD ===\")\n",
    "trainer.train()\n",
    "print(\"Final Eval:\", trainer.evaluate())\n",
    "\n",
    "# Speichern\n",
    "os.makedirs(OUT_DIR + \"/model\", exist_ok=True)\n",
    "trainer.save_model(OUT_DIR + \"/model\")\n",
    "tok.save_pretrained(OUT_DIR + \"/model\")\n",
    "joblib.dump({\"scaler\": scaler, \"feat_cols\": FEAT_COLS, \"label_encoder\": le}, OUT_DIR + \"/model/aux.pkl\")\n",
    "print(\"Gespeichert nach:\", OUT_DIR + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6f8b7b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1/96 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF (Text+Num-Netz)     : Acc=0.5975  F1m=0.3772\n",
      "      RandomForest only : Acc=0.3907  F1m=0.1654\n",
      "Ensemble a=0.90     : Acc=0.6014  F1m=0.3773\n",
      "Ensemble a=0.85     : Acc=0.6014  F1m=0.3763\n",
      "Ensemble a=0.80     : Acc=0.6041  F1m=0.3764\n",
      "Ensemble a=0.75     : Acc=0.6057  F1m=0.3751\n",
      "Ensemble a=0.70     : Acc=0.6057  F1m=0.3767\n",
      "Ensemble a=0.60     : Acc=0.5978  F1m=0.3667\n",
      "Ensemble a=0.50     : Acc=0.5939  F1m=0.3411\n",
      "\n",
      "Best Ensemble:\n",
      "a=0.70  Acc=0.6057  F1m=0.3767\n",
      "\n",
      "Per-Class Report (best a):\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       Cable/DSL/ISP       0.65      0.90      0.76      1521\n",
      "             Content       0.41      0.34      0.37       320\n",
      "Educational/Research       0.63      0.74      0.68       184\n",
      "          Enterprise       0.45      0.24      0.32       221\n",
      "          Government       0.56      0.62      0.59        16\n",
      "                 NSP       0.55      0.20      0.30       515\n",
      "    Network Services       0.00      0.00      0.00       102\n",
      "          Non-Profit       0.47      0.19      0.27        78\n",
      "     Route Collector       0.00      0.00      0.00         4\n",
      "        Route Server       0.42      0.56      0.48        80\n",
      "\n",
      "            accuracy                           0.61      3041\n",
      "           macro avg       0.41      0.38      0.38      3041\n",
      "        weighted avg       0.56      0.61      0.55      3041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==== Light Ensembling: HF (Text+Num) + XGBoost/RandomForest auf Numeric ====\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# 1) HF-Probabilitäten (Softmax über Logits)\n",
    "pred = trainer.predict(ds_eval)  # enthält .predictions (logits) & .label_ids\n",
    "logits = pred.predictions\n",
    "probs_hf = np.exp(logits - logits.max(axis=1, keepdims=True))\n",
    "probs_hf = probs_hf / probs_hf.sum(axis=1, keepdims=True)\n",
    "y_true   = pred.label_ids\n",
    "\n",
    "# 2) Zweitmodell auf numerischen Features: XGBoost, sonst RandomForest\n",
    "clf_name = None\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=800, max_depth=None,\n",
    "    class_weight=\"balanced_subsample\", n_jobs=-1, random_state=42\n",
    ")\n",
    "clf_name = \"RandomForest\"\n",
    "\n",
    "clf.fit(Xnum_train, y_train_np)\n",
    "\n",
    "# 2b) Ensure: proba-Aligment auf die globale Klassenreihenfolge\n",
    "def probs_aligned(clf, X, global_classes):\n",
    "    \"\"\"Align predict_proba Spalten auf global_classes-Reihenfolge.\"\"\"\n",
    "    p = clf.predict_proba(X)\n",
    "    # sklearn/xgb liefern eigene class_ Ordnung:\n",
    "    model_classes = clf.classes_\n",
    "    out = np.zeros((p.shape[0], len(global_classes)), dtype=float)\n",
    "    colmap = {c:i for i,c in enumerate(model_classes)}\n",
    "    for j, c in enumerate(global_classes):\n",
    "        if c in colmap:\n",
    "            out[:, j] = p[:, colmap[c]]\n",
    "        else:\n",
    "            # falls Klasse im Train fehlte: 0-Proba\n",
    "            out[:, j] = 0.0\n",
    "    # numerische Stabilität\n",
    "    row_sum = out.sum(axis=1, keepdims=True)\n",
    "    mask = row_sum.squeeze(-1) > 0\n",
    "    out[mask] /= row_sum[mask]\n",
    "    # falls Zeile Summe=0 (extremer Randfall): gleich verteilen\n",
    "    out[~mask] = 1.0/len(global_classes)\n",
    "    return out\n",
    "\n",
    "probs_num = probs_aligned(clf, Xnum_eval, global_classes=np.arange(len(le.classes_)))\n",
    "\n",
    "# 3) Baselines drucken\n",
    "acc_hf  = accuracy_score(y_true, probs_hf.argmax(1))\n",
    "f1m_hf  = f1_score(y_true, probs_hf.argmax(1), average=\"macro\")\n",
    "acc_num = accuracy_score(y_true, probs_num.argmax(1))\n",
    "f1m_num = f1_score(y_true, probs_num.argmax(1), average=\"macro\")\n",
    "\n",
    "print(f\"HF (Text+Num-Netz)     : Acc={acc_hf:.4f}  F1m={f1m_hf:.4f}\")\n",
    "print(f\"{clf_name:>18s} only : Acc={acc_num:.4f}  F1m={f1m_num:.4f}\")\n",
    "\n",
    "# 4) Gewichtetes Mischen (alpha = Anteil HF)\n",
    "alphas = [0.9, 0.85, 0.8, 0.75, 0.7, 0.6, 0.5]\n",
    "best = (-1, -1, None)  # (acc, f1m, alpha)\n",
    "for a in alphas:\n",
    "    probs_ens = a*probs_hf + (1-a)*probs_num\n",
    "    y_pred = probs_ens.argmax(1)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    print(f\"Ensemble a={a:.2f}     : Acc={acc:.4f}  F1m={f1m:.4f}\")\n",
    "    if (acc, f1m) > (best[0], best[1]):\n",
    "        best = (acc, f1m, a)\n",
    "\n",
    "print(\"\\nBest Ensemble:\")\n",
    "print(f\"a={best[2]:.2f}  Acc={best[0]:.4f}  F1m={best[1]:.4f}\")\n",
    "\n",
    "# Optional: detaillierter Report des besten Mischgewichts\n",
    "a = best[2]\n",
    "probs_best = a*probs_hf + (1-a)*probs_num\n",
    "y_pred_best = probs_best.argmax(1)\n",
    "print(\"\\nPer-Class Report (best a):\")\n",
    "print(classification_report(y_true, y_pred_best, target_names=list(le.classes_), zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fb3d4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STACKING  Acc=0.5571  F1m=0.3238\n",
      "\n",
      "Per-Class Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       Cable/DSL/ISP       0.63      0.84      0.72      1521\n",
      "             Content       0.32      0.28      0.30       320\n",
      "Educational/Research       0.60      0.60      0.60       184\n",
      "          Enterprise       0.36      0.22      0.27       221\n",
      "          Government       0.57      0.25      0.35        16\n",
      "                 NSP       0.39      0.23      0.29       515\n",
      "    Network Services       0.06      0.01      0.02       102\n",
      "          Non-Profit       0.31      0.13      0.18        78\n",
      "     Route Collector       0.00      0.00      0.00         4\n",
      "        Route Server       0.52      0.51      0.52        80\n",
      "\n",
      "            accuracy                           0.56      3041\n",
      "           macro avg       0.38      0.31      0.32      3041\n",
      "        weighted avg       0.51      0.56      0.52      3041\n",
      "\n",
      "Stacking artefacts saved to xlmr_org_trainer_out_mixed_fix/model/stacking.pkl\n"
     ]
    }
   ],
   "source": [
    "# ==== Stacking: Meta-LogReg lernt Kombination aus HF- und Numeric-Modell ====\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "# 0) Helper: Spalten der predict_proba an globale Klassenordnung angleichen\n",
    "def probs_aligned(clf, X, global_classes):\n",
    "    p = clf.predict_proba(X)\n",
    "    model_classes = clf.classes_\n",
    "    out = np.zeros((p.shape[0], len(global_classes)), dtype=float)\n",
    "    colmap = {c:i for i,c in enumerate(model_classes)}\n",
    "    for j, c in enumerate(global_classes):\n",
    "        if c in colmap:\n",
    "            out[:, j] = p[:, colmap[c]]\n",
    "        else:\n",
    "            out[:, j] = 0.0\n",
    "    row_sum = out.sum(axis=1, keepdims=True)\n",
    "    mask = row_sum.squeeze(-1) > 0\n",
    "    out[mask] /= row_sum[mask]\n",
    "    out[~mask] = 1.0/len(global_classes)\n",
    "    return out\n",
    "\n",
    "# 1) HF-Wahrscheinlichkeiten (Train + Eval)\n",
    "pred_tr = trainer.predict(ds_train)\n",
    "pred_ev = trainer.predict(ds_eval)\n",
    "\n",
    "logits_tr = pred_tr.predictions\n",
    "logits_ev = pred_ev.predictions\n",
    "\n",
    "# numerisch stabile Softmax\n",
    "probs_hf_tr = np.exp(logits_tr - logits_tr.max(axis=1, keepdims=True))\n",
    "probs_hf_tr /= probs_hf_tr.sum(axis=1, keepdims=True)\n",
    "\n",
    "probs_hf_ev = np.exp(logits_ev - logits_ev.max(axis=1, keepdims=True))\n",
    "probs_hf_ev /= probs_hf_ev.sum(axis=1, keepdims=True)\n",
    "\n",
    "y_true_tr = pred_tr.label_ids\n",
    "y_true_ev = pred_ev.label_ids\n",
    "\n",
    "# 2) Zweitmodell auf Numeric-Features (nimm dein vorhandenes; sonst RF)\n",
    "try:\n",
    "    clf  # falls du schon einen (XGB/RF) aus dem vorherigen Schritt hast\n",
    "except NameError:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=800,\n",
    "        class_weight=\"balanced_subsample\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    clf.fit(Xnum_train, y_train_np)\n",
    "\n",
    "probs_num_tr = probs_aligned(clf, Xnum_train, global_classes=np.arange(len(le.classes_)))\n",
    "probs_num_ev = probs_aligned(clf, Xnum_eval,  global_classes=np.arange(len(le.classes_)))\n",
    "\n",
    "# 3) Meta-Features: concat der Basis-Wahrscheinlichkeiten\n",
    "X_meta_tr = np.hstack([probs_hf_tr, probs_num_tr])\n",
    "X_meta_ev = np.hstack([probs_hf_ev, probs_num_ev])\n",
    "\n",
    "# 4) Meta-Learner (multinomial LogReg). Kein class_weight hier: Inputs sind schon Probabilitäten.\n",
    "meta = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    multi_class=\"multinomial\",\n",
    "    solver=\"lbfgs\"   # stabil für multinomial\n",
    ")\n",
    "meta.fit(X_meta_tr, y_true_tr)\n",
    "\n",
    "y_pred_stack = meta.predict(X_meta_ev)\n",
    "\n",
    "acc = accuracy_score(y_true_ev, y_pred_stack)\n",
    "f1m = f1_score(y_true_ev, y_pred_stack, average=\"macro\")\n",
    "print(f\"STACKING  Acc={acc:.4f}  F1m={f1m:.4f}\")\n",
    "print(\"\\nPer-Class Report:\")\n",
    "print(classification_report(y_true_ev, y_pred_stack, target_names=list(le.classes_), zero_division=0))\n",
    "\n",
    "# 5) Optional: speichern für Deployment\n",
    "joblib.dump(\n",
    "    {\"meta\": meta, \"base_numeric\": clf, \"label_encoder\": le},\n",
    "    OUT_DIR + \"/model/stacking.pkl\"\n",
    ")\n",
    "print(\"Stacking artefacts saved to\", OUT_DIR + \"/model/stacking.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20624e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost[emb+num]    : Acc=0.6136  F1m=0.4356\n",
      "HF (Text+Num-Netz) : Acc=0.5975  F1m=0.3772\n",
      "Ensemble a=0.90    : Acc=0.6044  F1m=0.3878\n",
      "Ensemble a=0.85    : Acc=0.6084  F1m=0.3961\n",
      "Ensemble a=0.80    : Acc=0.6110  F1m=0.3981\n",
      "Ensemble a=0.75    : Acc=0.6110  F1m=0.3979\n",
      "Ensemble a=0.70    : Acc=0.6143  F1m=0.4034\n",
      "Ensemble a=0.60    : Acc=0.6143  F1m=0.4024\n",
      "\n",
      "Best Ensemble:\n",
      "a=0.70  Acc=0.6143  F1m=0.4034\n",
      "\n",
      "Per-Class Report (best a):\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       Cable/DSL/ISP       0.66      0.89      0.76      1521\n",
      "             Content       0.44      0.36      0.40       320\n",
      "Educational/Research       0.62      0.75      0.68       184\n",
      "          Enterprise       0.42      0.24      0.31       221\n",
      "          Government       0.50      0.69      0.58        16\n",
      "                 NSP       0.50      0.24      0.33       515\n",
      "    Network Services       0.50      0.03      0.06       102\n",
      "          Non-Profit       0.59      0.24      0.35        78\n",
      "     Route Collector       0.00      0.00      0.00         4\n",
      "        Route Server       0.61      0.56      0.58        80\n",
      "\n",
      "            accuracy                           0.61      3041\n",
      "           macro avg       0.48      0.40      0.40      3041\n",
      "        weighted avg       0.58      0.61      0.57      3041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==== XGB auf [Text-Embeddings || Numeric] + Ensemble mit HF ====\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# 1) Embeddings aus dem bereits trainierten Modell ziehen (CLS)\n",
    "model.eval()\n",
    "@torch.no_grad()\n",
    "def get_embeds(dataset, batch=256):\n",
    "    embs = []\n",
    "    for i in range(0, len(dataset), batch):\n",
    "        batch_items = [dataset[j] for j in range(i, min(i+batch, len(dataset)))]\n",
    "        enc = tok.pad(\n",
    "            {\"input_ids\":[b[\"input_ids\"] for b in batch_items],\n",
    "             \"attention_mask\":[b[\"attention_mask\"] for b in batch_items]},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "        # Backbone forward\n",
    "        out = model.backbone(**enc)\n",
    "        cls = out.last_hidden_state[:, 0]          # CLS\n",
    "        embs.append(cls.cpu().numpy())\n",
    "    return np.vstack(embs)\n",
    "\n",
    "Xtxt_tr = get_embeds(ds_train)\n",
    "Xtxt_ev = get_embeds(ds_eval)\n",
    "\n",
    "# 2) Kombinieren mit numerischen Features\n",
    "Xcomb_tr = np.hstack([Xtxt_tr, Xnum_train])\n",
    "Xcomb_ev = np.hstack([Xtxt_ev, Xnum_eval])\n",
    "\n",
    "# 3) XGBoost (Fallback: RandomForest)\n",
    "def probs_aligned(clf, X, global_classes):\n",
    "    p = clf.predict_proba(X)\n",
    "    model_classes = clf.classes_\n",
    "    out = np.zeros((p.shape[0], len(global_classes)), dtype=float)\n",
    "    colmap = {c:i for i,c in enumerate(model_classes)}\n",
    "    for j, c in enumerate(global_classes):\n",
    "        out[:, j] = p[:, colmap[c]] if c in colmap else 0.0\n",
    "    s = out.sum(axis=1, keepdims=True)\n",
    "    ok = s.squeeze(-1) > 0\n",
    "    out[ok] /= s[ok]\n",
    "    out[~ok] = 1.0 / len(global_classes)\n",
    "    return out\n",
    "\n",
    "clf_name = None\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    clf = xgb.XGBClassifier(\n",
    "        objective=\"multi:softprob\",\n",
    "        num_class=len(le.classes_),\n",
    "        n_estimators=900, max_depth=8, learning_rate=0.06,\n",
    "        subsample=0.9, colsample_bytree=0.9,\n",
    "        reg_lambda=1.0, reg_alpha=0.0,\n",
    "        tree_method=\"hist\",            # falls GPU: \"gpu_hist\"\n",
    "        n_jobs=-1, random_state=42\n",
    "    )\n",
    "    clf_name = \"XGBoost[emb+num]\"\n",
    "except Exception:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=800, class_weight=\"balanced_subsample\",\n",
    "        n_jobs=-1, random_state=42\n",
    "    )\n",
    "    clf_name = \"RandomForest[emb+num]\"\n",
    "\n",
    "clf.fit(Xcomb_tr, y_train_np)\n",
    "probs_xgb = probs_aligned(clf, Xcomb_ev, global_classes=np.arange(len(le.classes_)))\n",
    "\n",
    "# 4) HF-Probabilitäten holen\n",
    "pred_ev = trainer.predict(ds_eval)\n",
    "logits_ev = pred_ev.predictions\n",
    "probs_hf = np.exp(logits_ev - logits_ev.max(axis=1, keepdims=True))\n",
    "probs_hf /= probs_hf.sum(axis=1, keepdims=True)\n",
    "y_true = pred_ev.label_ids\n",
    "\n",
    "# 5) Einzel-Performance\n",
    "acc_x = accuracy_score(y_true, probs_xgb.argmax(1))\n",
    "f1m_x = f1_score(y_true, probs_xgb.argmax(1), average=\"macro\")\n",
    "acc_h = accuracy_score(y_true, probs_hf.argmax(1))\n",
    "f1m_h = f1_score(y_true, probs_hf.argmax(1), average=\"macro\")\n",
    "print(f\"{clf_name:20s}: Acc={acc_x:.4f}  F1m={f1m_x:.4f}\")\n",
    "print(f\"HF (Text+Num-Netz) : Acc={acc_h:.4f}  F1m={f1m_h:.4f}\")\n",
    "\n",
    "# 6) Ensemble (gewichtetes Mischen der Wahrscheinlichkeiten)\n",
    "best = (-1, -1, None)\n",
    "for a in [0.9,0.85,0.8,0.75,0.7,0.6]:\n",
    "    probs_ens = a*probs_hf + (1-a)*probs_xgb\n",
    "    y_pred = probs_ens.argmax(1)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1m = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    print(f\"Ensemble a={a:.2f}    : Acc={acc:.4f}  F1m={f1m:.4f}\")\n",
    "    if (acc, f1m) > (best[0], best[1]):\n",
    "        best = (acc, f1m, a)\n",
    "\n",
    "print(\"\\nBest Ensemble:\")\n",
    "print(f\"a={best[2]:.2f}  Acc={best[0]:.4f}  F1m={best[1]:.4f}\")\n",
    "\n",
    "# 7) Report für das beste a\n",
    "from sklearn.metrics import classification_report\n",
    "probs_best = best[2]*probs_hf + (1-best[2])*probs_xgb\n",
    "y_pred_best = probs_best.argmax(1)\n",
    "print(\"\\nPer-Class Report (best a):\")\n",
    "print(classification_report(y_true, y_pred_best, target_names=list(le.classes_), zero_division=0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
