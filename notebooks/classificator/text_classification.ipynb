{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a33344",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "## Peeringdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1c32292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "org_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "aka",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "name_long",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "website",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "social_media",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "asn",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "looking_glass",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "route_server",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "irr_as_set",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "info_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "info_types",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "info_prefixes4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "info_prefixes6",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "info_traffic",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "info_ratio",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "info_scope",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "info_unicast",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "info_multicast",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "info_ipv6",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "info_never_via_route_servers",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "ix_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "fac_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "notes",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "netixlan_updated",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "netfac_updated",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "poc_updated",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "policy_url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "policy_general",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "policy_locations",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "policy_ratio",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "policy_contracts",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "allow_ixp_update",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "status_dashboard",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "rir_status",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "rir_status_updated",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "logo",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "created",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "updated",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "status",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "38e16568-b8c3-4630-8c2f-80ea8eb9641a",
       "rows": [
        [
         "0",
         "1",
         "8897",
         "GTT Communications (AS4436)",
         "Formerly known as nLayer Communications",
         "",
         "http://www.gtt.net",
         "[{'service': 'website', 'identifier': 'http://www.gtt.net'}]",
         "4436",
         "",
         "",
         "",
         "NSP",
         "['NSP']",
         "200000.0",
         "10000.0",
         "",
         "",
         "Global",
         "True",
         "False",
         "True",
         "False",
         "0",
         "0",
         "nLayer / AS4436 has been acquired by GTT Communications / AS3257 and is no longer directly peering.  Please refer all peering related inquiries to peering [at] gtt [dot] net.",
         "2021-09-22T00:06:59Z",
         "2016-09-19T05:47:27Z",
         "2016-03-14T21:53:18Z",
         "http://www.gtt.net/peering/",
         "Restrictive",
         "Required - International",
         "True",
         "Required",
         "False",
         null,
         "ok",
         "2024-06-26T04:47:55Z",
         null,
         "2004-07-28T00:00:00Z",
         "2022-07-27T05:33:22Z",
         "ok"
        ],
        [
         "1",
         "2",
         "14",
         "Akamai Technologies",
         "",
         "",
         "https://www.akamai.com/",
         "[{'service': 'website', 'identifier': 'https://www.akamai.com/'}]",
         "20940",
         "",
         "",
         "AS-AKAMAI",
         "Content",
         "['Content']",
         "12000.0",
         "5000.0",
         "100+Tbps",
         "Heavy Outbound",
         "Global",
         "True",
         "False",
         "True",
         "False",
         "231",
         "217",
         "",
         "2025-10-20T12:15:26Z",
         "2025-09-18T10:39:32Z",
         "2021-05-11T21:26:32Z",
         "",
         "Open",
         "Not Required",
         "False",
         "Not Required",
         "False",
         "https://www.akamaistatus.com/",
         "ok",
         "2024-06-26T04:47:55Z",
         null,
         "2004-07-28T00:00:00Z",
         "2025-10-20T12:16:12Z",
         "ok"
        ],
        [
         "2",
         "3",
         "17",
         "DALnet IRC Network",
         "",
         "",
         "http://www.dal.net",
         "[{'service': 'website', 'identifier': 'http://www.dal.net'}]",
         "31800",
         "",
         "",
         "AS31800",
         "Non-Profit",
         "['Non-Profit']",
         "2.0",
         "0.0",
         "100-1000Mbps",
         "Heavy Inbound",
         "Global",
         "True",
         "False",
         "False",
         "False",
         "15",
         "0",
         "",
         "2025-01-09T13:41:48Z",
         null,
         "2016-03-14T21:22:01Z",
         "http://peering.dal.net",
         "Open",
         "Preferred",
         "False",
         "Not Required",
         "False",
         "",
         "ok",
         "2024-06-26T04:47:55Z",
         null,
         "2004-07-28T00:00:00Z",
         "2025-01-09T13:42:07Z",
         "ok"
        ],
        [
         "3",
         "5",
         "9350",
         "Swisscom",
         "IP-Plus",
         "",
         "http://www.swisscom.com",
         "[{'service': 'website', 'identifier': 'http://www.swisscom.com'}]",
         "3303",
         "",
         "telnet://route-server.ip-plus.net",
         "RIPE::AS3303:AS-SWCMGLOBAL",
         "Cable/DSL/ISP",
         "['Cable/DSL/ISP']",
         "10000.0",
         "1800.0",
         "1-5Tbps",
         "Mostly Inbound",
         "Europe",
         "True",
         "False",
         "True",
         "False",
         "63",
         "32",
         "",
         "2025-10-16T06:47:17Z",
         "2025-04-10T13:25:48Z",
         "2020-01-22T04:24:08Z",
         "https://www.swisscom.ch/content/dam/swisscom/de/ws/documents/d-ott-dokumente/20230101_swisscom-peering-policy.pdf",
         "Selective",
         "Preferred",
         "True",
         "Required",
         "False",
         "",
         "ok",
         "2024-06-26T04:47:55Z",
         null,
         "2004-07-28T00:00:00Z",
         "2025-08-12T06:33:30Z",
         "ok"
        ],
        [
         "4",
         "6",
         "23",
         "Cox Communications",
         "Cox Communications",
         "",
         "http://www.cox.com/peering",
         "[{'service': 'website', 'identifier': 'http://www.cox.com/peering'}]",
         "22773",
         "",
         "",
         "AS22773:AS-CONE",
         "Cable/DSL/ISP",
         "['Cable/DSL/ISP']",
         "10000.0",
         "3000.0",
         "100-200Gbps",
         "Mostly Inbound",
         "North America",
         "True",
         "False",
         "True",
         "False",
         "0",
         "14",
         "",
         null,
         "2022-03-24T19:56:00Z",
         "2024-03-06T01:56:24Z",
         "http://www.cox.com/peering",
         "Selective",
         "Preferred",
         "False",
         "Required",
         "False",
         "",
         "ok",
         "2024-06-26T04:47:55Z",
         null,
         "2004-07-28T00:00:00Z",
         "2022-11-28T22:55:17Z",
         "ok"
        ]
       ],
       "shape": {
        "columns": 41,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>org_id</th>\n",
       "      <th>name</th>\n",
       "      <th>aka</th>\n",
       "      <th>name_long</th>\n",
       "      <th>website</th>\n",
       "      <th>social_media</th>\n",
       "      <th>asn</th>\n",
       "      <th>looking_glass</th>\n",
       "      <th>route_server</th>\n",
       "      <th>...</th>\n",
       "      <th>policy_ratio</th>\n",
       "      <th>policy_contracts</th>\n",
       "      <th>allow_ixp_update</th>\n",
       "      <th>status_dashboard</th>\n",
       "      <th>rir_status</th>\n",
       "      <th>rir_status_updated</th>\n",
       "      <th>logo</th>\n",
       "      <th>created</th>\n",
       "      <th>updated</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8897</td>\n",
       "      <td>GTT Communications (AS4436)</td>\n",
       "      <td>Formerly known as nLayer Communications</td>\n",
       "      <td></td>\n",
       "      <td>http://www.gtt.net</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'http://...</td>\n",
       "      <td>4436</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>Required</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2022-07-27T05:33:22Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>Akamai Technologies</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://www.akamai.com/</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'https:/...</td>\n",
       "      <td>20940</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Not Required</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.akamaistatus.com/</td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2025-10-20T12:16:12Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>DALnet IRC Network</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>http://www.dal.net</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'http://...</td>\n",
       "      <td>31800</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Not Required</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2025-01-09T13:42:07Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>9350</td>\n",
       "      <td>Swisscom</td>\n",
       "      <td>IP-Plus</td>\n",
       "      <td></td>\n",
       "      <td>http://www.swisscom.com</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'http://...</td>\n",
       "      <td>3303</td>\n",
       "      <td></td>\n",
       "      <td>telnet://route-server.ip-plus.net</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>Required</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2025-08-12T06:33:30Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>Cox Communications</td>\n",
       "      <td>Cox Communications</td>\n",
       "      <td></td>\n",
       "      <td>http://www.cox.com/peering</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'http://...</td>\n",
       "      <td>22773</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Required</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2022-11-28T22:55:17Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  org_id                         name  \\\n",
       "0   1    8897  GTT Communications (AS4436)   \n",
       "1   2      14          Akamai Technologies   \n",
       "2   3      17           DALnet IRC Network   \n",
       "3   5    9350                     Swisscom   \n",
       "4   6      23           Cox Communications   \n",
       "\n",
       "                                       aka name_long  \\\n",
       "0  Formerly known as nLayer Communications             \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                  IP-Plus             \n",
       "4                       Cox Communications             \n",
       "\n",
       "                      website  \\\n",
       "0          http://www.gtt.net   \n",
       "1     https://www.akamai.com/   \n",
       "2          http://www.dal.net   \n",
       "3     http://www.swisscom.com   \n",
       "4  http://www.cox.com/peering   \n",
       "\n",
       "                                        social_media    asn looking_glass  \\\n",
       "0  [{'service': 'website', 'identifier': 'http://...   4436                 \n",
       "1  [{'service': 'website', 'identifier': 'https:/...  20940                 \n",
       "2  [{'service': 'website', 'identifier': 'http://...  31800                 \n",
       "3  [{'service': 'website', 'identifier': 'http://...   3303                 \n",
       "4  [{'service': 'website', 'identifier': 'http://...  22773                 \n",
       "\n",
       "                        route_server  ... policy_ratio policy_contracts  \\\n",
       "0                                     ...         True         Required   \n",
       "1                                     ...        False     Not Required   \n",
       "2                                     ...        False     Not Required   \n",
       "3  telnet://route-server.ip-plus.net  ...         True         Required   \n",
       "4                                     ...        False         Required   \n",
       "\n",
       "  allow_ixp_update               status_dashboard  rir_status  \\\n",
       "0            False                           None          ok   \n",
       "1            False  https://www.akamaistatus.com/          ok   \n",
       "2            False                                         ok   \n",
       "3            False                                         ok   \n",
       "4            False                                         ok   \n",
       "\n",
       "     rir_status_updated  logo               created               updated  \\\n",
       "0  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2022-07-27T05:33:22Z   \n",
       "1  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2025-10-20T12:16:12Z   \n",
       "2  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2025-01-09T13:42:07Z   \n",
       "3  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2025-08-12T06:33:30Z   \n",
       "4  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2022-11-28T22:55:17Z   \n",
       "\n",
       "   status  \n",
       "0      ok  \n",
       "1      ok  \n",
       "2      ok  \n",
       "3      ok  \n",
       "4      ok  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "filepath = Path('../../preprocessing/data/peeringdb/peeringdb_2_dump_2025_10_21.json')\n",
    "\n",
    "with filepath.open('r', encoding='utf-8') as f:\n",
    "    dump = json.load(f)\n",
    "\n",
    "# extract the net.data section and load into a DataFrame\n",
    "net_data = dump.get('net', {}).get('data')\n",
    "if net_data is None:\n",
    "    raise KeyError(\"JSON does not contain 'net' -> 'data' structure\")\n",
    "\n",
    "net_df = pd.DataFrame(net_data)\n",
    "net_df['asn'] = net_df['asn'].astype(int)\n",
    "net_df = net_df[net_df['info_type'] != '']\n",
    "\n",
    "# show a quick preview\n",
    "net_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2f20d5",
   "metadata": {},
   "source": [
    "# Caida AS Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c190e0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "aut",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "changed",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "org_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "source",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "org_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "country",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "35202784-5d4e-4880-8b85-c995baf717d9",
       "rows": [
        [
         "0",
         "1",
         "20240618.0",
         "LPL-141-ARIN",
         "ARIN",
         "Level 3 Parent, LLC",
         "US"
        ],
        [
         "1",
         "2",
         "20231108.0",
         "UNIVER-19-Z-ARIN",
         "ARIN",
         "University of Delaware",
         "US"
        ],
        [
         "2",
         "3",
         "20100927.0",
         "MIT-2-ARIN",
         "ARIN",
         "Massachusetts Institute of Technology",
         "US"
        ],
        [
         "3",
         "4",
         "20230929.0",
         "USC-32-Z-ARIN",
         "ARIN",
         "University of Southern California",
         "US"
        ],
        [
         "4",
         "5",
         "20200723.0",
         "WGL-117-ARIN",
         "ARIN",
         "WFA Group LLC",
         "US"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aut</th>\n",
       "      <th>changed</th>\n",
       "      <th>org_id</th>\n",
       "      <th>source</th>\n",
       "      <th>org_name</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20240618.0</td>\n",
       "      <td>LPL-141-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Level 3 Parent, LLC</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20231108.0</td>\n",
       "      <td>UNIVER-19-Z-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>University of Delaware</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>20100927.0</td>\n",
       "      <td>MIT-2-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Massachusetts Institute of Technology</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>20230929.0</td>\n",
       "      <td>USC-32-Z-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>University of Southern California</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>20200723.0</td>\n",
       "      <td>WGL-117-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>WFA Group LLC</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   aut     changed            org_id source  \\\n",
       "0    1  20240618.0      LPL-141-ARIN   ARIN   \n",
       "1    2  20231108.0  UNIVER-19-Z-ARIN   ARIN   \n",
       "2    3  20100927.0        MIT-2-ARIN   ARIN   \n",
       "3    4  20230929.0     USC-32-Z-ARIN   ARIN   \n",
       "4    5  20200723.0      WGL-117-ARIN   ARIN   \n",
       "\n",
       "                                org_name country  \n",
       "0                    Level 3 Parent, LLC      US  \n",
       "1                 University of Delaware      US  \n",
       "2  Massachusetts Institute of Technology      US  \n",
       "3      University of Southern California      US  \n",
       "4                          WFA Group LLC      US  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "with open('/workspaces/pytorch-gpu-2/preprocessing/data/caida/20251001.as-org2info.txt', 'r', newline='', encoding='utf-8') as input_file:\n",
    "    lines = input_file.readlines()   \n",
    "    # Buffers initialisieren\n",
    "    aut_lines = []\n",
    "    org_lines = []\n",
    "    mode = None\n",
    "    total_lines = len(lines)\n",
    "    aut_count = 0\n",
    "    org_count = 0 \n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"# format:aut\"):\n",
    "            mode = \"aut\"\n",
    "            continue\n",
    "        elif line.startswith(\"# format:org_id\"):\n",
    "            mode = \"org\"\n",
    "            continue\n",
    "        elif line.startswith(\"#\") or not line:\n",
    "            # Andere Kommentar- oder Leerzeilen √ºberspringen\n",
    "            continue      \n",
    "        if mode == \"aut\":\n",
    "            aut_lines.append(line)\n",
    "            aut_count += 1\n",
    "        elif mode == \"org\":\n",
    "            org_lines.append(line)\n",
    "            org_count += 1\n",
    "    # StringIO-Objekte aus den gesammelten Zeilen bauen\n",
    "    aut_buffer = io.StringIO(\"\\n\".join(aut_lines))\n",
    "    org_buffer = io.StringIO(\"\\n\".join(org_lines))\n",
    "    # DataFrames einlesen\n",
    "    aut_df = pd.read_csv(aut_buffer, sep=\"|\",\n",
    "                        names=[\"aut\", \"changed\", \"aut_name\", \"org_id\", \"opaque_id\", \"source\"], usecols=[\"aut\", \"org_id\", \"source\", \"changed\"])\n",
    "    org_df = pd.read_csv(org_buffer, sep=\"|\",\n",
    "                        names=[\"org_id\", \"changed\", \"org_name\", \"country\", \"source\"], usecols=[\"org_id\", \"org_name\", \"country\"])\n",
    "\n",
    "    # Join the DataFrames\n",
    "    joined_df = pd.merge(aut_df, org_df, on=\"org_id\", how=\"left\")\n",
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73e5047",
   "metadata": {},
   "source": [
    "## Join both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6de8ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "asn",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "org_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "country",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "source",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "info_type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "8d7be83b-3b5c-479f-8e65-a34506fba5f2",
       "rows": [
        [
         "0",
         "4436",
         "GTT Americas, LLC",
         "US",
         "ARIN",
         "NSP"
        ],
        [
         "1",
         "20940",
         "Akamai International B.V.",
         "NL",
         "RIPE",
         "Content"
        ],
        [
         "2",
         "31800",
         "DALnet",
         "US",
         "ARIN",
         "Non-Profit"
        ],
        [
         "3",
         "3303",
         "Swisscom (Schweiz) AG",
         "CH",
         "RIPE",
         "Cable/DSL/ISP"
        ],
        [
         "4",
         "22773",
         "Cox Communications Inc.",
         "US",
         "ARIN",
         "Cable/DSL/ISP"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asn</th>\n",
       "      <th>org_name</th>\n",
       "      <th>country</th>\n",
       "      <th>source</th>\n",
       "      <th>info_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4436</td>\n",
       "      <td>GTT Americas, LLC</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>NSP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20940</td>\n",
       "      <td>Akamai International B.V.</td>\n",
       "      <td>NL</td>\n",
       "      <td>RIPE</td>\n",
       "      <td>Content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31800</td>\n",
       "      <td>DALnet</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Non-Profit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3303</td>\n",
       "      <td>Swisscom (Schweiz) AG</td>\n",
       "      <td>CH</td>\n",
       "      <td>RIPE</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22773</td>\n",
       "      <td>Cox Communications Inc.</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     asn                   org_name country source      info_type\n",
       "0   4436          GTT Americas, LLC      US   ARIN            NSP\n",
       "1  20940  Akamai International B.V.      NL   RIPE        Content\n",
       "2  31800                     DALnet      US   ARIN     Non-Profit\n",
       "3   3303      Swisscom (Schweiz) AG      CH   RIPE  Cable/DSL/ISP\n",
       "4  22773    Cox Communications Inc.      US   ARIN  Cable/DSL/ISP"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peering_df_joined = pd.merge(net_df, joined_df, left_on='asn', right_on='aut', how='left')\n",
    "peering_df_joined = peering_df_joined[['asn', 'org_name', 'country', 'source', 'info_type']]\n",
    "peering_df_joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5205f93a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "info_type",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "5aa870fb-b24e-4faa-b940-66ac1e5de5da",
       "rows": [
        [
         "Access",
         "11787"
        ],
        [
         "Transit",
         "3982"
        ],
        [
         "Content",
         "2486"
        ],
        [
         "Enterprise",
         "2460"
        ],
        [
         "Network Services",
         "1458"
        ],
        [
         "Education/Research",
         "1457"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 6
       }
      },
      "text/plain": [
       "Access                11787\n",
       "Transit                3982\n",
       "Content                2486\n",
       "Enterprise             2460\n",
       "Network Services       1458\n",
       "Education/Research     1457\n",
       "Name: info_type, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_map = {\n",
    "    \"NSP\": \"Transit\",\n",
    "    \"Content\": \"Content\",\n",
    "    \"Cable/DSL/ISP\": \"Access\",\n",
    "    \"Enterprise\": \"Enterprise\",\n",
    "    \"Educational/Research\": \"Education/Research\",\n",
    "    \"Non-Profit\": \"Enterprise\",\n",
    "    \"Government\": \"Enterprise\",\n",
    "    \"Route Server\": \"Network Services\",\n",
    "    \"Route Collector\": \"Network Services\",\n",
    "    \"Network Services\": \"Network Services\",\n",
    "    \"Not-Disclosed\": \"Unknown\"\n",
    "}\n",
    "\n",
    "peering_df_joined[\"info_type\"] = (\n",
    "    peering_df_joined[\"info_type\"]\n",
    "    .map(category_map)\n",
    "    .fillna(peering_df_joined[\"info_type\"])\n",
    ")\n",
    "peering_df_joined[\"info_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfbbfeb",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f565817d",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fce41b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SVM (calibrated) ===\n",
      "Accuracy: 0.58984375\n",
      "Macro-F1: 0.43285540823646373\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "            Access       0.63      0.92      0.75      1532\n",
      "           Content       0.45      0.33      0.38       323\n",
      "Education/Research       0.67      0.49      0.56       189\n",
      "        Enterprise       0.43      0.25      0.31       320\n",
      "  Network Services       0.64      0.25      0.36       190\n",
      "           Transit       0.40      0.16      0.23       518\n",
      "\n",
      "          accuracy                           0.59      3072\n",
      "         macro avg       0.54      0.40      0.43      3072\n",
      "      weighted avg       0.55      0.59      0.54      3072\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# ==== Daten ====\n",
    "df = peering_df_joined.copy()\n",
    "df[\"org_name\"] = df[\"org_name\"].fillna(\"unknown\").str.lower()\n",
    "valid = df[\"info_type\"].value_counts()\n",
    "df = df[df[\"info_type\"].isin(valid[valid >= 5].index)]  # sehr kleine Klassen raus (optional)\n",
    "\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    df[\"org_name\"], df[\"info_type\"], test_size=0.13, random_state=42, stratify=df[\"info_type\"]\n",
    ")\n",
    "\n",
    "# Gemeinsamer Vectorizer (fit nur auf Train!)\n",
    "vec = TfidfVectorizer(analyzer=\"char\", ngram_range=(1,6),\n",
    "                      lowercase=True, min_df=1, sublinear_tf=True)\n",
    "\n",
    "# ==== 1) SVM + Kalibrierung ====\n",
    "svm = LinearSVC(C=0.35, class_weight=\"balanced\")\n",
    "svm_cal = CalibratedClassifierCV(svm, method=\"sigmoid\", cv=3)\n",
    "\n",
    "svm_pipe = Pipeline([\n",
    "    (\"tfidf\", vec),\n",
    "    (\"svm_cal\", svm_cal)\n",
    "])\n",
    "\n",
    "svm_pipe.fit(X_train_text, y_train)\n",
    "y_pred_svm = svm_pipe.predict(X_test_text)\n",
    "print(\"\\n=== SVM (calibrated) ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"Macro-F1:\", f1_score(y_test, y_pred_svm, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a706a136",
   "metadata": {},
   "source": [
    "## Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04542519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: 6 Klassen -> ['Access', 'Content', 'Education/Research', 'Enterprise', 'Network Services', 'Transit']\n",
      "Device: cuda\n",
      "GPU-Name: NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [0.202 0.959 1.636 0.969 1.636 0.599]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5144' max='12860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5144/12860 23:13 < 34:50, 3.69 it/s, Epoch 8/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.626100</td>\n",
       "      <td>1.544434</td>\n",
       "      <td>0.509766</td>\n",
       "      <td>0.308875</td>\n",
       "      <td>0.359607</td>\n",
       "      <td>0.367892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.492700</td>\n",
       "      <td>1.415287</td>\n",
       "      <td>0.510417</td>\n",
       "      <td>0.414821</td>\n",
       "      <td>0.405639</td>\n",
       "      <td>0.457370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.365000</td>\n",
       "      <td>1.356411</td>\n",
       "      <td>0.493815</td>\n",
       "      <td>0.418727</td>\n",
       "      <td>0.402557</td>\n",
       "      <td>0.462343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.354000</td>\n",
       "      <td>1.371389</td>\n",
       "      <td>0.484049</td>\n",
       "      <td>0.430136</td>\n",
       "      <td>0.447004</td>\n",
       "      <td>0.476403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.286300</td>\n",
       "      <td>1.350164</td>\n",
       "      <td>0.497070</td>\n",
       "      <td>0.430287</td>\n",
       "      <td>0.410670</td>\n",
       "      <td>0.486535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.214000</td>\n",
       "      <td>1.357503</td>\n",
       "      <td>0.500326</td>\n",
       "      <td>0.447261</td>\n",
       "      <td>0.427028</td>\n",
       "      <td>0.492556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.245000</td>\n",
       "      <td>1.394686</td>\n",
       "      <td>0.464844</td>\n",
       "      <td>0.424991</td>\n",
       "      <td>0.423324</td>\n",
       "      <td>0.477538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.177100</td>\n",
       "      <td>1.448541</td>\n",
       "      <td>0.465495</td>\n",
       "      <td>0.423897</td>\n",
       "      <td>0.419283</td>\n",
       "      <td>0.466403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval: {'eval_loss': 1.357502818107605, 'eval_accuracy': 0.5003255208333334, 'eval_f1_macro': 0.44726056996683994, 'eval_precision': 0.42702826333251176, 'eval_recall': 0.49255561938059694, 'eval_runtime': 1.2179, 'eval_samples_per_second': 2522.412, 'eval_steps_per_second': 78.825, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('xlmr_org_trainer_out/model/tokenizer_config.json',\n",
       " 'xlmr_org_trainer_out/model/special_tokens_map.json',\n",
       " 'xlmr_org_trainer_out/model/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Ersatz f√ºr den HF-Datasets-Teil (kein pyarrow/datasets n√∂tig) ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding, TrainingArguments, Trainer,\n",
    "                          EarlyStoppingCallback, TextClassificationPipeline)\n",
    "\n",
    "# --------- Konfig ---------\n",
    "MODEL_NAME   = \"xlm-roberta-base\"   # multilingual, starkes Baseline-Modell\n",
    "MAX_LENGTH   = 64                   # Org-Namen sind kurz -> 64 reicht\n",
    "LR           = 1e-5\n",
    "EPOCHS       = 20\n",
    "BATCH_SIZE   = 32\n",
    "WARMUP_RATIO = 0.06\n",
    "SEED         = 42\n",
    "OUT_DIR      = \"xlmr_org_trainer_out\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "le = LabelEncoder()\n",
    "\n",
    "le = LabelEncoder()\n",
    "df = peering_df_joined\n",
    "df[\"label_id\"] = le.fit_transform(df[\"info_type\"])\n",
    "num_labels = len(le.classes_)\n",
    "print(f\"Labels: {num_labels} Klassen ->\", list(le.classes_))\n",
    "\n",
    "df[\"label_id\"] = le.fit_transform(df[\"info_type\"])\n",
    "\n",
    "\n",
    "df.fillna('Unknown', inplace=True)\n",
    "\n",
    "# Pr√ºfe GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU-Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Warnung: Keine GPU verf√ºgbar, CPU wird verwendet.\")\n",
    "\n",
    "# Train/Validation Split (stratifiziert)\n",
    "train_df, eval_df = train_test_split(\n",
    "    df[[\"org_name\", \"label_id\"]],\n",
    "    test_size=0.13,\n",
    "    random_state=SEED,\n",
    "    stratify=df[\"label_id\"]\n",
    ")\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "eval_df  = eval_df.reset_index(drop=True)\n",
    "\n",
    "# Texte & Labels aus den bereits vorbereiteten DataFrames (train_df, eval_df)\n",
    "train_texts = train_df[\"org_name\"].tolist()\n",
    "eval_texts  = eval_df[\"org_name\"].tolist()\n",
    "y_train_np  = train_df[\"label_id\"].to_numpy()\n",
    "y_eval_np   = eval_df[\"label_id\"].to_numpy()\n",
    "num_labels  = df[\"label_id\"].nunique()\n",
    "\n",
    "\n",
    "\n",
    "# Tokenisierung OHNE Padding (Padding macht sp√§ter der DataCollator)\n",
    "train_enc = tok(train_texts, truncation=True, max_length=MAX_LENGTH)\n",
    "eval_enc  = tok(eval_texts,  truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "class SimpleHFLikeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.enc = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(int(self.labels[idx]))\n",
    "        return item\n",
    "\n",
    "ds_train = SimpleHFLikeDataset(train_enc, y_train_np)\n",
    "ds_eval  = SimpleHFLikeDataset(eval_enc,  y_eval_np)\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tok)\n",
    "\n",
    "valid_classes = sorted(df[\"info_type\"].unique())\n",
    "\n",
    "# ---- Modell + Class Weights wie gehabt ----\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label={int(i): c for i, c in enumerate(valid_classes)},\n",
    "    label2id={c: int(i) for i, c in enumerate(valid_classes)}\n",
    ").to(device)\n",
    "\n",
    "# Class-Weights aus dem Trainingssplit\n",
    "class_counts = np.bincount(y_train_np, minlength=num_labels)\n",
    "weights = class_counts.sum() / np.maximum(class_counts, 1)\n",
    "weights = weights / weights.mean()\n",
    "class_weights = torch.tensor(weights, dtype=torch.float, device=device)\n",
    "print(\"Class weights:\", np.round(weights, 3))\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**{k:v for k,v in inputs.items() if k != \"labels\"})\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "        loss = loss_fct(logits.view(-1, num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUT_DIR + \"/checkpoints\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    fp16=(device.type==\"cuda\"),\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    seed=SEED,\n",
    "    report_to=[\"none\"],\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "    return {\n",
    "        \"accuracy\":  float(accuracy_score(labels, preds)),\n",
    "        \"f1_macro\":  float(f1_score(labels, preds, average=\"macro\")),\n",
    "        \"precision\": float(precision_score(labels, preds, average=\"macro\", zero_division=0)),\n",
    "        \"recall\":    float(recall_score(labels, preds, average=\"macro\")),\n",
    "    }\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_eval,\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Eval:\", metrics)\n",
    "\n",
    "trainer.save_model(OUT_DIR + \"/model\")\n",
    "tok.save_pretrained(OUT_DIR + \"/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa48be66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ensemble-Gewichtssuche ===\n",
      "Bestes w (SVM-Anteil): 0.50 | Macro-F1: 0.4886 | Acc: 0.5977\n",
      "\n",
      "=== Ensemble (SVM^w + XLM-R^(1-w)) auf Eval ===\n",
      "Accuracy: 0.59765625\n",
      "Macro-F1: 0.4885708991717628\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "            Access       0.72      0.81      0.76      1532\n",
      "           Content       0.40      0.48      0.44       323\n",
      "Education/Research       0.56      0.76      0.65       189\n",
      "        Enterprise       0.40      0.39      0.40       320\n",
      "  Network Services       0.56      0.35      0.43       190\n",
      "           Transit       0.39      0.20      0.26       518\n",
      "\n",
      "          accuracy                           0.60      3072\n",
      "         macro avg       0.51      0.50      0.49      3072\n",
      "      weighted avg       0.58      0.60      0.58      3072\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==== Ensemble aus kalibrierter SVM + XLM-R (Late Fusion) ====\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# 1) Hilfsfunktionen: Probas holen und Klassenreihenfolge erzwingen\n",
    "label_names = list(valid_classes)  # <- gleiche Reihenfolge wie beim HF-Modell (id2label/label2id)\n",
    "label_index = {lbl: i for i, lbl in enumerate(label_names)}\n",
    "\n",
    "def svm_proba(texts):\n",
    "    \"\"\"Kalibrierte SVM-Probas in label_names-Reihenfolge.\"\"\"\n",
    "    # svm_pipe.classes_ enth√§lt die Klassenreihenfolge des SVM-Teils\n",
    "    svm_labels = list(svm_pipe.named_steps[\"svm_cal\"].classes_) if hasattr(svm_pipe.named_steps[\"svm_cal\"], \"classes_\") \\\n",
    "                 else list(svm_pipe.classes_)\n",
    "    proba = svm_pipe.predict_proba(texts)  # shape: [N, n_classes_svm]\n",
    "    # Auf label_names umsortieren\n",
    "    idx_map = [svm_labels.index(lbl) for lbl in label_names]\n",
    "    proba_sorted = proba[:, idx_map]\n",
    "    return proba_sorted\n",
    "\n",
    "@torch.no_grad()\n",
    "def xlmr_proba(texts, batch_size=64, max_length=256):\n",
    "    \"\"\"Transformer-Softmax-Probas in label_names-Reihenfolge (id2label stimmt auf valid_classes).\"\"\"\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tok(batch, truncation=True, max_length=max_length, padding=True, return_tensors=\"pt\")\n",
    "        enc = {k: v.to(model.device) for k, v in enc.items()}\n",
    "        logits = model(**enc).logits  # [B, num_labels]\n",
    "        probs = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "        all_probs.append(probs)\n",
    "    return np.vstack(all_probs)  # [N, num_labels] schon in label_names-Reihenfolge\n",
    "\n",
    "# 2) Probas auf deinem (Ensemble-)Validierungs/Testsplit erzeugen\n",
    "X_eval = eval_df[\"org_name\"].tolist()\n",
    "y_eval = y_eval_np  # ints passend zur Reihenfolge in label_names\n",
    "\n",
    "P_svm  = svm_proba(X_eval)              # [N, C]\n",
    "P_xlmr = xlmr_proba(X_eval, max_length=MAX_LENGTH)  # [N, C]\n",
    "\n",
    "# 3) Gewicht per einfacher Grid-Search finden (0..1)\n",
    "grid = np.linspace(0.0, 1.0, 21)  # 0.00, 0.05, ..., 1.00\n",
    "best = {\"w\": None, \"f1\": -1.0, \"acc\": 0.0}\n",
    "\n",
    "for w in grid:\n",
    "    P_ens = w * P_svm + (1.0 - w) * P_xlmr\n",
    "    y_hat = P_ens.argmax(axis=1)\n",
    "    f1 = f1_score(y_eval, y_hat, average=\"macro\")\n",
    "    acc = accuracy_score(y_eval, y_hat)\n",
    "    if f1 > best[\"f1\"] or (f1 == best[\"f1\"] and acc > best[\"acc\"]):\n",
    "        best.update({\"w\": float(w), \"f1\": float(f1), \"acc\": float(acc)})\n",
    "\n",
    "print(f\"\\n=== Ensemble-Gewichtssuche ===\")\n",
    "print(f\"Bestes w (SVM-Anteil): {best['w']:.2f} | Macro-F1: {best['f1']:.4f} | Acc: {best['acc']:.4f}\")\n",
    "\n",
    "# 4) Finale Ensemble-Vorhersage + Report\n",
    "w = best[\"w\"]\n",
    "P_ens = w * P_svm + (1.0 - w) * P_xlmr\n",
    "y_pred = P_ens.argmax(axis=1)\n",
    "\n",
    "print(\"\\n=== Ensemble (SVM^w + XLM-R^(1-w)) auf Eval ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_eval, y_pred))\n",
    "print(\"Macro-F1:\", f1_score(y_eval, y_pred, average=\"macro\"))\n",
    "print(classification_report(y_eval, y_pred, target_names=label_names))\n",
    "\n",
    "# 5) Praktische Inferenzfunktion f√ºrs sp√§tere Nutzen\n",
    "def ensemble_predict(texts, return_proba=False, batch_size=64):\n",
    "    Ps = svm_proba(texts)\n",
    "    Pt = xlmr_proba(texts, batch_size=batch_size, max_length=MAX_LENGTH)\n",
    "    P = w * Ps + (1.0 - w) * Pt\n",
    "    preds = P.argmax(axis=1)\n",
    "    if return_proba:\n",
    "        return preds, P\n",
    "    return preds\n",
    "\n",
    "# Beispiel:\n",
    "# preds, proba = ensemble_predict([\"google llc\", \"university of oxford\"], return_proba=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3094bc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5846354166666666\n",
      "Macro-F1: 0.340165375175505\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       Cable/DSL/ISP       0.62      0.92      0.74      1532\n",
      "             Content       0.42      0.34      0.38       323\n",
      "Educational/Research       0.68      0.48      0.56       189\n",
      "          Enterprise       0.41      0.13      0.20       224\n",
      "          Government       0.50      0.25      0.33        16\n",
      "                 NSP       0.41      0.17      0.24       518\n",
      "    Network Services       0.00      0.00      0.00       105\n",
      "          Non-Profit       0.67      0.30      0.41        80\n",
      "     Route Collector       0.00      0.00      0.00         4\n",
      "        Route Server       0.62      0.48      0.54        81\n",
      "\n",
      "            accuracy                           0.58      3072\n",
      "           macro avg       0.43      0.31      0.34      3072\n",
      "        weighted avg       0.53      0.58      0.52      3072\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "\n",
    "# ==== Beispiel-Daten ====\n",
    "df = peering_df_joined.copy()\n",
    "# df[\"org_name\"], df[\"country\"], df[\"asn\"], df[\"ix_count\"], df[\"info_type\"]\n",
    "\n",
    "df[\"org_name\"] = df[\"org_name\"].fillna(\"unknown\").str.lower()\n",
    "\n",
    "# ==== Hilfsfunktion f√ºr Regex-Features aus dem Namen ====\n",
    "def regex_features(X):\n",
    "    X = X[\"org_name\"].astype(str).str.lower()\n",
    "    return pd.DataFrame({\n",
    "        \"has_isp\": X.str.contains(r\"isp|telecom|broadband|internet\").astype(int),\n",
    "        \"has_univ\": X.str.contains(r\"univ|college|schule|academy\").astype(int),\n",
    "        \"has_gov\": X.str.contains(r\"gov|ministerium|city|state|municipal\").astype(int),\n",
    "        \"has_ix\": X.str.contains(r\"\\bix\\b|exchange|route\").astype(int),\n",
    "        \"has_asn\": X.str.contains(r\"as\\d+\").astype(int),\n",
    "    })\n",
    "\n",
    "# ==== Spalten definieren ====\n",
    "text_col = \"org_name\"\n",
    "regex_col = [\"org_name\"]\n",
    "\n",
    "# ==== ColumnTransformer aufbauen ====\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"tfidf\", TfidfVectorizer(analyzer=\"char\", ngram_range=(2,6), sublinear_tf=True), text_col),\n",
    "    (\"regex\", FunctionTransformer(regex_features), regex_col)\n",
    "])\n",
    "\n",
    "# ==== Modell ====\n",
    "svm = LinearSVC(C=0.35, class_weight=\"balanced\")\n",
    "svm_cal = CalibratedClassifierCV(svm, method=\"sigmoid\", cv=3)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"features\", preprocessor),\n",
    "    (\"clf\", svm_cal)\n",
    "])\n",
    "\n",
    "# ==== Train/Test Split ====\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[[\"org_name\"]],\n",
    "    df[\"info_type\"],\n",
    "    test_size=0.13,\n",
    "    stratify=df[\"info_type\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ==== Trainieren + Bewerten ====\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Macro-F1:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3a81d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.1464868\ttotal: 997ms\tremaining: 16m 35s\n",
      "100:\tlearn: 0.3638968\ttotal: 1m 32s\tremaining: 13m 43s\n",
      "200:\tlearn: 0.3849558\ttotal: 3m 4s\tremaining: 12m 12s\n",
      "300:\tlearn: 0.4635967\ttotal: 4m 37s\tremaining: 10m 43s\n",
      "400:\tlearn: 0.4926904\ttotal: 6m 11s\tremaining: 9m 14s\n",
      "500:\tlearn: 0.5089481\ttotal: 7m 45s\tremaining: 7m 43s\n",
      "600:\tlearn: 0.5220419\ttotal: 9m 19s\tremaining: 6m 11s\n",
      "700:\tlearn: 0.5338786\ttotal: 10m 58s\tremaining: 4m 40s\n",
      "800:\tlearn: 0.5424692\ttotal: 12m 32s\tremaining: 3m 6s\n",
      "900:\tlearn: 0.5521177\ttotal: 14m 8s\tremaining: 1m 33s\n",
      "999:\tlearn: 0.5574348\ttotal: 15m 47s\tremaining: 0us\n",
      "Accuracy: 0.4248046875\n",
      "Macro-F1: 0.26779344876719907\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       Cable/DSL/ISP       0.78      0.55      0.65      1532\n",
      "             Content       0.25      0.45      0.32       323\n",
      "Educational/Research       0.45      0.39      0.41       189\n",
      "          Enterprise       0.20      0.28      0.23       224\n",
      "          Government       0.04      0.56      0.07        16\n",
      "                 NSP       0.35      0.18      0.24       518\n",
      "    Network Services       0.06      0.12      0.08       105\n",
      "          Non-Profit       0.29      0.29      0.29        80\n",
      "     Route Collector       0.00      0.00      0.00         4\n",
      "        Route Server       0.32      0.48      0.38        81\n",
      "\n",
      "            accuracy                           0.42      3072\n",
      "           macro avg       0.27      0.33      0.27      3072\n",
      "        weighted avg       0.53      0.42      0.46      3072\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "\n",
    "df = peering_df_joined.copy()\n",
    "df[\"org_name\"] = df[\"org_name\"].fillna(\"unknown\").str.lower()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[[\"org_name\"]],\n",
    "    df[\"info_type\"],\n",
    "    test_size=0.13,\n",
    "    stratify=df[\"info_type\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.05,\n",
    "    depth=8,\n",
    "    loss_function='MultiClass',\n",
    "    text_features=['org_name'],\n",
    "    auto_class_weights='Balanced',\n",
    "    eval_metric='TotalF1',\n",
    "    verbose=100\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Macro-F1:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred))\n",
    "#!/usr/bin/env python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0edc3411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: 10 Klassen -> ['Cable/DSL/ISP', 'Content', 'Educational/Research', 'Enterprise', 'Government', 'NSP', 'Network Services', 'Non-Profit', 'Route Collector', 'Route Server']\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:02<00:00, 37.04it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<00:00, 46.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights (LR): [ 0.2    0.95   1.621  1.373 18.689  0.593  2.941  3.857 76.141  3.793]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval: {'accuracy': 0.3688151041666667, 'f1_macro': 0.2633710874882087, 'precision': 0.24591686784053568, 'recall': 0.3843455165550887}\n",
      "\n",
      "Classification report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "       Cable/DSL/ISP       0.74      0.43      0.55      1532\n",
      "             Content       0.25      0.34      0.29       323\n",
      "Educational/Research       0.46      0.56      0.51       189\n",
      "          Enterprise       0.18      0.28      0.22       224\n",
      "          Government       0.09      0.50      0.16        16\n",
      "                 NSP       0.25      0.18      0.21       518\n",
      "    Network Services       0.06      0.15      0.08       105\n",
      "          Non-Profit       0.16      0.39      0.23        80\n",
      "     Route Collector       0.06      0.50      0.11         4\n",
      "        Route Server       0.19      0.51      0.28        81\n",
      "\n",
      "            accuracy                           0.37      3072\n",
      "           macro avg       0.25      0.38      0.26      3072\n",
      "        weighted avg       0.49      0.37      0.40      3072\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === SBERT-Embeddings + LogisticRegression-Klassifikator ===\n",
    "# Setzt voraus: pip install sentence-transformers scikit-learn joblib\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from joblib import dump, load\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --------- Konfig ---------\n",
    "MODEL_NAME   = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"  # sehr gutes Multilingual-SBERT\n",
    "MAX_LENGTH   = 64\n",
    "SEED         = 42\n",
    "OUT_DIR      = \"sbert_org_cls\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "rng = np.random.RandomState(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# --------- Daten ---------\n",
    "# Erwartet: peering_df_joined mit Spalten [\"org_name\", \"info_type\"]\n",
    "df = peering_df_joined.copy()\n",
    "df[\"org_name\"] = df[\"org_name\"].fillna(\"Unknown\").astype(str).str.strip()\n",
    "df[\"info_type\"] = df[\"info_type\"].fillna(\"Unknown\").astype(str).str.strip()\n",
    "\n",
    "# (Optional) leichte Normalisierung von Rechtsformen & Sonderzeichen\n",
    "def normalize_org_name(s: str) -> str:\n",
    "    s = re.sub(r\"[.,;:()\\-_/]+\", \" \", s)               # Satzzeichen -> Leerzeichen\n",
    "    s = re.sub(r\"\\b(ag|gmbh|mbh|ltd|llc|inc|s\\.a\\.|sa|sarl|co|kg|kgaa|se|oy|ab)\\b\", \"\", s, flags=re.I)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "df[\"org_name_norm\"] = df[\"org_name\"].map(normalize_org_name)\n",
    "\n",
    "# Label-Encoding\n",
    "le = LabelEncoder()\n",
    "df[\"label_id\"] = le.fit_transform(df[\"info_type\"])\n",
    "num_labels = len(le.classes_)\n",
    "print(f\"Labels: {num_labels} Klassen ->\", list(le.classes_))\n",
    "\n",
    "# Stratified Split\n",
    "train_df, eval_df = train_test_split(\n",
    "    df[[\"org_name_norm\", \"label_id\"]],\n",
    "    test_size=0.13,\n",
    "    random_state=SEED,\n",
    "    stratify=df[\"label_id\"]\n",
    ")\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "eval_df  = eval_df.reset_index(drop=True)\n",
    "\n",
    "# --------- SBERT laden & Embeddings berechnen ---------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "model = SentenceTransformer(MODEL_NAME, device=device)\n",
    "\n",
    "def embed(texts, batch_size=256):\n",
    "    # normalize_embeddings=True kann bei Cosine-√Ñhnlichkeit helfen ‚Äì f√ºr LR ist es optional.\n",
    "    return model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        convert_to_numpy=True,\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=False\n",
    "    )\n",
    "\n",
    "X_train = embed(train_df[\"org_name_norm\"].tolist())\n",
    "X_eval  = embed(eval_df[\"org_name_norm\"].tolist())\n",
    "y_train = train_df[\"label_id\"].to_numpy()\n",
    "y_eval  = eval_df[\"label_id\"].to_numpy()\n",
    "\n",
    "# --------- Klassifikator: Logistic Regression (mit Klassenbalancierung) ---------\n",
    "# Tipp: 'saga' oder 'lbfgs' funktionieren gut; C kann man via CV tunen.\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
    "class_weight_dict = {c: w for c, w in zip(classes, class_weights)}\n",
    "print(\"Class weights (LR):\", np.round(class_weights, 3))\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    multi_class=\"auto\",\n",
    "    solver=\"lbfgs\",\n",
    "    C=2.0,\n",
    "    max_iter=2000,\n",
    "    n_jobs=-1,\n",
    "    class_weight=class_weight_dict,\n",
    "    random_state=SEED,\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# --------- Evaluation ---------\n",
    "y_pred = clf.predict(X_eval)\n",
    "metrics = {\n",
    "    \"accuracy\":  float(accuracy_score(y_eval, y_pred)),\n",
    "    \"f1_macro\":  float(f1_score(y_eval, y_pred, average=\"macro\")),\n",
    "    \"precision\": float(precision_score(y_eval, y_pred, average=\"macro\", zero_division=0)),\n",
    "    \"recall\":    float(recall_score(y_eval, y_pred, average=\"macro\")),\n",
    "}\n",
    "print(\"Eval:\", metrics)\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_eval, y_pred, target_names=le.classes_))\n",
    "\n",
    "# --------- Persistenz ---------\n",
    "dump(clf, os.path.join(OUT_DIR, \"clf.joblib\"))\n",
    "dump(le,  os.path.join(OUT_DIR, \"label_encoder.joblib\"))\n",
    "\n",
    "# Hinweis: Das SBERT-Basis-Modell selbst wird nicht ver√§ndert. F√ºr Reproduzierbarkeit:\n",
    "with open(os.path.join(OUT_DIR, \"MODEL_NAME.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(MODEL_NAME)\n",
    "\n",
    "# --------- Inferenz-Funktion ---------\n",
    "def predict_org_classes(org_names):\n",
    "    names_norm = [normalize_org_name(x or \"Unknown\") for x in org_names]\n",
    "    X = embed(names_norm)\n",
    "    yhat = clf.predict(X)\n",
    "    labels = le.inverse_transform(yhat)\n",
    "    return labels\n",
    "\n",
    "# Beispiel:\n",
    "# print(predict_org_classes([\"Siemens AG\", \"Acme GmbH\", \"Universit√§t Z√ºrich\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "353b602b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: 10 Klassen -> ['Cable/DSL/ISP', 'Content', 'Educational/Research', 'Enterprise', 'Government', 'NSP', 'Network Services', 'Non-Profit', 'Route Collector', 'Route Server']\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:803: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81/81 [00:01<00:00, 52.61it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<00:00, 74.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights (LR): [ 0.2    0.95   1.621  1.373 18.689  0.593  2.941  3.857 76.141  3.793]\n",
      "Eval: {'accuracy': 0.2884114583333333, 'f1_macro': 0.21081945105245822, 'precision': 0.21046043930577016, 'recall': 0.36153178433530075}\n",
      "\n",
      "Classification report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "       Cable/DSL/ISP       0.73      0.30      0.43      1532\n",
      "             Content       0.27      0.33      0.30       323\n",
      "Educational/Research       0.30      0.48      0.37       189\n",
      "          Enterprise       0.16      0.24      0.19       224\n",
      "          Government       0.06      0.69      0.10        16\n",
      "                 NSP       0.24      0.15      0.19       518\n",
      "    Network Services       0.03      0.10      0.05       105\n",
      "          Non-Profit       0.10      0.33      0.15        80\n",
      "     Route Collector       0.04      0.50      0.08         4\n",
      "        Route Server       0.17      0.51      0.25        81\n",
      "\n",
      "            accuracy                           0.29      3072\n",
      "           macro avg       0.21      0.36      0.21      3072\n",
      "        weighted avg       0.47      0.29      0.33      3072\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === SBERT-Embeddings + LogisticRegression-Klassifikator ===\n",
    "# Setzt voraus: pip install sentence-transformers scikit-learn joblib\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from joblib import dump, load\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --------- Konfig ---------\n",
    "MODEL_NAME   = \"Vsevolod/company-names-similarity-sentence-transformer\"  # sehr gutes Multilingual-SBERT\n",
    "MAX_LENGTH   = 64\n",
    "SEED         = 42\n",
    "OUT_DIR      = \"sbert_org_cls\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "rng = np.random.RandomState(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# --------- Daten ---------\n",
    "# Erwartet: peering_df_joined mit Spalten [\"org_name\", \"info_type\"]\n",
    "df = peering_df_joined.copy()\n",
    "df[\"org_name\"] = df[\"org_name\"].fillna(\"Unknown\").astype(str).str.strip()\n",
    "df[\"info_type\"] = df[\"info_type\"].fillna(\"Unknown\").astype(str).str.strip()\n",
    "\n",
    "# (Optional) leichte Normalisierung von Rechtsformen & Sonderzeichen\n",
    "def normalize_org_name(s: str) -> str:\n",
    "    s = re.sub(r\"[.,;:()\\-_/]+\", \" \", s)               # Satzzeichen -> Leerzeichen\n",
    "    s = re.sub(r\"\\b(ag|gmbh|mbh|ltd|llc|inc|s\\.a\\.|sa|sarl|co|kg|kgaa|se|oy|ab)\\b\", \"\", s, flags=re.I)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "df[\"org_name_norm\"] = df[\"org_name\"].map(normalize_org_name)\n",
    "\n",
    "# Label-Encoding\n",
    "le = LabelEncoder()\n",
    "df[\"label_id\"] = le.fit_transform(df[\"info_type\"])\n",
    "num_labels = len(le.classes_)\n",
    "print(f\"Labels: {num_labels} Klassen ->\", list(le.classes_))\n",
    "\n",
    "# Stratified Split\n",
    "train_df, eval_df = train_test_split(\n",
    "    df[[\"org_name_norm\", \"label_id\"]],\n",
    "    test_size=0.13,\n",
    "    random_state=SEED,\n",
    "    stratify=df[\"label_id\"]\n",
    ")\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "eval_df  = eval_df.reset_index(drop=True)\n",
    "\n",
    "# --------- SBERT laden & Embeddings berechnen ---------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "model = SentenceTransformer(MODEL_NAME, device=device)\n",
    "\n",
    "def embed(texts, batch_size=256):\n",
    "    # normalize_embeddings=True kann bei Cosine-√Ñhnlichkeit helfen ‚Äì f√ºr LR ist es optional.\n",
    "    return model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        convert_to_numpy=True,\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=False\n",
    "    )\n",
    "\n",
    "X_train = embed(train_df[\"org_name_norm\"].tolist())\n",
    "X_eval  = embed(eval_df[\"org_name_norm\"].tolist())\n",
    "y_train = train_df[\"label_id\"].to_numpy()\n",
    "y_eval  = eval_df[\"label_id\"].to_numpy()\n",
    "\n",
    "# --------- Klassifikator: Logistic Regression (mit Klassenbalancierung) ---------\n",
    "# Tipp: 'saga' oder 'lbfgs' funktionieren gut; C kann man via CV tunen.\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
    "class_weight_dict = {c: w for c, w in zip(classes, class_weights)}\n",
    "print(\"Class weights (LR):\", np.round(class_weights, 3))\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    multi_class=\"auto\",\n",
    "    solver=\"lbfgs\",\n",
    "    C=2.0,\n",
    "    max_iter=2000,\n",
    "    n_jobs=-1,\n",
    "    class_weight=class_weight_dict,\n",
    "    random_state=SEED,\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# --------- Evaluation ---------\n",
    "y_pred = clf.predict(X_eval)\n",
    "metrics = {\n",
    "    \"accuracy\":  float(accuracy_score(y_eval, y_pred)),\n",
    "    \"f1_macro\":  float(f1_score(y_eval, y_pred, average=\"macro\")),\n",
    "    \"precision\": float(precision_score(y_eval, y_pred, average=\"macro\", zero_division=0)),\n",
    "    \"recall\":    float(recall_score(y_eval, y_pred, average=\"macro\")),\n",
    "}\n",
    "print(\"Eval:\", metrics)\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_eval, y_pred, target_names=le.classes_))\n",
    "\n",
    "# --------- Persistenz ---------\n",
    "dump(clf, os.path.join(OUT_DIR, \"clf.joblib\"))\n",
    "dump(le,  os.path.join(OUT_DIR, \"label_encoder.joblib\"))\n",
    "\n",
    "# Hinweis: Das SBERT-Basis-Modell selbst wird nicht ver√§ndert. F√ºr Reproduzierbarkeit:\n",
    "with open(os.path.join(OUT_DIR, \"MODEL_NAME.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(MODEL_NAME)\n",
    "\n",
    "# --------- Inferenz-Funktion ---------\n",
    "def predict_org_classes(org_names):\n",
    "    names_norm = [normalize_org_name(x or \"Unknown\") for x in org_names]\n",
    "    X = embed(names_norm)\n",
    "    yhat = clf.predict(X)\n",
    "    labels = le.inverse_transform(yhat)\n",
    "    return labels\n",
    "\n",
    "# Beispiel:\n",
    "# print(predict_org_classes([\"Siemens AG\", \"Acme GmbH\", \"Universit√§t Z√ºrich\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49c07603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: 10 -> ['Cable/DSL/ISP', 'Content', 'Educational/Research', 'Enterprise', 'Government', 'NSP', 'Network Services', 'Non-Profit', 'Route Collector', 'Route Server']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Üí Embeddings berechnen (Train)‚Ä¶\n",
      "‚Üí Embeddings berechnen (Eval)‚Ä¶\n",
      "Class weights: [ 0.2    0.95   1.621  1.373 18.689  0.593  2.941  3.857 76.141  3.793]\n",
      "Eval: {'accuracy': 0.37890625, 'f1_macro': 0.26589287257832417, 'precision': 0.25268507217483904, 'recall': 0.4105633100295777}\n",
      "[{'org_name': 'Siemens AG', 'pred_label': 'Enterprise', 'pred_id': 3, 'proba': 0.44211757165396004}, {'org_name': 'Harvard University', 'pred_label': 'Educational/Research', 'pred_id': 2, 'proba': 0.9297410543876621}, {'org_name': 'ACME GmbH', 'pred_label': 'Enterprise', 'pred_id': 3, 'proba': 0.2695421460279461}]\n"
     ]
    }
   ],
   "source": [
    "# ===================== Sentence-BERT (frozen) + klassischer Klassifikator =====================\n",
    "# Setup\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import joblib\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --------- Konfig ---------\n",
    "SENTENCE_MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"  # stark & multilingual\n",
    "BATCH_SIZE_EMB = 256         # Embedding-Batchsize (nur Encoding, kein Finetune)\n",
    "SEED          = 42\n",
    "TEST_SIZE     = 0.13\n",
    "OUT_DIR       = \"sbert_org_cls_out\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --------- Daten vorbereiten ---------\n",
    "df = peering_df_joined.copy()\n",
    "df[\"org_name\"] = df[\"org_name\"].fillna(\"Unknown\")\n",
    "df[\"info_type\"] = df[\"info_type\"].fillna(\"Unknown\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "df[\"label_id\"] = le.fit_transform(df[\"info_type\"])\n",
    "classes = le.classes_\n",
    "num_labels = len(classes)\n",
    "print(f\"Labels: {num_labels} -> {list(classes)}\")\n",
    "\n",
    "# Stratified Split\n",
    "train_df, eval_df = train_test_split(\n",
    "    df[[\"org_name\", \"label_id\"]],\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=SEED,\n",
    "    stratify=df[\"label_id\"]\n",
    ")\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "eval_df  = eval_df.reset_index(drop=True)\n",
    "\n",
    "X_train = train_df[\"org_name\"].tolist()\n",
    "y_train = train_df[\"label_id\"].to_numpy()\n",
    "X_eval  = eval_df[\"org_name\"].tolist()\n",
    "y_eval  = eval_df[\"label_id\"].to_numpy()\n",
    "\n",
    "# --------- SBERT Modell laden (nur Encoding) ---------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(SENTENCE_MODEL, device=device)\n",
    "\n",
    "# Helper: Batched Encoding\n",
    "def encode_batched(texts, batch_size=BATCH_SIZE_EMB):\n",
    "    embs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        with torch.inference_mode():\n",
    "            emb = model.encode(\n",
    "                batch,\n",
    "                batch_size=len(batch),  # SBERT handhabt intern batching; hier kein Overhead\n",
    "                convert_to_numpy=True,\n",
    "                normalize_embeddings=True  # i.d.R. gut f√ºr Cosine/LR\n",
    "            )\n",
    "        embs.append(emb)\n",
    "    return np.vstack(embs) if embs else np.empty((0, model.get_sentence_embedding_dimension()))\n",
    "\n",
    "print(\"‚Üí Embeddings berechnen (Train)‚Ä¶\")\n",
    "X_train_emb = encode_batched(X_train)\n",
    "print(\"‚Üí Embeddings berechnen (Eval)‚Ä¶\")\n",
    "X_eval_emb  = encode_batched(X_eval)\n",
    "\n",
    "# --------- Klassifikator (Logistic Regression) mit Klassen-Gewichten ---------\n",
    "cls_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.arange(num_labels),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = {i: w for i, w in enumerate(cls_weights)}\n",
    "print(\"Class weights:\", np.round(cls_weights, 3))\n",
    "\n",
    "# C und max_iter ggf. anpassen (Bremsen l√∂sen, falls Konvergenz-Warnung)\n",
    "clf = LogisticRegression(\n",
    "    multi_class=\"multinomial\",\n",
    "    solver=\"lbfgs\",\n",
    "    class_weight=class_weight_dict,\n",
    "    max_iter=2000,\n",
    "    n_jobs=-1,\n",
    "    random_state=SEED\n",
    ")\n",
    "clf.fit(X_train_emb, y_train)\n",
    "\n",
    "# --------- Evaluation ---------\n",
    "y_pred = clf.predict(X_eval_emb)\n",
    "metrics = {\n",
    "    \"accuracy\":  float(accuracy_score(y_eval, y_pred)),\n",
    "    \"f1_macro\":  float(f1_score(y_eval, y_pred, average=\"macro\")),\n",
    "    \"precision\": float(precision_score(y_eval, y_pred, average=\"macro\", zero_division=0)),\n",
    "    \"recall\":    float(recall_score(y_eval, y_pred, average=\"macro\")),\n",
    "}\n",
    "print(\"Eval:\", metrics)\n",
    "\n",
    "# --------- Speichern (Klassifikator + LabelEncoder + Meta) ---------\n",
    "joblib.dump(clf, os.path.join(OUT_DIR, \"clf.joblib\"))\n",
    "joblib.dump(le,  os.path.join(OUT_DIR, \"label_encoder.joblib\"))\n",
    "with open(os.path.join(OUT_DIR, \"sbert_model_name.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(SENTENCE_MODEL)\n",
    "\n",
    "# --------- Inferenz-Funktion ---------\n",
    "def predict_org_classes(org_names):\n",
    "    \"\"\"\n",
    "    org_names: List[str]\n",
    "    returns: List[{\"org_name\", \"pred_label\", \"pred_id\", \"proba\"}]\n",
    "    \"\"\"\n",
    "    if isinstance(org_names, str):\n",
    "        org_names = [org_names]\n",
    "\n",
    "    embs = encode_batched(org_names)\n",
    "    proba = clf.predict_proba(embs)\n",
    "    pred_ids = proba.argmax(axis=1)\n",
    "    pred_labels = le.inverse_transform(pred_ids)\n",
    "\n",
    "    out = []\n",
    "    for i, name in enumerate(org_names):\n",
    "        out.append({\n",
    "            \"org_name\": name,\n",
    "            \"pred_label\": pred_labels[i],\n",
    "            \"pred_id\": int(pred_ids[i]),\n",
    "            \"proba\": float(proba[i, pred_ids[i]])\n",
    "        })\n",
    "    return out\n",
    "\n",
    "# Beispiel:\n",
    "print(predict_org_classes([\"Siemens AG\", \"Harvard University\", \"ACME GmbH\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8649635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "SoftmaxLoss requires transformers >= 4.43.0 to work correctly. Otherwise, the classifier layer that maps embeddings to the labels cannot be updated. Consider updating transformers with `pip install transformers>=4.43.0`.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LabelAccuracyEvaluator.__init__() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_examples, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Evaluator (Accuracy auf Dev)\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLabelAccuracyEvaluator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_sents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdev\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     57\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# --- Training ---\u001b[39;00m\n\u001b[1;32m     59\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: LabelAccuracyEvaluator.__init__() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "# ================= SBERT Multiclass Fine-Tuning (SoftmaxLoss) =================\n",
    "import os, numpy as np, pandas as pd, torch, random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Modellwahl: multilingual & stark f√ºr kurze Texte\n",
    "SBERT_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "OUT_DIR = \"sbert_softmax_out\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Daten ---\n",
    "df = peering_df_joined.copy()\n",
    "df[\"org_name\"]  = df[\"org_name\"].fillna(\"Unknown\")\n",
    "df[\"info_type\"] = df[\"info_type\"].fillna(\"Unknown\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "df[\"label_id\"] = le.fit_transform(df[\"info_type\"])\n",
    "num_labels = df[\"label_id\"].nunique()\n",
    "\n",
    "train_df, eval_df = train_test_split(\n",
    "    df[[\"org_name\",\"label_id\"]],\n",
    "    test_size=0.13, random_state=SEED, stratify=df[\"label_id\"]\n",
    ")\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "eval_df  = eval_df.reset_index(drop=True)\n",
    "\n",
    "# --- InputExamples bauen ---\n",
    "train_examples = [InputExample(texts=[t], label=int(l)) for t,l in zip(train_df[\"org_name\"], train_df[\"label_id\"])]\n",
    "eval_sents  = eval_df[\"org_name\"].tolist()\n",
    "eval_labels = eval_df[\"label_id\"].tolist()\n",
    "\n",
    "# --- Modell + Loss ---\n",
    "model = SentenceTransformer(SBERT_NAME, device=DEVICE)\n",
    "loss_fn = losses.SoftmaxLoss(\n",
    "    model=model,\n",
    "    sentence_embedding_dimension=model.get_sentence_embedding_dimension(),\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "# Class weighting f√ºr kurzen Text hilft selten, aber EarlyStopping & Eval helfen\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_examples, shuffle=True, batch_size=batch_size, drop_last=False)\n",
    "\n",
    "# Evaluator (Accuracy auf Dev)\n",
    "evaluator = evaluation.LabelAccuracyEvaluator(\n",
    "    eval_sents,\n",
    "    labels=eval_labels,\n",
    "    name=\"dev\"\n",
    ")\n",
    "# --- Training ---\n",
    "epochs = 8\n",
    "warmup_ratio = 0.06\n",
    "eval_steps = max(1, len(train_loader)//2)  # 2x pro Epoche evaluieren\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_loader, loss_fn)],\n",
    "    evaluator=evaluator,\n",
    "    epochs=epochs,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    evaluation_steps=eval_steps,\n",
    "    output_path=OUT_DIR,\n",
    "    use_amp=torch.cuda.is_available(),\n",
    "    checkpoint_path=os.path.join(OUT_DIR, \"ckpts\"),\n",
    "    checkpoint_save_steps=eval_steps,\n",
    ")\n",
    "\n",
    "# --- Eval (Macro-F1 etc.) ---\n",
    "model = SentenceTransformer(OUT_DIR, device=DEVICE)  # bestes Modell laden\n",
    "with torch.inference_mode():\n",
    "    X_eval_emb = model.encode(eval_sents, batch_size=512, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "# Ein lineares K√∂pfchen f√ºr reporting (optional)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(\n",
    "    multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=2000, n_jobs=-1, random_state=SEED\n",
    ").fit(\n",
    "    model.encode(train_df[\"org_name\"].tolist(), batch_size=512, convert_to_numpy=True, normalize_embeddings=True),\n",
    "    train_df[\"label_id\"].to_numpy()\n",
    ")\n",
    "y_pred = clf.predict(X_eval_emb)\n",
    "print({\n",
    "    \"accuracy\":  float(accuracy_score(eval_labels, y_pred)),\n",
    "    \"f1_macro\":  float(f1_score(eval_labels, y_pred, average=\"macro\")),\n",
    "    \"precision\": float(precision_score(eval_labels, y_pred, average=\"macro\", zero_division=0)),\n",
    "    \"recall\":    float(recall_score(eval_labels, y_pred, average=\"macro\")),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55906fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Stacked Eval:\n",
      "{'accuracy': 0.453125, 'f1_macro': 0.3560013073085809, 'precision': 0.32372381753312884, 'recall': 0.4334219119795401}\n",
      "\n",
      "Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "       Cable/DSL/ISP       0.77      0.52      0.62      1532\n",
      "             Content       0.32      0.45      0.37       323\n",
      "Educational/Research       0.45      0.65      0.53       189\n",
      "          Enterprise       0.24      0.36      0.29       224\n",
      "          Government       0.31      0.56      0.40        16\n",
      "                 NSP       0.32      0.30      0.31       518\n",
      "    Network Services       0.05      0.10      0.06       105\n",
      "          Non-Profit       0.30      0.39      0.34        80\n",
      "     Route Collector       0.17      0.50      0.25         4\n",
      "        Route Server       0.32      0.52      0.40        81\n",
      "\n",
      "            accuracy                           0.45      3072\n",
      "           macro avg       0.32      0.43      0.36      3072\n",
      "        weighted avg       0.54      0.45      0.48      3072\n",
      "\n",
      "SVM Stacked Eval:\n",
      "{'accuracy': 0.5237630208333334, 'f1_macro': 0.3650172153966057, 'precision': 0.35364058358842787, 'recall': 0.38332780434399794}\n",
      "\n",
      "Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "       Cable/DSL/ISP       0.73      0.71      0.72      1532\n",
      "             Content       0.32      0.36      0.34       323\n",
      "Educational/Research       0.47      0.59      0.52       189\n",
      "          Enterprise       0.24      0.26      0.25       224\n",
      "          Government       0.40      0.38      0.39        16\n",
      "                 NSP       0.35      0.28      0.31       518\n",
      "    Network Services       0.09      0.10      0.10       105\n",
      "          Non-Profit       0.36      0.34      0.35        80\n",
      "     Route Collector       0.17      0.25      0.20         4\n",
      "        Route Server       0.41      0.57      0.47        81\n",
      "\n",
      "            accuracy                           0.52      3072\n",
      "           macro avg       0.35      0.38      0.37      3072\n",
      "        weighted avg       0.53      0.52      0.53      3072\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ================= Stacking: SBERT + Char TF-IDF =================\n",
    "import os, numpy as np, pandas as pd, torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SBERT_NAME = \"Vsevolod/company-names-similarity-sentence-transformer\"\n",
    "OUT_DIR = \"sbert2_tfidf_stack_out\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "df = peering_df_joined.copy()\n",
    "df[\"org_name\"]  = df[\"org_name\"].fillna(\"Unknown\")\n",
    "df[\"info_type\"] = df[\"info_type\"].fillna(\"Unknown\")\n",
    "\n",
    "# Einfache Normalisierung (hilft stark):\n",
    "def normalize_org(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    s = s.replace(\"&\", \" und \")\n",
    "    # Rechtsformen entfernen/vereinheitlichen\n",
    "    for suf in [\" gmbh\", \" ag\", \" inc\", \" ltd\", \" llc\", \" s.a.\", \" s.a\", \" srl\", \" co.\", \" co\", \" company\"]:\n",
    "        if s.lower().endswith(suf):\n",
    "            s = s[: -len(suf)]\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "df[\"org_norm\"] = df[\"org_name\"].astype(str).map(normalize_org)\n",
    "\n",
    "le = LabelEncoder()\n",
    "df[\"label_id\"] = le.fit_transform(df[\"info_type\"])\n",
    "y = df[\"label_id\"].to_numpy()\n",
    "\n",
    "train_df, eval_df = train_test_split(\n",
    "    df[[\"org_name\",\"org_norm\",\"label_id\"]],\n",
    "    test_size=0.13, random_state=SEED, stratify=df[\"label_id\"]\n",
    ")\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "eval_df  = eval_df.reset_index(drop=True)\n",
    "\n",
    "# --- SBERT embeddings ---\n",
    "model = SentenceTransformer(SBERT_NAME, device=DEVICE)\n",
    "with torch.inference_mode():\n",
    "    X_train_emb = model.encode(train_df[\"org_name\"].tolist(), batch_size=512, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    X_eval_emb  = model.encode(eval_df[\"org_name\"].tolist(),  batch_size=512, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "# --- Char TF-IDF (3‚Äì5-gram ist ein robuster Sweet-Spot) ---\n",
    "tfidf = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5), min_df=2)\n",
    "X_train_tf = tfidf.fit_transform(train_df[\"org_norm\"])\n",
    "X_eval_tf  = tfidf.transform(eval_df[\"org_norm\"])\n",
    "\n",
    "# --- Stack: [TF-IDF | SBERT] ---\n",
    "X_train_stack = hstack([X_train_tf, csr_matrix(X_train_emb)], format=\"csr\")\n",
    "X_eval_stack  = hstack([X_eval_tf,  csr_matrix(X_eval_emb)],  format=\"csr\")\n",
    "\n",
    "# --- Class weights ---\n",
    "num_labels = df[\"label_id\"].nunique()\n",
    "cls_w = compute_class_weight(\"balanced\", classes=np.arange(num_labels), y=train_df[\"label_id\"].to_numpy())\n",
    "cls_w = {i:w for i,w in enumerate(cls_w)}\n",
    "\n",
    "# --- Klassifikator ---\n",
    "clf = LogisticRegression(\n",
    "    multi_class=\"multinomial\",\n",
    "    solver=\"lbfgs\",\n",
    "    class_weight=cls_w,\n",
    "    max_iter=3000,\n",
    "    n_jobs=-1,\n",
    "    random_state=SEED\n",
    ")\n",
    "clf.fit(X_train_stack, train_df[\"label_id\"].to_numpy())\n",
    "\n",
    "y_pred = clf.predict(X_eval_stack)\n",
    "print(\"Logistic Regression Stacked Eval:\")\n",
    "print({\n",
    "    \"accuracy\":  float(accuracy_score(eval_df[\"label_id\"], y_pred)),\n",
    "    \"f1_macro\":  float(f1_score(eval_df[\"label_id\"], y_pred, average=\"macro\")),\n",
    "    \"precision\": float(precision_score(eval_df[\"label_id\"], y_pred, average=\"macro\", zero_division=0)),\n",
    "    \"recall\":    float(recall_score(eval_df[\"label_id\"], y_pred, average=\"macro\")),\n",
    "})\n",
    "print(\"\\nClassification Report:\\n\", classification_report(eval_df[\"label_id\"], y_pred, target_names=le.classes_))\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "svm = LinearSVC(\n",
    "    C=1.0,\n",
    "    class_weight=cls_w,      # wie oben berechnet\n",
    "    loss=\"squared_hinge\",\n",
    "    dual=True,               # bei vielen Features (TF-IDF) meist besser\n",
    "    random_state=SEED\n",
    ")\n",
    "svm.fit(X_train_stack, train_df[\"label_id\"].to_numpy())\n",
    "\n",
    "y_pred = svm.predict(X_eval_stack)\n",
    "print(\"SVM Stacked Eval:\")\n",
    "print({\n",
    "    \"accuracy\":  float(accuracy_score(eval_df[\"label_id\"], y_pred)),\n",
    "    \"f1_macro\":  float(f1_score(eval_df[\"label_id\"], y_pred, average=\"macro\")),\n",
    "    \"precision\": float(precision_score(eval_df[\"label_id\"], y_pred, average=\"macro\", zero_division=0)),\n",
    "    \"recall\":    float(recall_score(eval_df[\"label_id\"], y_pred, average=\"macro\")),\n",
    "})\n",
    "print(\"\\nClassification Report:\\n\", classification_report(eval_df[\"label_id\"], y_pred, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aeebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= Stacking: SBERT + Char TF-IDF =================\n",
    "import os, numpy as np, pandas as pd, torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SBERT_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "OUT_DIR = \"sbert_tfidf_stack_out\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "df = peering_df_joined.copy()\n",
    "df[\"org_name\"]  = df[\"org_name\"].fillna(\"Unknown\")\n",
    "df[\"info_type\"] = df[\"info_type\"].fillna(\"Unknown\")\n",
    "\n",
    "# Einfache Normalisierung (hilft stark):\n",
    "def normalize_org(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    s = s.replace(\"&\", \" und \")\n",
    "    # Rechtsformen entfernen/vereinheitlichen\n",
    "    for suf in [\" gmbh\", \" ag\", \" inc\", \" ltd\", \" llc\", \" s.a.\", \" s.a\", \" srl\", \" co.\", \" co\", \" company\"]:\n",
    "        if s.lower().endswith(suf):\n",
    "            s = s[: -len(suf)]\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "df[\"org_norm\"] = df[\"org_name\"].astype(str).map(normalize_org)\n",
    "\n",
    "le = LabelEncoder()\n",
    "df[\"label_id\"] = le.fit_transform(df[\"info_type\"])\n",
    "y = df[\"label_id\"].to_numpy()\n",
    "\n",
    "train_df, eval_df = train_test_split(\n",
    "    df[[\"org_name\",\"org_norm\",\"label_id\"]],\n",
    "    test_size=0.13, random_state=SEED, stratify=df[\"label_id\"]\n",
    ")\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "eval_df  = eval_df.reset_index(drop=True)\n",
    "\n",
    "# --- SBERT embeddings ---\n",
    "model = SentenceTransformer(SBERT_NAME, device=DEVICE)\n",
    "with torch.inference_mode():\n",
    "    X_train_emb = model.encode(train_df[\"org_name\"].tolist(), batch_size=512, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    X_eval_emb  = model.encode(eval_df[\"org_name\"].tolist(),  batch_size=512, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "# --- Char TF-IDF (3‚Äì5-gram ist ein robuster Sweet-Spot) ---\n",
    "tfidf = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5), min_df=2)\n",
    "X_train_tf = tfidf.fit_transform(train_df[\"org_norm\"])\n",
    "X_eval_tf  = tfidf.transform(eval_df[\"org_norm\"])\n",
    "\n",
    "# --- Stack: [TF-IDF | SBERT] ---\n",
    "X_train_stack = hstack([X_train_tf, csr_matrix(X_train_emb)], format=\"csr\")\n",
    "X_eval_stack  = hstack([X_eval_tf,  csr_matrix(X_eval_emb)],  format=\"csr\")\n",
    "\n",
    "# --- Class weights ---\n",
    "num_labels = df[\"label_id\"].nunique()\n",
    "cls_w = compute_class_weight(\"balanced\", classes=np.arange(num_labels), y=train_df[\"label_id\"].to_numpy())\n",
    "cls_w = {i:w for i,w in enumerate(cls_w)}\n",
    "\n",
    "# --- Klassifikator ---\n",
    "clf = LogisticRegression(\n",
    "    multi_class=\"multinomial\",\n",
    "    solver=\"lbfgs\",\n",
    "    class_weight=cls_w,\n",
    "    max_iter=3000,\n",
    "    n_jobs=-1,\n",
    "    random_state=SEED\n",
    ")\n",
    "clf.fit(X_train_stack, train_df[\"label_id\"].to_numpy())\n",
    "\n",
    "y_pred = clf.predict(X_eval_stack)\n",
    "print({\n",
    "    \"accuracy\":  float(accuracy_score(eval_df[\"label_id\"], y_pred)),\n",
    "    \"f1_macro\":  float(f1_score(eval_df[\"label_id\"], y_pred, average=\"macro\")),\n",
    "    \"precision\": float(precision_score(eval_df[\"label_id\"], y_pred, average=\"macro\", zero_division=0)),\n",
    "    \"recall\":    float(recall_score(eval_df[\"label_id\"], y_pred, average=\"macro\")),\n",
    "})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
