{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a33344",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "## Peeringdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1c32292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "org_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "aka",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "name_long",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "website",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "social_media",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "asn",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "looking_glass",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "route_server",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "irr_as_set",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "info_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "info_types",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "info_prefixes4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "info_prefixes6",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "info_traffic",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "info_ratio",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "info_scope",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "info_unicast",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "info_multicast",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "info_ipv6",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "info_never_via_route_servers",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "ix_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "fac_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "notes",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "netixlan_updated",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "netfac_updated",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "poc_updated",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "policy_url",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "policy_general",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "policy_locations",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "policy_ratio",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "policy_contracts",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "allow_ixp_update",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "status_dashboard",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "rir_status",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "rir_status_updated",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "logo",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "created",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "updated",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "status",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "ee5b422f-0b21-4c2f-a5f1-317e50fb3cf7",
       "rows": [
        [
         "0",
         "1",
         "8897",
         "GTT Communications (AS4436)",
         "Formerly known as nLayer Communications",
         "",
         "http://www.gtt.net",
         "[{'service': 'website', 'identifier': 'http://www.gtt.net'}]",
         "4436",
         "",
         "",
         "",
         "NSP",
         "['NSP']",
         "200000.0",
         "10000.0",
         "",
         "",
         "Global",
         "True",
         "False",
         "True",
         "False",
         "0",
         "0",
         "nLayer / AS4436 has been acquired by GTT Communications / AS3257 and is no longer directly peering.  Please refer all peering related inquiries to peering [at] gtt [dot] net.",
         "2021-09-22T00:06:59Z",
         "2016-09-19T05:47:27Z",
         "2016-03-14T21:53:18Z",
         "http://www.gtt.net/peering/",
         "Restrictive",
         "Required - International",
         "True",
         "Required",
         "False",
         null,
         "ok",
         "2024-06-26T04:47:55Z",
         null,
         "2004-07-28T00:00:00Z",
         "2022-07-27T05:33:22Z",
         "ok"
        ],
        [
         "1",
         "2",
         "14",
         "Akamai Technologies",
         "",
         "",
         "https://www.akamai.com/",
         "[{'service': 'website', 'identifier': 'https://www.akamai.com/'}]",
         "20940",
         "",
         "",
         "AS-AKAMAI",
         "Content",
         "['Content']",
         "12000.0",
         "5000.0",
         "100+Tbps",
         "Heavy Outbound",
         "Global",
         "True",
         "False",
         "True",
         "False",
         "231",
         "217",
         "",
         "2025-10-20T12:15:26Z",
         "2025-09-18T10:39:32Z",
         "2021-05-11T21:26:32Z",
         "",
         "Open",
         "Not Required",
         "False",
         "Not Required",
         "False",
         "https://www.akamaistatus.com/",
         "ok",
         "2024-06-26T04:47:55Z",
         null,
         "2004-07-28T00:00:00Z",
         "2025-10-20T12:16:12Z",
         "ok"
        ],
        [
         "2",
         "3",
         "17",
         "DALnet IRC Network",
         "",
         "",
         "http://www.dal.net",
         "[{'service': 'website', 'identifier': 'http://www.dal.net'}]",
         "31800",
         "",
         "",
         "AS31800",
         "Non-Profit",
         "['Non-Profit']",
         "2.0",
         "0.0",
         "100-1000Mbps",
         "Heavy Inbound",
         "Global",
         "True",
         "False",
         "False",
         "False",
         "15",
         "0",
         "",
         "2025-01-09T13:41:48Z",
         null,
         "2016-03-14T21:22:01Z",
         "http://peering.dal.net",
         "Open",
         "Preferred",
         "False",
         "Not Required",
         "False",
         "",
         "ok",
         "2024-06-26T04:47:55Z",
         null,
         "2004-07-28T00:00:00Z",
         "2025-01-09T13:42:07Z",
         "ok"
        ],
        [
         "3",
         "5",
         "9350",
         "Swisscom",
         "IP-Plus",
         "",
         "http://www.swisscom.com",
         "[{'service': 'website', 'identifier': 'http://www.swisscom.com'}]",
         "3303",
         "",
         "telnet://route-server.ip-plus.net",
         "RIPE::AS3303:AS-SWCMGLOBAL",
         "Cable/DSL/ISP",
         "['Cable/DSL/ISP']",
         "10000.0",
         "1800.0",
         "1-5Tbps",
         "Mostly Inbound",
         "Europe",
         "True",
         "False",
         "True",
         "False",
         "63",
         "32",
         "",
         "2025-10-16T06:47:17Z",
         "2025-04-10T13:25:48Z",
         "2020-01-22T04:24:08Z",
         "https://www.swisscom.ch/content/dam/swisscom/de/ws/documents/d-ott-dokumente/20230101_swisscom-peering-policy.pdf",
         "Selective",
         "Preferred",
         "True",
         "Required",
         "False",
         "",
         "ok",
         "2024-06-26T04:47:55Z",
         null,
         "2004-07-28T00:00:00Z",
         "2025-08-12T06:33:30Z",
         "ok"
        ],
        [
         "4",
         "6",
         "23",
         "Cox Communications",
         "Cox Communications",
         "",
         "http://www.cox.com/peering",
         "[{'service': 'website', 'identifier': 'http://www.cox.com/peering'}]",
         "22773",
         "",
         "",
         "AS22773:AS-CONE",
         "Cable/DSL/ISP",
         "['Cable/DSL/ISP']",
         "10000.0",
         "3000.0",
         "100-200Gbps",
         "Mostly Inbound",
         "North America",
         "True",
         "False",
         "True",
         "False",
         "0",
         "14",
         "",
         null,
         "2022-03-24T19:56:00Z",
         "2024-03-06T01:56:24Z",
         "http://www.cox.com/peering",
         "Selective",
         "Preferred",
         "False",
         "Required",
         "False",
         "",
         "ok",
         "2024-06-26T04:47:55Z",
         null,
         "2004-07-28T00:00:00Z",
         "2022-11-28T22:55:17Z",
         "ok"
        ]
       ],
       "shape": {
        "columns": 41,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>org_id</th>\n",
       "      <th>name</th>\n",
       "      <th>aka</th>\n",
       "      <th>name_long</th>\n",
       "      <th>website</th>\n",
       "      <th>social_media</th>\n",
       "      <th>asn</th>\n",
       "      <th>looking_glass</th>\n",
       "      <th>route_server</th>\n",
       "      <th>...</th>\n",
       "      <th>policy_ratio</th>\n",
       "      <th>policy_contracts</th>\n",
       "      <th>allow_ixp_update</th>\n",
       "      <th>status_dashboard</th>\n",
       "      <th>rir_status</th>\n",
       "      <th>rir_status_updated</th>\n",
       "      <th>logo</th>\n",
       "      <th>created</th>\n",
       "      <th>updated</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8897</td>\n",
       "      <td>GTT Communications (AS4436)</td>\n",
       "      <td>Formerly known as nLayer Communications</td>\n",
       "      <td></td>\n",
       "      <td>http://www.gtt.net</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'http://...</td>\n",
       "      <td>4436</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>Required</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2022-07-27T05:33:22Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>Akamai Technologies</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>https://www.akamai.com/</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'https:/...</td>\n",
       "      <td>20940</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Not Required</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.akamaistatus.com/</td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2025-10-20T12:16:12Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>DALnet IRC Network</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>http://www.dal.net</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'http://...</td>\n",
       "      <td>31800</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Not Required</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2025-01-09T13:42:07Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>9350</td>\n",
       "      <td>Swisscom</td>\n",
       "      <td>IP-Plus</td>\n",
       "      <td></td>\n",
       "      <td>http://www.swisscom.com</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'http://...</td>\n",
       "      <td>3303</td>\n",
       "      <td></td>\n",
       "      <td>telnet://route-server.ip-plus.net</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>Required</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2025-08-12T06:33:30Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>Cox Communications</td>\n",
       "      <td>Cox Communications</td>\n",
       "      <td></td>\n",
       "      <td>http://www.cox.com/peering</td>\n",
       "      <td>[{'service': 'website', 'identifier': 'http://...</td>\n",
       "      <td>22773</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Required</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>ok</td>\n",
       "      <td>2024-06-26T04:47:55Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2004-07-28T00:00:00Z</td>\n",
       "      <td>2022-11-28T22:55:17Z</td>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  org_id                         name  \\\n",
       "0   1    8897  GTT Communications (AS4436)   \n",
       "1   2      14          Akamai Technologies   \n",
       "2   3      17           DALnet IRC Network   \n",
       "3   5    9350                     Swisscom   \n",
       "4   6      23           Cox Communications   \n",
       "\n",
       "                                       aka name_long  \\\n",
       "0  Formerly known as nLayer Communications             \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                  IP-Plus             \n",
       "4                       Cox Communications             \n",
       "\n",
       "                      website  \\\n",
       "0          http://www.gtt.net   \n",
       "1     https://www.akamai.com/   \n",
       "2          http://www.dal.net   \n",
       "3     http://www.swisscom.com   \n",
       "4  http://www.cox.com/peering   \n",
       "\n",
       "                                        social_media    asn looking_glass  \\\n",
       "0  [{'service': 'website', 'identifier': 'http://...   4436                 \n",
       "1  [{'service': 'website', 'identifier': 'https:/...  20940                 \n",
       "2  [{'service': 'website', 'identifier': 'http://...  31800                 \n",
       "3  [{'service': 'website', 'identifier': 'http://...   3303                 \n",
       "4  [{'service': 'website', 'identifier': 'http://...  22773                 \n",
       "\n",
       "                        route_server  ... policy_ratio policy_contracts  \\\n",
       "0                                     ...         True         Required   \n",
       "1                                     ...        False     Not Required   \n",
       "2                                     ...        False     Not Required   \n",
       "3  telnet://route-server.ip-plus.net  ...         True         Required   \n",
       "4                                     ...        False         Required   \n",
       "\n",
       "  allow_ixp_update               status_dashboard  rir_status  \\\n",
       "0            False                           None          ok   \n",
       "1            False  https://www.akamaistatus.com/          ok   \n",
       "2            False                                         ok   \n",
       "3            False                                         ok   \n",
       "4            False                                         ok   \n",
       "\n",
       "     rir_status_updated  logo               created               updated  \\\n",
       "0  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2022-07-27T05:33:22Z   \n",
       "1  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2025-10-20T12:16:12Z   \n",
       "2  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2025-01-09T13:42:07Z   \n",
       "3  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2025-08-12T06:33:30Z   \n",
       "4  2024-06-26T04:47:55Z  None  2004-07-28T00:00:00Z  2022-11-28T22:55:17Z   \n",
       "\n",
       "   status  \n",
       "0      ok  \n",
       "1      ok  \n",
       "2      ok  \n",
       "3      ok  \n",
       "4      ok  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "filepath = Path('../../preprocessing/data/peeringdb/peeringdb_2_dump_2025_10_21.json')\n",
    "\n",
    "with filepath.open('r', encoding='utf-8') as f:\n",
    "    dump = json.load(f)\n",
    "\n",
    "# extract the net.data section and load into a DataFrame\n",
    "net_data = dump.get('net', {}).get('data')\n",
    "if net_data is None:\n",
    "    raise KeyError(\"JSON does not contain 'net' -> 'data' structure\")\n",
    "\n",
    "net_df = pd.DataFrame(net_data)\n",
    "net_df['asn'] = net_df['asn'].astype(int)\n",
    "net_df = net_df[net_df['info_type'] != '']\n",
    "\n",
    "# show a quick preview\n",
    "net_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2f20d5",
   "metadata": {},
   "source": [
    "# Caida AS Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c190e0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "aut",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "changed",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "org_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "source",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "org_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "country",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "f3279f6c-eebf-47a8-a6e1-d3221d5b2e32",
       "rows": [
        [
         "0",
         "1",
         "20240618.0",
         "LPL-141-ARIN",
         "ARIN",
         "Level 3 Parent, LLC",
         "US"
        ],
        [
         "1",
         "2",
         "20231108.0",
         "UNIVER-19-Z-ARIN",
         "ARIN",
         "University of Delaware",
         "US"
        ],
        [
         "2",
         "3",
         "20100927.0",
         "MIT-2-ARIN",
         "ARIN",
         "Massachusetts Institute of Technology",
         "US"
        ],
        [
         "3",
         "4",
         "20230929.0",
         "USC-32-Z-ARIN",
         "ARIN",
         "University of Southern California",
         "US"
        ],
        [
         "4",
         "5",
         "20200723.0",
         "WGL-117-ARIN",
         "ARIN",
         "WFA Group LLC",
         "US"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aut</th>\n",
       "      <th>changed</th>\n",
       "      <th>org_id</th>\n",
       "      <th>source</th>\n",
       "      <th>org_name</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20240618.0</td>\n",
       "      <td>LPL-141-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Level 3 Parent, LLC</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20231108.0</td>\n",
       "      <td>UNIVER-19-Z-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>University of Delaware</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>20100927.0</td>\n",
       "      <td>MIT-2-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Massachusetts Institute of Technology</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>20230929.0</td>\n",
       "      <td>USC-32-Z-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>University of Southern California</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>20200723.0</td>\n",
       "      <td>WGL-117-ARIN</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>WFA Group LLC</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   aut     changed            org_id source  \\\n",
       "0    1  20240618.0      LPL-141-ARIN   ARIN   \n",
       "1    2  20231108.0  UNIVER-19-Z-ARIN   ARIN   \n",
       "2    3  20100927.0        MIT-2-ARIN   ARIN   \n",
       "3    4  20230929.0     USC-32-Z-ARIN   ARIN   \n",
       "4    5  20200723.0      WGL-117-ARIN   ARIN   \n",
       "\n",
       "                                org_name country  \n",
       "0                    Level 3 Parent, LLC      US  \n",
       "1                 University of Delaware      US  \n",
       "2  Massachusetts Institute of Technology      US  \n",
       "3      University of Southern California      US  \n",
       "4                          WFA Group LLC      US  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "with open('/workspaces/pytorch-gpu-2/preprocessing/data/caida/20251001.as-org2info.txt', 'r', newline='', encoding='utf-8') as input_file:\n",
    "    lines = input_file.readlines()   \n",
    "    # Buffers initialisieren\n",
    "    aut_lines = []\n",
    "    org_lines = []\n",
    "    mode = None\n",
    "    total_lines = len(lines)\n",
    "    aut_count = 0\n",
    "    org_count = 0 \n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"# format:aut\"):\n",
    "            mode = \"aut\"\n",
    "            continue\n",
    "        elif line.startswith(\"# format:org_id\"):\n",
    "            mode = \"org\"\n",
    "            continue\n",
    "        elif line.startswith(\"#\") or not line:\n",
    "            # Andere Kommentar- oder Leerzeilen überspringen\n",
    "            continue      \n",
    "        if mode == \"aut\":\n",
    "            aut_lines.append(line)\n",
    "            aut_count += 1\n",
    "        elif mode == \"org\":\n",
    "            org_lines.append(line)\n",
    "            org_count += 1\n",
    "    # StringIO-Objekte aus den gesammelten Zeilen bauen\n",
    "    aut_buffer = io.StringIO(\"\\n\".join(aut_lines))\n",
    "    org_buffer = io.StringIO(\"\\n\".join(org_lines))\n",
    "    # DataFrames einlesen\n",
    "    aut_df = pd.read_csv(aut_buffer, sep=\"|\",\n",
    "                        names=[\"aut\", \"changed\", \"aut_name\", \"org_id\", \"opaque_id\", \"source\"], usecols=[\"aut\", \"org_id\", \"source\", \"changed\"])\n",
    "    org_df = pd.read_csv(org_buffer, sep=\"|\",\n",
    "                        names=[\"org_id\", \"changed\", \"org_name\", \"country\", \"source\"], usecols=[\"org_id\", \"org_name\", \"country\"])\n",
    "\n",
    "    # Join the DataFrames\n",
    "    joined_df = pd.merge(aut_df, org_df, on=\"org_id\", how=\"left\")\n",
    "joined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73e5047",
   "metadata": {},
   "source": [
    "## Join both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6de8ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "asn",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "org_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "country",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "source",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "info_type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "35d73372-e8f1-4409-afce-092545669371",
       "rows": [
        [
         "0",
         "4436",
         "GTT Americas, LLC",
         "US",
         "ARIN",
         "NSP"
        ],
        [
         "1",
         "20940",
         "Akamai International B.V.",
         "NL",
         "RIPE",
         "Content"
        ],
        [
         "2",
         "31800",
         "DALnet",
         "US",
         "ARIN",
         "Non-Profit"
        ],
        [
         "3",
         "3303",
         "Swisscom (Schweiz) AG",
         "CH",
         "RIPE",
         "Cable/DSL/ISP"
        ],
        [
         "4",
         "22773",
         "Cox Communications Inc.",
         "US",
         "ARIN",
         "Cable/DSL/ISP"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asn</th>\n",
       "      <th>org_name</th>\n",
       "      <th>country</th>\n",
       "      <th>source</th>\n",
       "      <th>info_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4436</td>\n",
       "      <td>GTT Americas, LLC</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>NSP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20940</td>\n",
       "      <td>Akamai International B.V.</td>\n",
       "      <td>NL</td>\n",
       "      <td>RIPE</td>\n",
       "      <td>Content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31800</td>\n",
       "      <td>DALnet</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Non-Profit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3303</td>\n",
       "      <td>Swisscom (Schweiz) AG</td>\n",
       "      <td>CH</td>\n",
       "      <td>RIPE</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22773</td>\n",
       "      <td>Cox Communications Inc.</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     asn                   org_name country source      info_type\n",
       "0   4436          GTT Americas, LLC      US   ARIN            NSP\n",
       "1  20940  Akamai International B.V.      NL   RIPE        Content\n",
       "2  31800                     DALnet      US   ARIN     Non-Profit\n",
       "3   3303      Swisscom (Schweiz) AG      CH   RIPE  Cable/DSL/ISP\n",
       "4  22773    Cox Communications Inc.      US   ARIN  Cable/DSL/ISP"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peering_df_joined = pd.merge(net_df, joined_df, left_on='asn', right_on='aut', how='left')\n",
    "peering_df_joined = peering_df_joined[['asn', 'org_name', 'country', 'source', 'info_type']]\n",
    "peering_df_joined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99053eb",
   "metadata": {},
   "source": [
    "## Load AS Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85e496cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "asn",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "rank",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "asnDegree_total",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "asnDegree_customer",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "asnDegree_peer",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "asnDegree_provider",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "cone_numberAsns",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "cone_numberPrefixes",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "cone_numberAddresses",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "1c3a3b29-b34e-4d61-b1bb-b08ef1bb1003",
       "rows": [
        [
         "0",
         "3356",
         "1",
         "6613",
         "6545",
         "68",
         "0",
         "53986",
         "873410",
         "3468642119"
        ],
        [
         "1",
         "1299",
         "2",
         "2567",
         "2509",
         "58",
         "0",
         "41193",
         "776707",
         "3219679484"
        ],
        [
         "2",
         "174",
         "3",
         "6723",
         "6626",
         "97",
         "0",
         "38887",
         "730166",
         "3034352967"
        ],
        [
         "3",
         "3257",
         "4",
         "1853",
         "1816",
         "37",
         "0",
         "36040",
         "612491",
         "2791999209"
        ],
        [
         "4",
         "2914",
         "5",
         "1541",
         "1483",
         "58",
         "0",
         "25179",
         "576134",
         "2918763154"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asn</th>\n",
       "      <th>rank</th>\n",
       "      <th>asnDegree_total</th>\n",
       "      <th>asnDegree_customer</th>\n",
       "      <th>asnDegree_peer</th>\n",
       "      <th>asnDegree_provider</th>\n",
       "      <th>cone_numberAsns</th>\n",
       "      <th>cone_numberPrefixes</th>\n",
       "      <th>cone_numberAddresses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3356</td>\n",
       "      <td>1</td>\n",
       "      <td>6613</td>\n",
       "      <td>6545</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>53986</td>\n",
       "      <td>873410</td>\n",
       "      <td>3468642119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1299</td>\n",
       "      <td>2</td>\n",
       "      <td>2567</td>\n",
       "      <td>2509</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>41193</td>\n",
       "      <td>776707</td>\n",
       "      <td>3219679484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>174</td>\n",
       "      <td>3</td>\n",
       "      <td>6723</td>\n",
       "      <td>6626</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>38887</td>\n",
       "      <td>730166</td>\n",
       "      <td>3034352967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3257</td>\n",
       "      <td>4</td>\n",
       "      <td>1853</td>\n",
       "      <td>1816</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>36040</td>\n",
       "      <td>612491</td>\n",
       "      <td>2791999209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2914</td>\n",
       "      <td>5</td>\n",
       "      <td>1541</td>\n",
       "      <td>1483</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>25179</td>\n",
       "      <td>576134</td>\n",
       "      <td>2918763154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    asn  rank  asnDegree_total  asnDegree_customer  asnDegree_peer  \\\n",
       "0  3356     1             6613                6545              68   \n",
       "1  1299     2             2567                2509              58   \n",
       "2   174     3             6723                6626              97   \n",
       "3  3257     4             1853                1816              37   \n",
       "4  2914     5             1541                1483              58   \n",
       "\n",
       "   asnDegree_provider  cone_numberAsns  cone_numberPrefixes  \\\n",
       "0                   0            53986               873410   \n",
       "1                   0            41193               776707   \n",
       "2                   0            38887               730166   \n",
       "3                   0            36040               612491   \n",
       "4                   0            25179               576134   \n",
       "\n",
       "   cone_numberAddresses  \n",
       "0            3468642119  \n",
       "1            3219679484  \n",
       "2            3034352967  \n",
       "3            2791999209  \n",
       "4            2918763154  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "as_rank_df = pd.read_csv('/workspaces/pytorch-gpu-2/preprocessing/data/asrank/as_rank_df.csv')\n",
    "as_rank_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521c4fdf",
   "metadata": {},
   "source": [
    "## Join both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "498be035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "asn",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "org_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "country",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "source",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "info_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "rank",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "asnDegree_total",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "asnDegree_customer",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "asnDegree_peer",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "asnDegree_provider",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cone_numberAsns",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cone_numberPrefixes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cone_numberAddresses",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "b0eec318-2100-46a6-b508-dd516c4c3dfe",
       "rows": [
        [
         "0",
         "4436",
         "GTT Americas, LLC",
         "US",
         "ARIN",
         "NSP",
         "78320.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0"
        ],
        [
         "1",
         "20940",
         "Akamai International B.V.",
         "NL",
         "RIPE",
         "Content",
         "1894.0",
         "485.0",
         "14.0",
         "366.0",
         "105.0",
         "15.0",
         "8945.0",
         "14612752.0"
        ],
        [
         "2",
         "31800",
         "DALnet",
         "US",
         "ARIN",
         "Non-Profit",
         "47745.0",
         "78.0",
         "0.0",
         "74.0",
         "4.0",
         "1.0",
         "2.0",
         "512.0"
        ],
        [
         "3",
         "3303",
         "Swisscom (Schweiz) AG",
         "CH",
         "RIPE",
         "Cable/DSL/ISP",
         "81.0",
         "1273.0",
         "166.0",
         "1101.0",
         "6.0",
         "733.0",
         "22131.0",
         "42899794.0"
        ],
        [
         "4",
         "22773",
         "Cox Communications Inc.",
         "US",
         "ARIN",
         "Cable/DSL/ISP",
         "110.0",
         "499.0",
         "489.0",
         "8.0",
         "2.0",
         "505.0",
         "11982.0",
         "31992440.0"
        ]
       ],
       "shape": {
        "columns": 13,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asn</th>\n",
       "      <th>org_name</th>\n",
       "      <th>country</th>\n",
       "      <th>source</th>\n",
       "      <th>info_type</th>\n",
       "      <th>rank</th>\n",
       "      <th>asnDegree_total</th>\n",
       "      <th>asnDegree_customer</th>\n",
       "      <th>asnDegree_peer</th>\n",
       "      <th>asnDegree_provider</th>\n",
       "      <th>cone_numberAsns</th>\n",
       "      <th>cone_numberPrefixes</th>\n",
       "      <th>cone_numberAddresses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4436</td>\n",
       "      <td>GTT Americas, LLC</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>NSP</td>\n",
       "      <td>78320.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20940</td>\n",
       "      <td>Akamai International B.V.</td>\n",
       "      <td>NL</td>\n",
       "      <td>RIPE</td>\n",
       "      <td>Content</td>\n",
       "      <td>1894.0</td>\n",
       "      <td>485.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8945.0</td>\n",
       "      <td>14612752.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31800</td>\n",
       "      <td>DALnet</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Non-Profit</td>\n",
       "      <td>47745.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3303</td>\n",
       "      <td>Swisscom (Schweiz) AG</td>\n",
       "      <td>CH</td>\n",
       "      <td>RIPE</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "      <td>81.0</td>\n",
       "      <td>1273.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>733.0</td>\n",
       "      <td>22131.0</td>\n",
       "      <td>42899794.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22773</td>\n",
       "      <td>Cox Communications Inc.</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "      <td>110.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>505.0</td>\n",
       "      <td>11982.0</td>\n",
       "      <td>31992440.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     asn                   org_name country source      info_type     rank  \\\n",
       "0   4436          GTT Americas, LLC      US   ARIN            NSP  78320.0   \n",
       "1  20940  Akamai International B.V.      NL   RIPE        Content   1894.0   \n",
       "2  31800                     DALnet      US   ARIN     Non-Profit  47745.0   \n",
       "3   3303      Swisscom (Schweiz) AG      CH   RIPE  Cable/DSL/ISP     81.0   \n",
       "4  22773    Cox Communications Inc.      US   ARIN  Cable/DSL/ISP    110.0   \n",
       "\n",
       "   asnDegree_total  asnDegree_customer  asnDegree_peer  asnDegree_provider  \\\n",
       "0              0.0                 0.0             0.0                 0.0   \n",
       "1            485.0                14.0           366.0               105.0   \n",
       "2             78.0                 0.0            74.0                 4.0   \n",
       "3           1273.0               166.0          1101.0                 6.0   \n",
       "4            499.0               489.0             8.0                 2.0   \n",
       "\n",
       "   cone_numberAsns  cone_numberPrefixes  cone_numberAddresses  \n",
       "0              1.0                  0.0                   0.0  \n",
       "1             15.0               8945.0            14612752.0  \n",
       "2              1.0                  2.0                 512.0  \n",
       "3            733.0              22131.0            42899794.0  \n",
       "4            505.0              11982.0            31992440.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peering_df_joined_with_asrank = pd.merge(\n",
    "    peering_df_joined,\n",
    "    as_rank_df,\n",
    "    left_on='asn',\n",
    "    right_on='asn',\n",
    "    how='left'\n",
    ")\n",
    "peering_df_joined_with_asrank['rank'] = peering_df_joined_with_asrank['rank'].fillna(peering_df_joined_with_asrank['rank'].median())\n",
    "peering_df_joined_with_asrank['asnDegree_total'] = peering_df_joined_with_asrank['asnDegree_total'].fillna(peering_df_joined_with_asrank['asnDegree_total'].median())\n",
    "peering_df_joined_with_asrank['asnDegree_customer'] = peering_df_joined_with_asrank['asnDegree_customer'].fillna(peering_df_joined_with_asrank['asnDegree_customer'].median())\n",
    "peering_df_joined_with_asrank['asnDegree_peer'] = peering_df_joined_with_asrank['asnDegree_peer'].fillna(peering_df_joined_with_asrank['asnDegree_peer'].median())\n",
    "peering_df_joined_with_asrank['asnDegree_provider'] = peering_df_joined_with_asrank['asnDegree_provider'].fillna(peering_df_joined_with_asrank['asnDegree_provider'].median())\n",
    "peering_df_joined_with_asrank['cone_numberAsns'] = peering_df_joined_with_asrank['cone_numberAsns'].fillna(peering_df_joined_with_asrank['cone_numberAsns'].median())\n",
    "peering_df_joined_with_asrank['cone_numberPrefixes'] = peering_df_joined_with_asrank['cone_numberPrefixes'].fillna(peering_df_joined_with_asrank['cone_numberPrefixes'].median())\n",
    "peering_df_joined_with_asrank['cone_numberAddresses'] = peering_df_joined_with_asrank['cone_numberAddresses'].fillna(peering_df_joined_with_asrank['cone_numberAddresses'].median())\n",
    "\n",
    "peering_df_joined_with_asrank.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91da8c67",
   "metadata": {},
   "source": [
    "## Load domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37d9a971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ASN",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "domains",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "3b160e03-79d9-4132-8341-dbb33bffb499",
       "rows": [
        [
         "0",
         "16509",
         "139276485"
        ],
        [
         "1",
         "13335",
         "63477595"
        ],
        [
         "2",
         "52925",
         "32915972"
        ],
        [
         "3",
         "396982",
         "24543491"
        ],
        [
         "4",
         "47846",
         "17833760"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASN</th>\n",
       "      <th>domains</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16509</td>\n",
       "      <td>139276485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13335</td>\n",
       "      <td>63477595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52925</td>\n",
       "      <td>32915972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>396982</td>\n",
       "      <td>24543491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47846</td>\n",
       "      <td>17833760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ASN    domains\n",
       "0   16509  139276485\n",
       "1   13335   63477595\n",
       "2   52925   32915972\n",
       "3  396982   24543491\n",
       "4   47846   17833760"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipinfo_df = pd.read_csv('../../preprocessing/data/ipinfo_domains/ipinfo_domains.csv')\n",
    "ipinfo_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a612c6db",
   "metadata": {},
   "source": [
    "## Join both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c144c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "asn",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "org_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "country",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "source",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "info_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "rank",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "asnDegree_total",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "asnDegree_customer",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "asnDegree_peer",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "asnDegree_provider",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cone_numberAsns",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cone_numberPrefixes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cone_numberAddresses",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ASN",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "domains",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "29bb77d8-0f98-48d7-b0a9-8b5b142b5832",
       "rows": [
        [
         "0",
         "4436",
         "GTT Americas, LLC",
         "US",
         "ARIN",
         "NSP",
         "78320.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "1.0",
         "0.0",
         "0.0",
         null,
         null
        ],
        [
         "1",
         "20940",
         "Akamai International B.V.",
         "NL",
         "RIPE",
         "Content",
         "1894.0",
         "485.0",
         "14.0",
         "366.0",
         "105.0",
         "15.0",
         "8945.0",
         "14612752.0",
         "20940.0",
         null
        ],
        [
         "2",
         "31800",
         "DALnet",
         "US",
         "ARIN",
         "Non-Profit",
         "47745.0",
         "78.0",
         "0.0",
         "74.0",
         "4.0",
         "1.0",
         "2.0",
         "512.0",
         null,
         null
        ],
        [
         "3",
         "3303",
         "Swisscom (Schweiz) AG",
         "CH",
         "RIPE",
         "Cable/DSL/ISP",
         "81.0",
         "1273.0",
         "166.0",
         "1101.0",
         "6.0",
         "733.0",
         "22131.0",
         "42899794.0",
         "3303.0",
         null
        ],
        [
         "4",
         "22773",
         "Cox Communications Inc.",
         "US",
         "ARIN",
         "Cable/DSL/ISP",
         "110.0",
         "499.0",
         "489.0",
         "8.0",
         "2.0",
         "505.0",
         "11982.0",
         "31992440.0",
         "22773.0",
         null
        ]
       ],
       "shape": {
        "columns": 15,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asn</th>\n",
       "      <th>org_name</th>\n",
       "      <th>country</th>\n",
       "      <th>source</th>\n",
       "      <th>info_type</th>\n",
       "      <th>rank</th>\n",
       "      <th>asnDegree_total</th>\n",
       "      <th>asnDegree_customer</th>\n",
       "      <th>asnDegree_peer</th>\n",
       "      <th>asnDegree_provider</th>\n",
       "      <th>cone_numberAsns</th>\n",
       "      <th>cone_numberPrefixes</th>\n",
       "      <th>cone_numberAddresses</th>\n",
       "      <th>ASN</th>\n",
       "      <th>domains</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4436</td>\n",
       "      <td>GTT Americas, LLC</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>NSP</td>\n",
       "      <td>78320.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20940</td>\n",
       "      <td>Akamai International B.V.</td>\n",
       "      <td>NL</td>\n",
       "      <td>RIPE</td>\n",
       "      <td>Content</td>\n",
       "      <td>1894.0</td>\n",
       "      <td>485.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8945.0</td>\n",
       "      <td>14612752.0</td>\n",
       "      <td>20940.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31800</td>\n",
       "      <td>DALnet</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Non-Profit</td>\n",
       "      <td>47745.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3303</td>\n",
       "      <td>Swisscom (Schweiz) AG</td>\n",
       "      <td>CH</td>\n",
       "      <td>RIPE</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "      <td>81.0</td>\n",
       "      <td>1273.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>733.0</td>\n",
       "      <td>22131.0</td>\n",
       "      <td>42899794.0</td>\n",
       "      <td>3303.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22773</td>\n",
       "      <td>Cox Communications Inc.</td>\n",
       "      <td>US</td>\n",
       "      <td>ARIN</td>\n",
       "      <td>Cable/DSL/ISP</td>\n",
       "      <td>110.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>505.0</td>\n",
       "      <td>11982.0</td>\n",
       "      <td>31992440.0</td>\n",
       "      <td>22773.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     asn                   org_name country source      info_type     rank  \\\n",
       "0   4436          GTT Americas, LLC      US   ARIN            NSP  78320.0   \n",
       "1  20940  Akamai International B.V.      NL   RIPE        Content   1894.0   \n",
       "2  31800                     DALnet      US   ARIN     Non-Profit  47745.0   \n",
       "3   3303      Swisscom (Schweiz) AG      CH   RIPE  Cable/DSL/ISP     81.0   \n",
       "4  22773    Cox Communications Inc.      US   ARIN  Cable/DSL/ISP    110.0   \n",
       "\n",
       "   asnDegree_total  asnDegree_customer  asnDegree_peer  asnDegree_provider  \\\n",
       "0              0.0                 0.0             0.0                 0.0   \n",
       "1            485.0                14.0           366.0               105.0   \n",
       "2             78.0                 0.0            74.0                 4.0   \n",
       "3           1273.0               166.0          1101.0                 6.0   \n",
       "4            499.0               489.0             8.0                 2.0   \n",
       "\n",
       "   cone_numberAsns  cone_numberPrefixes  cone_numberAddresses      ASN domains  \n",
       "0              1.0                  0.0                   0.0      NaN    None  \n",
       "1             15.0               8945.0            14612752.0  20940.0    None  \n",
       "2              1.0                  2.0                 512.0      NaN    None  \n",
       "3            733.0              22131.0            42899794.0   3303.0    None  \n",
       "4            505.0              11982.0            31992440.0  22773.0    None  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peering_df_joined_with_asrank_and_domains = pd.merge(\n",
    "    peering_df_joined_with_asrank,\n",
    "    ipinfo_df,\n",
    "    left_on='asn',\n",
    "    right_on='ASN',\n",
    "    how='left'\n",
    ")\n",
    "peering_df_joined_with_asrank_and_domains['domains'] = peering_df_joined_with_asrank_and_domains['domains'].fillna(peering_df_joined_with_asrank_and_domains['domains'].median(), inplace=True)\n",
    "peering_df_joined_with_asrank_and_domains.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c082156c",
   "metadata": {},
   "source": [
    "## Load geolocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2902187",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'clickhouse_connect'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mclickhouse_connect\u001b[39;00m\n\u001b[32m      3\u001b[39m client = clickhouse_connect.get_client(\n\u001b[32m      4\u001b[39m     host=\u001b[33m'\u001b[39m\u001b[33mlocalhost\u001b[39m\u001b[33m'\u001b[39m, port=\u001b[32m8123\u001b[39m,\n\u001b[32m      5\u001b[39m     username=\u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m, password=\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m query = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[33m/* Gewichtetes Zentrum und umfangreiche Distanz-Statistiken (Kilometer) */\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[33mWITH base AS (\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    121\u001b[39m \u001b[33mORDER BY asn\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'clickhouse_connect'"
     ]
    }
   ],
   "source": [
    "import clickhouse_connect\n",
    "\n",
    "client = clickhouse_connect.get_client(\n",
    "    host='localhost', port=8123,\n",
    "    username='default', password=''\n",
    ")\n",
    "\n",
    "query = \"\"\"\n",
    "/* Gewichtetes Zentrum und umfangreiche Distanz-Statistiken (Kilometer) */\n",
    "WITH base AS (\n",
    "    SELECT\n",
    "        asn,\n",
    "        latitude,\n",
    "        longitude,\n",
    "        country,\n",
    "        toUInt64(ip_end - ip_start + 1) AS w\n",
    "    FROM ip_location_asn\n",
    "    WHERE ip_version = 4\n",
    "      AND origin = 'ipinfo'\n",
    "),\n",
    "country_entropy AS (\n",
    "    SELECT\n",
    "        asn,\n",
    "        -- Gewichte je Land\n",
    "        groupArray(wc)           AS arr_w,\n",
    "        sum(wc)                  AS W,\n",
    "        -- Shannon-Entropie (Bits)\n",
    "        -arraySum(x -> (x / W) * log2(x / W), arr_w) AS country_entropy_bits\n",
    "    FROM (\n",
    "        SELECT asn, country, sum(w) AS wc\n",
    "        FROM base\n",
    "        GROUP BY asn, country\n",
    "    )\n",
    "    GROUP BY asn\n",
    "),\n",
    "vec AS (\n",
    "    SELECT\n",
    "        asn,\n",
    "        sum(w * cos(radians(latitude)) * cos(radians(longitude))) AS X,\n",
    "        sum(w * cos(radians(latitude)) * sin(radians(longitude))) AS Y,\n",
    "        sum(w * sin(radians(latitude)))                           AS Z,\n",
    "        sum(w)                                                    AS W\n",
    "    FROM base\n",
    "    GROUP BY asn\n",
    "),\n",
    "center AS (\n",
    "    SELECT\n",
    "        asn,\n",
    "        degrees(atan2(Y, X))                       AS center_lon,\n",
    "        degrees(atan2(Z, sqrt(X * X + Y * Y)))     AS center_lat\n",
    "    FROM vec\n",
    "),\n",
    "joined AS (\n",
    "    SELECT\n",
    "        b.asn,\n",
    "        b.w,\n",
    "        b.country,\n",
    "        c.center_lat,\n",
    "        c.center_lon,\n",
    "        greatCircleDistance(b.longitude, b.latitude, c.center_lon, c.center_lat) / 1000 AS d_km\n",
    "    FROM base AS b\n",
    "    INNER JOIN center AS c USING (asn)\n",
    "),\n",
    "stats AS (\n",
    "    SELECT\n",
    "        asn,\n",
    "        any(center_lat) AS center_lat,\n",
    "        any(center_lon) AS center_lon,\n",
    "        sum(w)  AS total_weight,\n",
    "        avgWeighted(d_km,       w) AS mean_km,\n",
    "        avgWeighted(d_km * d_km, w) AS mean_sq_km2,\n",
    "        quantileExactWeighted(0.25)(d_km, w) AS p25_km,\n",
    "        quantileExactWeighted(0.50)(d_km, w) AS p50_km,\n",
    "        quantileExactWeighted(0.75)(d_km, w) AS p75_km,\n",
    "        quantileExactWeighted(0.90)(d_km, w) AS p90_km,\n",
    "        quantileExactWeighted(0.95)(d_km, w) AS p95_km,\n",
    "        quantileExactWeighted(0.99)(d_km, w) AS p99_km,\n",
    "        min(d_km) AS min_km,\n",
    "        max(d_km) AS max_km,\n",
    "        sumIf(w, d_km <=  100) / sum(w) AS share_le_100km,\n",
    "        sumIf(w, d_km <=  500) / sum(w) AS share_le_500km,\n",
    "        sumIf(w, d_km <= 1000) / sum(w) AS share_le_1000km,\n",
    "    FROM joined\n",
    "    GROUP BY asn\n",
    "),\n",
    "geo_meta AS (\n",
    "    SELECT\n",
    "        asn,\n",
    "        uniqExact((latitude, longitude)) AS unique_points,\n",
    "        uniqExact(country)               AS country_count\n",
    "    FROM base\n",
    "    GROUP BY asn\n",
    ")\n",
    "SELECT\n",
    "    s.asn,\n",
    "    round(s.center_lat, 5) AS center_lat,\n",
    "    round(s.center_lon, 5) AS center_lon,\n",
    "    s.total_weight,\n",
    "    gm.unique_points,\n",
    "    gm.country_count,\n",
    "    round(s.mean_km, 2)                         AS mean_km,\n",
    "    round(greatest(s.mean_sq_km2 - s.mean_km * s.mean_km, 0), 2) AS var_km2,\n",
    "    round(sqrt(greatest(s.mean_sq_km2 - s.mean_km * s.mean_km, 0)), 2) AS std_km,\n",
    "    round(s.p75_km - s.p25_km, 2)               AS iqr_km,\n",
    "    round(s.p25_km, 2)                          AS p25_km,\n",
    "    round(s.p50_km, 2)                          AS p50_km,\n",
    "    round(s.p75_km, 2)                          AS p75_km,\n",
    "    round(s.p90_km, 2)                          AS p90_km,\n",
    "    round(s.p95_km, 2)                          AS p95_km,\n",
    "    round(s.p99_km, 2)                          AS p99_km,\n",
    "    round(s.min_km, 2)                          AS min_km,\n",
    "    round(s.max_km, 2)                          AS max_km,\n",
    "    round(s.share_le_100km * 100, 2)            AS pct_ips_le_100km,\n",
    "    round(s.share_le_500km * 100, 2)            AS pct_ips_le_500km,\n",
    "    round(s.share_le_1000km * 100, 2)           AS pct_ips_le_1000km,\n",
    "    cm.country_entropy_bits,\n",
    "    round(cm.country_entropy_bits / nullIf(log2(gm.country_count), 0), 4) AS country_entropy_norm\n",
    "FROM stats AS s\n",
    "LEFT JOIN geo_meta AS gm USING (asn)\n",
    "LEFT JOIN country_entropy cm USING (asn)\n",
    "ORDER BY asn\n",
    "\"\"\"\n",
    "\n",
    "ch_df = client.query_df(query)\n",
    "ch_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a528804",
   "metadata": {},
   "source": [
    "## Join both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbb447ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asn',\n",
       " 'org_name',\n",
       " 'country',\n",
       " 'source',\n",
       " 'info_type',\n",
       " 'rank',\n",
       " 'asnDegree_total',\n",
       " 'asnDegree_customer',\n",
       " 'asnDegree_peer',\n",
       " 'asnDegree_provider',\n",
       " 'cone_numberAsns',\n",
       " 'cone_numberPrefixes',\n",
       " 'cone_numberAddresses',\n",
       " 'ASN',\n",
       " 'domains',\n",
       " 'center_lat',\n",
       " 'center_lon',\n",
       " 'total_weight',\n",
       " 'unique_points',\n",
       " 'country_count',\n",
       " 'mean_km',\n",
       " 'var_km2',\n",
       " 'std_km',\n",
       " 'iqr_km',\n",
       " 'p25_km',\n",
       " 'p50_km',\n",
       " 'p75_km',\n",
       " 'p90_km',\n",
       " 'p95_km',\n",
       " 'p99_km',\n",
       " 'min_km',\n",
       " 'max_km',\n",
       " 'pct_ips_le_100km',\n",
       " 'pct_ips_le_500km',\n",
       " 'pct_ips_le_1000km']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peering_df_joined_with_asrank_and_domains_and_geoloc = pd.merge(\n",
    "    peering_df_joined_with_asrank_and_domains,\n",
    "    ch_df,\n",
    "    left_on='asn',\n",
    "    right_on='asn',\n",
    "    how='left'\n",
    ")\n",
    "peering_df_joined_with_asrank_and_domains_and_geoloc['org_name'] = peering_df_joined_with_asrank_and_domains_and_geoloc['org_name'].fillna('unknown').str.lower()\n",
    "peering_df_joined_with_asrank_and_domains_and_geoloc.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfbbfeb",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f565817d",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fce41b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SVM (calibrated) ===\n",
      "Accuracy: 0.5964749536178108\n",
      "Macro-F1: 0.36873452553342556\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       Cable/DSL/ISP       0.63      0.92      0.75      1633\n",
      "             Content       0.48      0.32      0.39       330\n",
      "Educational/Research       0.64      0.49      0.55       199\n",
      "          Enterprise       0.51      0.20      0.29       233\n",
      "          Government       0.54      0.39      0.45        18\n",
      "                 NSP       0.39      0.19      0.26       542\n",
      "    Network Services       0.33      0.04      0.07       110\n",
      "          Non-Profit       0.72      0.40      0.52        84\n",
      "     Route Collector       0.00      0.00      0.00         4\n",
      "        Route Server       0.59      0.32      0.42        81\n",
      "\n",
      "            accuracy                           0.60      3234\n",
      "           macro avg       0.48      0.33      0.37      3234\n",
      "        weighted avg       0.56      0.60      0.54      3234\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# ==== Daten ====\n",
    "df = peering_df_joined_with_asrank_and_domains_and_geoloc.copy()\n",
    "df[\"org_name\"] = df[\"org_name\"].fillna(\"unknown\").str.lower()\n",
    "valid = df[\"info_type\"].value_counts()\n",
    "df = df[df[\"info_type\"].isin(valid[valid >= 5].index)]  # sehr kleine Klassen raus (optional)\n",
    "\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    df[\"org_name\"], df[\"info_type\"], test_size=0.13, random_state=42, stratify=df[\"info_type\"]\n",
    ")\n",
    "\n",
    "# Gemeinsamer Vectorizer (fit nur auf Train!)\n",
    "vec = TfidfVectorizer(analyzer=\"char\", ngram_range=(1,6),\n",
    "                      lowercase=True, min_df=1, sublinear_tf=True)\n",
    "\n",
    "# ==== 1) SVM + Kalibrierung ====\n",
    "svm = LinearSVC(C=0.35, class_weight=\"balanced\")\n",
    "svm_cal = CalibratedClassifierCV(svm, method=\"sigmoid\", cv=3)\n",
    "\n",
    "svm_pipe = Pipeline([\n",
    "    (\"tfidf\", vec),\n",
    "    (\"svm_cal\", svm_cal)\n",
    "])\n",
    "\n",
    "svm_pipe.fit(X_train_text, y_train)\n",
    "y_pred_svm = svm_pipe.predict(X_test_text)\n",
    "print(\"\\n=== SVM (calibrated) ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"Macro-F1:\", f1_score(y_test, y_pred_svm, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a706a136",
   "metadata": {},
   "source": [
    "## Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "310a1d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TF-IDF(Text) + Numerik -> Calibrated LinearSVC ===\n",
      "Accuracy: 0.6221397649969078\n",
      "Macro-F1: 0.4139509901721804\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       Cable/DSL/ISP       0.66      0.92      0.77      1633\n",
      "             Content       0.49      0.35      0.41       330\n",
      "Educational/Research       0.64      0.56      0.60       199\n",
      "          Enterprise       0.51      0.25      0.34       233\n",
      "          Government       0.64      0.50      0.56        18\n",
      "                 NSP       0.48      0.25      0.33       542\n",
      "    Network Services       0.40      0.04      0.07       110\n",
      "          Non-Profit       0.74      0.40      0.52        84\n",
      "     Route Collector       0.00      0.00      0.00         4\n",
      "        Route Server       0.60      0.51      0.55        81\n",
      "\n",
      "            accuracy                           0.62      3234\n",
      "           macro avg       0.52      0.38      0.41      3234\n",
      "        weighted avg       0.59      0.62      0.58      3234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === TF-IDF (org_name) + Numerik -> Calibrated LinearSVC ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# ---------- Daten vorbereiten ----------\n",
    "df = peering_df_joined_with_asrank_and_domains_and_geoloc.copy()\n",
    "\n",
    "# Text normalisieren\n",
    "df[\"org_name\"] = df[\"org_name\"].fillna(\"unknown\").astype(str).str.lower()\n",
    "\n",
    "# Sehr kleine Klassen optional rausfiltern\n",
    "valid = df[\"info_type\"].value_counts()\n",
    "df = df[df[\"info_type\"].isin(valid[valid >= 5].index)].reset_index(drop=True)\n",
    "\n",
    "# Zielspalte\n",
    "y = df[\"info_type\"].astype(str)\n",
    "\n",
    "# Numerische Spalten bestimmen (alles außer Text/Kat/Label)\n",
    "ignore = {\"org_name\", \"info_type\", \"country\", \"source\"}\n",
    "num_candidates = [c for c in df.columns if c not in ignore]\n",
    "\n",
    "# Nur numerisch verwertbare Spalten (coerce -> float)\n",
    "num_df = df[num_candidates].apply(pd.to_numeric, errors=\"coerce\")\n",
    "num_df = num_df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "num_cols = num_df.columns.tolist()\n",
    "\n",
    "# Feature-DataFrame für Pipeline\n",
    "X = pd.concat([df[[\"org_name\"]].reset_index(drop=True),\n",
    "               num_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# ---------- Train/Test Split ----------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.13, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ---------- Preprocessing ----------\n",
    "def log1p_array(A):\n",
    "    A = np.asarray(A, dtype=float)\n",
    "    A = np.clip(A, a_min=0.0, a_max=None)\n",
    "    return np.log1p(A)\n",
    "\n",
    "num_pipe = Pipeline(steps=[\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"log1p\", FunctionTransformer(log1p_array, validate=True)),\n",
    "    (\"scale\", MaxAbsScaler())\n",
    "])\n",
    "\n",
    "pre = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"text\", TfidfVectorizer(analyzer=\"char\", ngram_range=(2,6),\n",
    "                                 lowercase=True, sublinear_tf=True, min_df=1),\n",
    "         \"org_name\"),\n",
    "        (\"num\",  num_pipe, num_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.3\n",
    ")\n",
    "\n",
    "# ---------- SVM + Kalibrierung ----------\n",
    "base_svm = LinearSVC(C=0.35, class_weight=\"balanced\", random_state=42)\n",
    "cal = CalibratedClassifierCV(base_svm, cv=3, method=\"sigmoid\")  # robust\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"svm\", cal)\n",
    "])\n",
    "\n",
    "# ---------- Trainieren ----------\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# ---------- Evaluieren ----------\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(\"\\n=== TF-IDF(Text) + Numerik -> Calibrated LinearSVC ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Macro-F1:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# ---------- (Optional) Probas für Ensembling ----------\n",
    "# P_test = pipe.predict_proba(X_test)   # shape [N, n_classes]\n",
    "# classes_ = pipe.named_steps[\"svm\"].classes_.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca5a40ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# TF-IDF (org_name) + Numerik + country\n",
    "# -> LinearSVC (unkalibriert)  und  SGD(modified_huber, mit Probas)\n",
    "# ===============================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, MaxAbsScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def amplify_numeric(X, factor=3.0):\n",
    "    return X * factor\n",
    "\n",
    "# ==== 0) Quelle laden (passe ggf. den DF-Namen an) ====\n",
    "df = peering_df_joined_with_asrank_and_domains_and_geoloc.copy()\n",
    "\n",
    "# ==== 1) Vorverarbeitung & Label-Filter ====\n",
    "df[\"org_name\"] = df[\"org_name\"].fillna(\"unknown\").astype(str).str.lower()\n",
    "valid = df[\"info_type\"].value_counts()\n",
    "df = df[df[\"info_type\"].isin(valid[valid >= 5].index)].reset_index(drop=True)\n",
    "\n",
    "# Ziel\n",
    "y = df[\"info_type\"].astype(str)\n",
    "\n",
    "# ==== 2) Feature-Spalten bestimmen ====\n",
    "# Wir nehmen country dazu (starkes Signal) und alle numerisch konvertierbaren Spalten\n",
    "ignore = {\"org_name\", \"info_type\", \"source\"}  # 'country' NICHT ignorieren\n",
    "all_cols = df.columns.tolist()\n",
    "text_col = \"org_name\"\n",
    "cat_cols  = [\"country\"] if \"country\" in df.columns else []\n",
    "\n",
    "# numerische Kandidaten = alles außer Text/Label/Source/country\n",
    "num_candidates = [c for c in all_cols if c not in ignore.union({text_col}).union(set(cat_cols))]\n",
    "num_df = df[num_candidates].apply(pd.to_numeric, errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "num_cols = num_df.columns.tolist()\n",
    "\n",
    "# Endgültiges X-DF\n",
    "X = pd.concat([df[[text_col] + cat_cols].reset_index(drop=True),\n",
    "               num_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# ==== 3) Train/Test Split ====\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.13, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ==== 4) Preprocessor bauen ====\n",
    "# Numerik: Impute -> log1p -> MaxAbsScaler (gut in Kombi mit TF-IDF, bleibt sparse-freundlich)\n",
    "def log1p_array(A):\n",
    "    A = np.asarray(A, dtype=float)\n",
    "    A = np.where(np.isfinite(A), A, np.nan)\n",
    "    # Median-Imputation passiert vorher; hier nur Sicherung\n",
    "    A = np.clip(A, a_min=0.0, a_max=None)\n",
    "    return np.log1p(A)\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"log1p\", FunctionTransformer(log1p_array, validate=True)),\n",
    "    (\"scale\", MaxAbsScaler()),\n",
    "    (\"boost\", FunctionTransformer(lambda X: X * 3.0, validate=False)),  # Gewichtung!\n",
    "])\n",
    "\n",
    "transformers = [\n",
    "    (\"text\", TfidfVectorizer(analyzer=\"char\", ngram_range=(2,6),\n",
    "                             lowercase=True, sublinear_tf=True, min_df=1), text_col),\n",
    "]\n",
    "if cat_cols:\n",
    "    transformers.append((\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols))\n",
    "if num_cols:\n",
    "    transformers.append((\"num\", num_pipe, num_cols))\n",
    "\n",
    "pre = ColumnTransformer(transformers, remainder=\"drop\", sparse_threshold=0.3)\n",
    "\n",
    "# ==== 5) MODELL A: LinearSVC (unkalibriert) ====\n",
    "svm_linear = LinearSVC(\n",
    "    C=0.35,\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=5000,          # Konvergenz sichern\n",
    "    random_state=42\n",
    ")\n",
    "pipe_svm = Pipeline([(\"pre\", pre), (\"clf\", svm_linear)])\n",
    "pipe_svm.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svm = pipe_svm.predict(X_test)\n",
    "print(\"\\n=== LinearSVC (TF-IDF + Numerik + country) ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"Macro-F1:\", f1_score(y_test, y_pred_svm, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred_svm, zero_division=0))\n",
    "\n",
    "# ==== 6) MODELL B: SGDClassifier (modified_huber) -> liefert predict_proba ====\n",
    "sgd = SGDClassifier(\n",
    "    loss=\"modified_huber\",   # SVM-ähnlich, aber mit Probas\n",
    "    alpha=1e-4,\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=5000,\n",
    "    tol=1e-3,\n",
    "    random_state=42\n",
    ")\n",
    "pipe_sgd = Pipeline([(\"pre\", pre), (\"clf\", sgd)])\n",
    "pipe_sgd.fit(X_train, y_train)\n",
    "\n",
    "y_pred_sgd = pipe_sgd.predict(X_test)\n",
    "print(\"\\n=== SGD(modified_huber) (TF-IDF + Numerik + country) ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_sgd))\n",
    "print(\"Macro-F1:\", f1_score(y_test, y_pred_sgd, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred_sgd, zero_division=0))\n",
    "\n",
    "# Optional: Probas (für Ensembling/Routing)\n",
    "# P_test = pipe_sgd.predict_proba(X_test)\n",
    "# classes_ = pipe_sgd.named_steps[\"clf\"].classes_.tolist()\n",
    "\n",
    "# ==== 7) Mini-Tuning (optional, schnell) ====\n",
    "# Wenn du noch 2-3 Punkte rausholen willst, probier leicht andere C/alpha:\n",
    "#   - LinearSVC: C in [0.25, 0.35, 0.5, 0.75, 1.0]\n",
    "#   - SGD alpha in [5e-5, 1e-4, 2e-4]\n",
    "# Oder n-gram Range auf (2,7) testen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87acba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- schnelle & stabile Pipeline ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "df = peering_df_joined_with_asrank_and_domains_and_geoloc.copy()\n",
    "\n",
    "# Labels filtern\n",
    "df[\"org_name\"] = df[\"org_name\"].fillna(\"unknown\").astype(str).str.lower()\n",
    "valid = df[\"info_type\"].value_counts()\n",
    "df = df[df[\"info_type\"].isin(valid[valid >= 5].index)].reset_index(drop=True)\n",
    "y = df[\"info_type\"].astype(str)\n",
    "\n",
    "# Spalten\n",
    "ignore = {\"org_name\", \"info_type\", \"source\"}\n",
    "text_col = \"org_name\"\n",
    "cat_cols  = [\"country\"] if \"country\" in df.columns else []\n",
    "num_candidates = [c for c in df.columns if c not in ignore.union({text_col}).union(set(cat_cols))]\n",
    "num_df = df[num_candidates].apply(pd.to_numeric, errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "num_cols = num_df.columns.tolist()\n",
    "\n",
    "X = pd.concat([df[[text_col] + cat_cols].reset_index(drop=True),\n",
    "               num_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.13, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "def log1p_array(A):\n",
    "    A = np.asarray(A, dtype=float)\n",
    "    A = np.where(np.isfinite(A), A, np.nan)\n",
    "    A = np.clip(A, 0.0, None)\n",
    "    return np.log1p(A)\n",
    "\n",
    "# Numerik (klein → darf dicht sein)\n",
    "NUM_BOOST = 8.0\n",
    "num_pipe = Pipeline([\n",
    "    (\"imp\",   SimpleImputer(strategy=\"median\")),\n",
    "    (\"log1p\", FunctionTransformer(log1p_array, validate=True)),\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"boost\", FunctionTransformer(lambda X: X * NUM_BOOST, validate=False)),\n",
    "])\n",
    "\n",
    "# Text: TF-IDF → SVD (SVD reduziert massiv! DANN ist Dichte ok)\n",
    "text_pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        analyzer=\"char\", ngram_range=(2, 6),\n",
    "        lowercase=True, sublinear_tf=True,\n",
    "        min_df=3,          # etwas strenger für Speed\n",
    "        dtype=np.float32\n",
    "    )),\n",
    "    (\"svd\", TruncatedSVD(n_components=350, random_state=42, n_iter=5)),\n",
    "])\n",
    "\n",
    "# Country: OHE (wenig Kardinalität → dicht ok)\n",
    "try:\n",
    "    cat_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "except TypeError:\n",
    "    cat_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "transformers = [(\"text\", text_pipe, text_col)]\n",
    "if cat_cols: transformers.append((\"cat\", cat_encoder, cat_cols))\n",
    "if num_cols: transformers.append((\"num\", num_pipe, num_cols))\n",
    "\n",
    "# WICHTIG: KEIN sparse_threshold=0.0 -> nichts wird unnötig verdichtet\n",
    "pre = ColumnTransformer(transformers, remainder=\"drop\")\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    solver=\"saga\",\n",
    "    penalty=\"l2\",\n",
    "    C=2.0,\n",
    "    max_iter=4000,               # dank SVD meist ausreichend\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    "    multi_class=\"multinomial\",\n",
    ")\n",
    "\n",
    "pipe = Pipeline([(\"pre\", pre), (\"clf\", clf)])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(\"\\n=== Fast LogisticRegression (TFIDF->SVD + OHE + Num[boost]) ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Macro-F1:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c3cb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Nur Numerik -> HistGradientBoosting ===\n",
      "Accuracy: 0.44712430426716143\n",
      "Macro-F1: 0.26504239987683176\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       Cable/DSL/ISP       0.81      0.55      0.66      1633\n",
      "             Content       0.41      0.39      0.40       330\n",
      "Educational/Research       0.30      0.25      0.27       199\n",
      "          Enterprise       0.23      0.25      0.24       233\n",
      "          Government       0.05      0.28      0.08        18\n",
      "                 NSP       0.40      0.32      0.36       542\n",
      "    Network Services       0.10      0.18      0.13       110\n",
      "          Non-Profit       0.18      0.33      0.23        84\n",
      "     Route Collector       0.00      0.00      0.00         4\n",
      "        Route Server       0.16      0.93      0.28        81\n",
      "\n",
      "            accuracy                           0.45      3234\n",
      "           macro avg       0.26      0.35      0.27      3234\n",
      "        weighted avg       0.57      0.45      0.49      3234\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 109\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# ---------- 8) (Optional) Feature-Importance per Permutation ----------\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Hinweis: kann etwas dauern; zeigt dir, welche Numerik-Spalten wirklich tragen.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minspection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m permutation_importance\n\u001b[0;32m--> 109\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mpermutation_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# ColumnTransformer erzeugt eine kombinierte Featureliste:\u001b[39;00m\n\u001b[1;32m    111\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/inspection/_permutation_importance.py:259\u001b[0m, in \u001b[0;36mpermutation_importance\u001b[0;34m(estimator, X, y, scoring, n_repeats, n_jobs, random_state, sample_weight, max_samples)\u001b[0m\n\u001b[1;32m    255\u001b[0m     scorer \u001b[38;5;241m=\u001b[39m _MultimetricScorer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscorers_dict)\n\u001b[1;32m    257\u001b[0m baseline_score \u001b[38;5;241m=\u001b[39m _weights_scorer(scorer, estimator, X, y, sample_weight)\n\u001b[0;32m--> 259\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_calculate_permutation_scores\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(baseline_score, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    276\u001b[0m         name: _create_importances_bunch(\n\u001b[1;32m    277\u001b[0m             baseline_score[name],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m baseline_score\n\u001b[1;32m    282\u001b[0m     }\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py:1855\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1853\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1854\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1855\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1860\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1861\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py:1784\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1784\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/inspection/_permutation_importance.py:64\u001b[0m, in \u001b[0;36m_calculate_permutation_scores\u001b[0;34m(estimator, X, y, sample_weight, col_idx, random_state, n_repeats, scorer, max_samples)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m         X_permuted[:, col_idx] \u001b[38;5;241m=\u001b[39m X_permuted[shuffling_idx, col_idx]\n\u001b[0;32m---> 64\u001b[0m     scores\u001b[38;5;241m.\u001b[39mappend(\u001b[43m_weights_scorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_permuted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scores[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     67\u001b[0m     scores \u001b[38;5;241m=\u001b[39m _aggregate_score_dicts(scores)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/inspection/_permutation_importance.py:19\u001b[0m, in \u001b[0;36m_weights_scorer\u001b[0;34m(scorer, estimator, X, y, sample_weight)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scorer(estimator, X, y, sample_weight)\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_scorer.py:430\u001b[0m, in \u001b[0;36m_passthrough_scorer\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_passthrough_scorer\u001b[39m(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    429\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Function that wraps estimator.score\"\"\"\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py:723\u001b[0m, in \u001b[0;36mPipeline.score\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    722\u001b[0m     score_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[0;32m--> 723\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscore_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py:638\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;124;03mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03m    Mean accuracy of ``self.predict(X)`` wrt. `y`.\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m--> 638\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy_score(y, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1874\u001b[0m, in \u001b[0;36mHistGradientBoostingClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1861\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict classes for X.\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m \n\u001b[1;32m   1863\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1871\u001b[0m \u001b[38;5;124;03m    The predicted classes.\u001b[39;00m\n\u001b[1;32m   1872\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1873\u001b[0m \u001b[38;5;66;03m# TODO: This could be done in parallel\u001b[39;00m\n\u001b[0;32m-> 1874\u001b[0m encoded_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1875\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[encoded_classes]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1912\u001b[0m, in \u001b[0;36mHistGradientBoostingClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_proba\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m   1900\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Predict class probabilities for X.\u001b[39;00m\n\u001b[1;32m   1901\u001b[0m \n\u001b[1;32m   1902\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1910\u001b[0m \u001b[38;5;124;03m        The class probabilities of the input samples.\u001b[39;00m\n\u001b[1;32m   1911\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1912\u001b[0m     raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1913\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss\u001b[38;5;241m.\u001b[39mpredict_proba(raw_predictions)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1043\u001b[0m, in \u001b[0;36mBaseHistGradientBoosting._raw_predict\u001b[0;34m(self, X, n_threads)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m# We intentionally decouple the number of threads used at prediction\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;66;03m# time from the number of threads used at fit time because the model\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;66;03m# can be deployed on a different machine for prediction purposes.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m n_threads \u001b[38;5;241m=\u001b[39m _openmp_effective_n_threads(n_threads)\n\u001b[0;32m-> 1043\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_iterations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predictors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_binned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m raw_predictions\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:1071\u001b[0m, in \u001b[0;36mBaseHistGradientBoosting._predict_iterations\u001b[0;34m(self, X, predictors, raw_predictions, is_binned, n_threads)\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m     predict \u001b[38;5;241m=\u001b[39m partial(\n\u001b[1;32m   1066\u001b[0m         predictor\u001b[38;5;241m.\u001b[39mpredict,\n\u001b[1;32m   1067\u001b[0m         known_cat_bitsets\u001b[38;5;241m=\u001b[39mknown_cat_bitsets,\n\u001b[1;32m   1068\u001b[0m         f_idx_map\u001b[38;5;241m=\u001b[39mf_idx_map,\n\u001b[1;32m   1069\u001b[0m         n_threads\u001b[38;5;241m=\u001b[39mn_threads,\n\u001b[1;32m   1070\u001b[0m     )\n\u001b[0;32m-> 1071\u001b[0m raw_predictions[:, k] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_hist_gradient_boosting/predictor.py:69\u001b[0m, in \u001b[0;36mTreePredictor.predict\u001b[0;34m(self, X, known_cat_bitsets, f_idx_map, n_threads)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict raw values for non-binned data.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    The raw predicted values.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mY_DTYPE)\n\u001b[0;32m---> 69\u001b[0m \u001b[43m_predict_from_raw_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_left_cat_bitsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mknown_cat_bitsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf_idx_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# === Nur Numerik -> HistGradientBoostingClassifier (mit Log1p für schiefe Spalten) ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa: F401\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# ---------- 1) Daten vorbereiten ----------\n",
    "# Nimm den gleichen DF wie in deinen letzten Läufen:\n",
    "df = peering_df_joined_with_asrank_and_domains_and_geoloc.copy()\n",
    "\n",
    "# Optional: sehr kleine Klassen entfernen (wie zuvor)\n",
    "valid = df[\"info_type\"].value_counts()\n",
    "df = df[df[\"info_type\"].isin(valid[valid >= 5].index)].reset_index(drop=True)\n",
    "\n",
    "# Ziel\n",
    "y = df[\"info_type\"].astype(str)\n",
    "\n",
    "# Kandidaten für Numerik: alles außer offensichtlichem Text/Kat/Label\n",
    "ignore = {\"org_name\", \"info_type\", \"country\", \"source\"}\n",
    "num_candidates = [c for c in df.columns if c not in ignore]\n",
    "num_candidates.remove(\"asn\")  # ASN ist numerisch, aber keine sinnvolle Zahl\n",
    "\n",
    "# In numerisch zwingen; Unendlichkeiten -> NaN\n",
    "num_df = df[num_candidates].apply(pd.to_numeric, errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Falls Spalten komplett NaN sind, droppen\n",
    "num_df = num_df.drop(columns=num_df.columns[num_df.isna().all()], errors=\"ignore\")\n",
    "\n",
    "# Endgültige Numerik-Spaltenliste\n",
    "num_cols = num_df.columns.tolist()\n",
    "\n",
    "# Feature-DF\n",
    "X = num_df.copy()\n",
    "\n",
    "# ---------- 2) Split ----------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.13, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ---------- 3) Spalten in log/linear trennen ----------\n",
    "# Log1p nur für Spalten, die (nach Imputation) nichtnegativ sind\n",
    "# Wir schätzen die Nichtnegativität vorab grob über die Trainingsdaten (NaN -> 0 für Prüfung)\n",
    "_train_tmp = X_train.copy()\n",
    "_train_tmp = _train_tmp.fillna(0)\n",
    "log_cols = [c for c in _train_tmp.columns if _train_tmp[c].min() >= 0.0]\n",
    "lin_cols = [c for c in _train_tmp.columns if c not in log_cols]\n",
    "\n",
    "def log1p_array(A):\n",
    "    A = np.asarray(A, dtype=float)\n",
    "    # A kommt nach Imputation; zur Sicherheit clippen\n",
    "    A = np.clip(A, a_min=0.0, a_max=None)\n",
    "    return np.log1p(A)\n",
    "\n",
    "# ---------- 4) Preprocessing ----------\n",
    "num_log_pipe = Pipeline([\n",
    "    (\"imp\",   SimpleImputer(strategy=\"median\")),\n",
    "    (\"log1p\", FunctionTransformer(log1p_array, validate=True)),\n",
    "    (\"sc\",    StandardScaler(with_mean=False)),\n",
    "])\n",
    "\n",
    "num_lin_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"sc\",  StandardScaler(with_mean=False)),\n",
    "])\n",
    "\n",
    "transformers = []\n",
    "if log_cols:\n",
    "    transformers.append((\"num_log\", num_log_pipe, log_cols))\n",
    "if lin_cols:\n",
    "    transformers.append((\"num_lin\", num_lin_pipe, lin_cols))\n",
    "\n",
    "pre = ColumnTransformer(transformers, remainder=\"drop\")\n",
    "\n",
    "# ---------- 5) Modell ----------\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.06,\n",
    "    max_leaf_nodes=31,\n",
    "    min_samples_leaf=20,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"pre\", pre),\n",
    "    (\"clf\", hgb)\n",
    "])\n",
    "\n",
    "# ---------- 6) Train ----------\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# ---------- 7) Eval ----------\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(\"\\n=== Nur Numerik -> HistGradientBoosting ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Macro-F1:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# ---------- 8) (Optional) Feature-Importance per Permutation ----------\n",
    "# Hinweis: kann etwas dauern; zeigt dir, welche Numerik-Spalten wirklich tragen.\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "res = permutation_importance(pipe, X_test, y_test, n_repeats=5, random_state=42)\n",
    "# ColumnTransformer erzeugt eine kombinierte Featureliste:\n",
    "feature_names = []\n",
    "if log_cols:\n",
    "    feature_names += [f\"[log] {c}\" for c in log_cols]\n",
    "if lin_cols:\n",
    "    feature_names += [f\"[lin] {c}\" for c in lin_cols]\n",
    "\n",
    "imp = pd.Series(res.importances_mean, index=feature_names).sort_values(ascending=False)\n",
    "print(\"\\nTop-20 Numerik-Features (Permutation Importance):\")\n",
    "print(imp.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f31e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# TF-IDF (org_name) + Numerik + country\n",
    "# -> LinearSVC (unkalibriert)  und  SGD(modified_huber, mit Probas)\n",
    "# == Numerik gezielt boosten + Wirkung prüfen\n",
    "# ===============================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, MaxAbsScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# ==== 0) Quelle laden ====\n",
    "df = peering_df_joined_with_asrank_and_domains_and_geoloc.copy()\n",
    "\n",
    "# ==== 1) Vorverarbeitung & Label-Filter ====\n",
    "df[\"org_name\"] = df[\"org_name\"].fillna(\"unknown\").astype(str).str.lower()\n",
    "valid = df[\"info_type\"].value_counts()\n",
    "df = df[df[\"info_type\"].isin(valid[valid >= 5].index)].reset_index(drop=True)\n",
    "\n",
    "y = df[\"info_type\"].astype(str)\n",
    "\n",
    "# ==== 2) Feature-Spalten ====\n",
    "ignore = {\"org_name\", \"info_type\", \"source\"}  # 'country' NICHT ignorieren\n",
    "all_cols = df.columns.tolist()\n",
    "text_col = \"org_name\"\n",
    "cat_cols  = [\"country\"] if \"country\" in df.columns else []\n",
    "\n",
    "num_candidates = [c for c in all_cols if c not in ignore.union({text_col}).union(set(cat_cols))]\n",
    "num_df = df[num_candidates].apply(pd.to_numeric, errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "num_cols = num_df.columns.tolist()\n",
    "\n",
    "X = pd.concat([df[[text_col] + cat_cols].reset_index(drop=True),\n",
    "               num_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# ==== 3) Split ====\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.13, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ==== 4) Preprocessor ====\n",
    "def log1p_array(A):\n",
    "    A = np.asarray(A, dtype=float)\n",
    "    A = np.where(np.isfinite(A), A, np.nan)\n",
    "    A = np.clip(A, a_min=0.0, a_max=None)\n",
    "    return np.log1p(A)\n",
    "\n",
    "# >>> EINZIGER KNOPF: Boost-Faktor für Numerik <<<\n",
    "NUM_BOOST = 8.0   # typ. 5–15 probieren; 8 ist guter Start\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    (\"imp\",   SimpleImputer(strategy=\"median\")),\n",
    "    (\"log1p\", FunctionTransformer(log1p_array, validate=True)),\n",
    "    (\"scale\", MaxAbsScaler()),                             # sparse-freundlich\n",
    "    (\"boost\", FunctionTransformer(lambda X: X * NUM_BOOST, # hier boosten!\n",
    "                                  validate=False)),\n",
    "])\n",
    "\n",
    "# OneHotEncoder explizit SPARSE lassen (Performance, kein Densify)\n",
    "try:\n",
    "    cat_enc = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "except TypeError:\n",
    "    cat_enc = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "\n",
    "transformers = [\n",
    "    (\"text\", TfidfVectorizer(analyzer=\"char\", ngram_range=(2,6),\n",
    "                             lowercase=True, sublinear_tf=True, min_df=1,\n",
    "                             dtype=np.float32), text_col),\n",
    "]\n",
    "if cat_cols:\n",
    "    transformers.append((\"cat\", cat_enc, cat_cols))\n",
    "if num_cols:\n",
    "    transformers.append((\"num\", num_pipe, num_cols))\n",
    "\n",
    "# Wichtig: NICHT künstlich verdichten\n",
    "pre = ColumnTransformer(transformers, remainder=\"drop\")  # kein sparse_threshold=0.0\n",
    "\n",
    "# ==== 5) MODELL A: LinearSVC ====\n",
    "svm_linear = LinearSVC(\n",
    "    C=0.5,                      # etwas höher, weil wir Numerik pushen\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=5000,\n",
    "    random_state=42\n",
    ")\n",
    "pipe_svm = Pipeline([(\"pre\", pre), (\"clf\", svm_linear)])\n",
    "pipe_svm.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svm = pipe_svm.predict(X_test)\n",
    "print(\"\\n=== LinearSVC (TF-IDF + Numerik*{:.1f} + country) ===\".format(NUM_BOOST))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"Macro-F1:\", f1_score(y_test, y_pred_svm, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred_svm, zero_division=0))\n",
    "\n",
    "# ==== 6) MODELL B: SGD (modified_huber) ====\n",
    "sgd = SGDClassifier(\n",
    "    loss=\"modified_huber\",\n",
    "    alpha=1e-4,                 # ggf. auf 5e-5 senken, wenn du noch mehr Numerik willst\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=5000,\n",
    "    tol=1e-3,\n",
    "    random_state=42\n",
    ")\n",
    "pipe_sgd = Pipeline([(\"pre\", pre), (\"clf\", sgd)])\n",
    "pipe_sgd.fit(X_train, y_train)\n",
    "\n",
    "y_pred_sgd = pipe_sgd.predict(X_test)\n",
    "print(\"\\n=== SGD(modified_huber) (TF-IDF + Numerik*{:.1f} + country) ===\".format(NUM_BOOST))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_sgd))\n",
    "print(\"Macro-F1:\", f1_score(y_test, y_pred_sgd, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred_sgd, zero_division=0))\n",
    "\n",
    "# ==== 7) Kurzer Wirkungs-Check: tragen die Numerik-Koeffizienten jetzt mehr? ====\n",
    "def block_norm_ratio(pipe):\n",
    "    pre = pipe.named_steps[\"pre\"]\n",
    "    clf = pipe.named_steps[\"clf\"]\n",
    "    names = pre.get_feature_names_out()\n",
    "\n",
    "    # Indexe der Numerik-Features\n",
    "    num_idx = np.array([i for i, n in enumerate(names) if n.startswith(\"num__\")])\n",
    "    if num_idx.size == 0 or not hasattr(clf, \"coef_\"):\n",
    "        return np.nan\n",
    "\n",
    "    # multi-class: L2-Norm pro Klasse mitteln\n",
    "    W = clf.coef_          # shape [K, D] (bei binary ggf. [1, D])\n",
    "    num_norm = np.mean(np.linalg.norm(W[:, num_idx], axis=1))\n",
    "    all_norm = np.mean(np.linalg.norm(W, axis=1))\n",
    "    return float(num_norm / (all_norm + 1e-12))\n",
    "\n",
    "print(\"\\n[Diagnose] Anteil der Numerik-Koeffizienten (L2-Norm) an Gesamt:\")\n",
    "print(\"LinearSVC  num/all =\", block_norm_ratio(pipe_svm))\n",
    "print(\"SGD        num/all =\", block_norm_ratio(pipe_sgd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a469bc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['asn', 'org_name', 'country', 'source', 'info_type', 'rank',\n",
       "       'asnDegree_total', 'asnDegree_customer', 'asnDegree_peer',\n",
       "       'asnDegree_provider', 'cone_numberAsns', 'cone_numberPrefixes',\n",
       "       'cone_numberAddresses', 'ASN', 'domains', 'center_lat', 'center_lon',\n",
       "       'total_weight', 'unique_points', 'country_count', 'mean_km', 'var_km2',\n",
       "       'std_km', 'iqr_km', 'p25_km', 'p50_km', 'p75_km', 'p90_km', 'p95_km',\n",
       "       'p99_km', 'min_km', 'max_km', 'pct_ips_le_100km', 'pct_ips_le_500km',\n",
       "       'pct_ips_le_1000km'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peering_df_joined_with_asrank_and_domains_and_geoloc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe91f249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "org_name",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "70a26df4-8cbc-40af-95e6-005d031dab7a",
       "rows": [
        [
         "0",
         "gtt americas, llc"
        ],
        [
         "1",
         "akamai international b.v."
        ],
        [
         "2",
         "dalnet"
        ],
        [
         "3",
         "swisscom (schweiz) ag"
        ],
        [
         "4",
         "cox communications inc."
        ],
        [
         "5",
         "rcn"
        ],
        [
         "6",
         "charter communications inc"
        ],
        [
         "7",
         "arelion sweden ab"
        ],
        [
         "8",
         "at&t enterprises, llc"
        ],
        [
         "9",
         "verizon business"
        ],
        [
         "10",
         "gtt communications inc."
        ],
        [
         "11",
         "bbc"
        ],
        [
         "12",
         "webpass inc."
        ],
        [
         "13",
         "oracle america inc."
        ],
        [
         "14",
         "rackspace hosting"
        ],
        [
         "15",
         "korea telecom"
        ],
        [
         "16",
         "verizon business"
        ],
        [
         "17",
         "yahoo holdings inc."
        ],
        [
         "18",
         "windstream communications llc"
        ],
        [
         "19",
         "bouygues telecom sa"
        ],
        [
         "20",
         "sinister and solitary"
        ],
        [
         "21",
         "telecom italia sparkle s.p.a."
        ],
        [
         "22",
         "gtt communications inc."
        ],
        [
         "23",
         "cologix, inc"
        ],
        [
         "24",
         "jerome's furniture"
        ],
        [
         "25",
         "telstra international limited"
        ],
        [
         "26",
         "level 3 parent, llc"
        ],
        [
         "27",
         "telus communications inc."
        ],
        [
         "28",
         "rock island communications"
        ],
        [
         "29",
         "yahoo holdings inc."
        ],
        [
         "30",
         "tch network services"
        ],
        [
         "31",
         "easynet global services"
        ],
        [
         "32",
         "easynet global services"
        ],
        [
         "33",
         "enet communications limited"
        ],
        [
         "34",
         "british telecommunications plc"
        ],
        [
         "35",
         "rm education ltd"
        ],
        [
         "36",
         "plactrix technologies"
        ],
        [
         "37",
         "netcologne gesellschaft fur telekommunikation mbh"
        ],
        [
         "38",
         "zen internet ltd"
        ],
        [
         "39",
         "itility limited"
        ],
        [
         "40",
         "domainmaster ltd"
        ],
        [
         "41",
         "digital space group limited"
        ],
        [
         "42",
         "digi romania s.a."
        ],
        [
         "43",
         "gtt americas, llc"
        ],
        [
         "44",
         "orange business communications technology limited"
        ],
        [
         "45",
         "sunrise gmbh"
        ],
        [
         "46",
         "telefonica germany gmbh & co.ohg"
        ],
        [
         "47",
         "vodafone limited"
        ],
        [
         "48",
         "british telecommunications plc"
        ],
        [
         "49",
         "free sas"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 24875
       }
      },
      "text/plain": [
       "0                                        gtt americas, llc\n",
       "1                                akamai international b.v.\n",
       "2                                                   dalnet\n",
       "3                                    swisscom (schweiz) ag\n",
       "4                                  cox communications inc.\n",
       "                               ...                        \n",
       "24870    max technology & support services private limited\n",
       "24871                                                  NaN\n",
       "24872                                                  NaN\n",
       "24873                                      bjoern schleyer\n",
       "24874                                         kiwi telecom\n",
       "Name: org_name, Length: 24875, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peering_df_joined_with_asrank_and_domains_and_geoloc['org_name'].fillna('unknown').str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3cf983",
   "metadata": {},
   "outputs": [],
   "source": [
    "peering_df_joined_with_asrank_and_domains_and_geoloc['org_name'] = peering_df_joined_with_asrank_and_domains_and_geoloc['org_name'].fillna('unknown').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13601f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] Macro-F1 mit per-Klasse-Thresholds: 0.2888\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 178\u001b[0m\n\u001b[1;32m    175\u001b[0m class_weights_train \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (class_counts_train \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m alpha))\n\u001b[1;32m    176\u001b[0m sw_train \u001b[38;5;241m=\u001b[39m class_weights_train[y_train]\n\u001b[0;32m--> 178\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel__sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msw_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m P_test \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# a) Thresholded Predictions\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py:406\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    405\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 406\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py:1490\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1462\u001b[0m (\n\u001b[1;32m   1463\u001b[0m     model,\n\u001b[1;32m   1464\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1470\u001b[0m )\n\u001b[1;32m   1471\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1472\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1473\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1487\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1488\u001b[0m )\n\u001b[0;32m-> 1490\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1505\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1918\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1922\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import FunctionTransformer, RobustScaler, StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer, RobustScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "\n",
    "df = peering_df_joined_with_asrank_and_domains_and_geoloc.copy()\n",
    "X = df[['org_name', 'rank', 'asnDegree_total', 'asnDegree_customer', 'asnDegree_peer', 'asnDegree_provider', 'cone_numberAsns', 'cone_numberPrefixes',\n",
    "    'cone_numberAddresses', 'domains',  'total_weight', 'unique_points', 'country_count', 'mean_km', 'var_km2', 'std_km', 'iqr_km', 'p25_km', 'p50_km', 'p75_km', 'p90_km', 'p95_km',\n",
    "    'p99_km', 'min_km', 'max_km', 'pct_ips_le_100km', 'pct_ips_le_500km', 'pct_ips_le_1000km']]\n",
    "y = df['info_type']\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "n_classes = len(le.classes_)\n",
    "text_col = 'org_name'\n",
    "num_cols = ['rank', 'asnDegree_total', 'asnDegree_customer', 'asnDegree_peer', 'asnDegree_provider', 'cone_numberAsns', 'cone_numberPrefixes',\n",
    "    'cone_numberAddresses', 'domains',  'total_weight', 'unique_points', 'country_count', 'mean_km', 'var_km2', 'std_km', 'iqr_km', 'p25_km', 'p50_km', 'p75_km', 'p90_km', 'p95_km',\n",
    "    'p99_km', 'min_km', 'max_km', 'pct_ips_le_100km', 'pct_ips_le_500km', 'pct_ips_le_1000km']\n",
    "\n",
    "pct_cols = ['pct_ips_le_100km', 'pct_ips_le_500km', 'pct_ips_le_1000km']\n",
    "\n",
    "# Distanz-/Streumaße (km): oft stark rechtsschief\n",
    "km_cols = ['mean_km', 'var_km2', 'std_km', 'iqr_km', 'p25_km', 'p50_km', 'p75_km', 'p90_km', 'p95_km', 'p99_km',\n",
    "           'min_km', 'max_km']\n",
    "\n",
    "# Zähl-/Skalenwerte: Ränge, Degrees, Cones, Domains, Gewichte, Länder, Punkte\n",
    "count_like_cols = ['rank', 'asnDegree_total', 'asnDegree_customer', 'asnDegree_peer', 'asnDegree_provider',\n",
    "                   'cone_numberAsns', 'cone_numberPrefixes', 'cone_numberAddresses',\n",
    "                   'domains', 'total_weight', 'unique_points', 'country_count']\n",
    "\n",
    "# Falls du ASN wirklich als Zahl nutzen willst: packe ihn hier rein\n",
    "# count_like_cols += ['ASN']\n",
    "\n",
    "# --- Teil-Pipelines ---\n",
    "\n",
    "# 1) Count/Skalen: impute -> log1p -> robust scale\n",
    "count_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n",
    "    (\"log1p\", FunctionTransformer(lambda X: np.log1p(np.clip(X, a_min=0, a_max=None)), feature_names_out=\"one-to-one\")),\n",
    "    (\"scale\", RobustScaler())\n",
    "])\n",
    "\n",
    "# 2) km-Metriken: impute -> log1p -> robust scale\n",
    "km_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n",
    "    (\"log1p\", FunctionTransformer(lambda X: np.log1p(np.clip(X, a_min=0, a_max=None)), feature_names_out=\"one-to-one\")),\n",
    "    (\"scale\", RobustScaler())\n",
    "])\n",
    "\n",
    "# 3) Prozente: impute -> clip [0,1] -> standard scale\n",
    "pct_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n",
    "    (\"clip01\", FunctionTransformer(lambda X: np.clip(X, 0.0, 100.0), feature_names_out=\"one-to-one\")),\n",
    "    (\"scale\", StandardScaler())\n",
    "])\n",
    "\n",
    "# --- Text-Pipeline (deine) ---\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "text_pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        analyzer=\"char\",\n",
    "        ngram_range=(2,5),\n",
    "        min_df=1,\n",
    "        lowercase=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "text_col = \"org_name\"\n",
    "\n",
    "# --- ColumnTransformer zusammenbauen ---\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"text\", text_pipe, text_col),\n",
    "        (\"num_count\", count_pipe, count_like_cols),\n",
    "        (\"num_km\", km_pipe, km_cols),\n",
    "        (\"num_pct\", pct_pipe, pct_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    transformer_weights={\n",
    "        \"text\": 1.0,\n",
    "        \"num_count\": 10.0,\n",
    "        \"num_km\": 8.0,\n",
    "        \"num_pct\": 2.0,\n",
    "    },\n",
    "    sparse_threshold=0.3\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "clf = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", XGBClassifier(\n",
    "        tree_method=\"gpu_hist\",           # \"gpu_hist\" falls wirklich GPU\n",
    "        n_estimators=1200,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=6,\n",
    "        min_child_weight=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=2.0,\n",
    "        reg_alpha=0.5,\n",
    "        objective=\"multi:softprob\",\n",
    "        eval_metric=\"mlogloss\",\n",
    "        num_class=n_classes,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_enc, test_size=0.25, random_state=42, stratify=y_enc\n",
    ")\n",
    "class_counts = pd.Series(y_enc).value_counts().sort_index().values\n",
    "alpha = 0.5                      # 0.5–1.0 probieren\n",
    "class_weights = (1.0 / (class_counts ** alpha))\n",
    "sample_weight = class_weights[y_enc]\n",
    "\n",
    "clf.fit(X_train, y_train, model__sample_weight=sample_weight[X_train.index])\n",
    "\n",
    "# --- 3) Threshold-Tuning auf Validation ------------------------------------\n",
    "P_val = clf.predict_proba(X_train)  # shape: (n_val, n_classes)\n",
    "n_classes = P_val.shape[1]\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "def predict_with_class_thresholds(P, th_vec):\n",
    "    th_safe = np.clip(th_vec, 1e-6, 1.0)   # Schutz gegen 0\n",
    "    scores = P / th_safe                   # kleiner th_j begünstigt Klasse j\n",
    "    return scores.argmax(axis=1)\n",
    "\n",
    "def coordinate_descent_thresholds(P, y_true, n_classes, iters=3, grid=None, init=0.5):\n",
    "    if grid is None:\n",
    "        grid = np.linspace(0.2, 0.8, 13)\n",
    "    th = np.full(n_classes, init, dtype=float)\n",
    "    best_pred = predict_with_class_thresholds(P, th)\n",
    "    best_f1 = f1_score(y_true, best_pred, average=\"macro\")\n",
    "    for _ in range(iters):\n",
    "        improved = False\n",
    "        for c in range(n_classes):\n",
    "            best_th_c = th[c]\n",
    "            best_local = best_f1\n",
    "            for t in grid:\n",
    "                trial = th.copy(); trial[c] = t\n",
    "                pred = predict_with_class_thresholds(P, trial)\n",
    "                f1 = f1_score(y_true, pred, average=\"macro\")\n",
    "                if f1 > best_local:\n",
    "                    best_local = f1\n",
    "                    best_th_c = t\n",
    "            if best_th_c != th[c]:\n",
    "                th[c] = best_th_c\n",
    "                best_f1 = best_local\n",
    "                improved = True\n",
    "        if not improved:\n",
    "            break\n",
    "    return th, best_f1\n",
    "\n",
    "best_th_vec, best_f1_val = coordinate_descent_thresholds(\n",
    "    P_val, y_test, n_classes=n_classes, iters=3, grid=np.linspace(0.2, 0.8, 13), init=0.5\n",
    ")\n",
    "print(f\"[Val] Macro-F1 mit per-Klasse-Thresholds: {best_f1_val:.4f}\")\n",
    "# Optional: die Schwellen je Klasse ansehen:\n",
    "# for cls, th in zip(le.classes_, best_th_vec): print(cls, round(th, 2))\n",
    "\n",
    "\n",
    "# --- 4) Finales Training auf ganzem Training + Test-Eval -------------------\n",
    "class_counts_train = pd.Series(y_train).value_counts().sort_index().values\n",
    "class_weights_train = (1.0 / (class_counts_train ** alpha))\n",
    "sw_train = class_weights_train[y_train]\n",
    "\n",
    "clf.fit(X_train, y_train, model__sample_weight=sw_train)\n",
    "\n",
    "P_test = clf.predict_proba(X_test)\n",
    "\n",
    "# a) Thresholded Predictions\n",
    "y_pred_thr = predict_with_class_thresholds(P_test, best_th_vec)\n",
    "\n",
    "# b) Vergleich: Argmax (Baseline)\n",
    "y_pred_argmax = P_test.argmax(axis=1)\n",
    "\n",
    "print(\"=== Thresholded ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_thr))\n",
    "print(\"Macro-F1:\", f1_score(y_test, y_pred_thr, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred_thr, zero_division=0, target_names=le.classes_))\n",
    "\n",
    "print(\"=== Argmax (Baseline) ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_argmax))\n",
    "print(\"Macro-F1:\", f1_score(y_test, y_pred_argmax, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred_argmax, zero_division=0, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77186ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Voller Code: Tabular + TF-IDF Pipeline, XGBoost, per-Klasse Threshold-Tuning (ohne Test-Leakage)\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer, RobustScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# ===== 0) Daten vorbereiten =====\n",
    "# Erwartet: peering_df_joined_with_asrank_and_domains_and_geoloc (DataFrame)\n",
    "df = peering_df_joined_with_asrank_and_domains_and_geoloc.copy()\n",
    "\n",
    "X = df[['org_name',\n",
    "        'rank', 'asnDegree_total', 'asnDegree_customer', 'asnDegree_peer', 'asnDegree_provider',\n",
    "        'cone_numberAsns', 'cone_numberPrefixes', 'cone_numberAddresses',\n",
    "        'domains', 'total_weight', 'unique_points', 'country_count',\n",
    "        'mean_km', 'var_km2', 'std_km', 'iqr_km', 'p25_km', 'p50_km', 'p75_km', 'p90_km', 'p95_km',\n",
    "        'p99_km', 'min_km', 'max_km',\n",
    "        'pct_ips_le_100km', 'pct_ips_le_500km', 'pct_ips_le_1000km'\n",
    "       ]].copy()\n",
    "\n",
    "y = df['info_type'].copy()\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "n_classes = len(le.classes_)\n",
    "\n",
    "text_col = 'org_name'\n",
    "\n",
    "# Distanz-/Streumaße (km): oft rechtsschief\n",
    "km_cols = ['mean_km', 'var_km2', 'std_km', 'iqr_km', 'p25_km', 'p50_km', 'p75_km', 'p90_km', 'p95_km',\n",
    "           'p99_km', 'min_km', 'max_km']\n",
    "\n",
    "# Zähl-/Skalenwerte: Ränge, Degrees, Cones, Domains, Gewichte, Länder, Punkte\n",
    "count_like_cols = ['rank', 'asnDegree_total', 'asnDegree_customer', 'asnDegree_peer', 'asnDegree_provider',\n",
    "                   'cone_numberAsns', 'cone_numberPrefixes', 'cone_numberAddresses',\n",
    "                   'domains', 'total_weight', 'unique_points', 'country_count']\n",
    "\n",
    "# Prozent-Features (aus SQL typischerweise 0..100)\n",
    "pct_cols = ['pct_ips_le_100km', 'pct_ips_le_500km', 'pct_ips_le_1000km']\n",
    "\n",
    "\n",
    "# ===== 1) Teil-Pipelines =====\n",
    "\n",
    "# 1a) Count/Skalen: impute -> log1p -> robust scale\n",
    "count_pipe = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n",
    "    (\"log1p\", FunctionTransformer(lambda A: np.log1p(np.clip(A, a_min=0, a_max=None)),\n",
    "                                  feature_names_out=\"one-to-one\")),\n",
    "    (\"scale\", RobustScaler())\n",
    "])\n",
    "\n",
    "# 1b) km-Metriken: impute -> log1p -> robust scale\n",
    "km_pipe = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n",
    "    (\"log1p\", FunctionTransformer(lambda A: np.log1p(np.clip(A, a_min=0, a_max=None)),\n",
    "                                  feature_names_out=\"one-to-one\")),\n",
    "    (\"scale\", RobustScaler())\n",
    "])\n",
    "\n",
    "# 1c) Prozent: 0..100 -> 0..1, dann standardisieren\n",
    "pct_pipe = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n",
    "    (\"to01\", FunctionTransformer(lambda A: np.clip(A / 100.0, 0.0, 1.0),\n",
    "                                 feature_names_out=\"one-to-one\")),\n",
    "    (\"scale\", StandardScaler())\n",
    "])\n",
    "\n",
    "# 1d) Text: TF-IDF (Char-NGrams). Optional könntest du hier noch Word-NGrams hinzufügen.\n",
    "text_pipe = Pipeline(steps=[\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        analyzer=\"char\",\n",
    "        ngram_range=(2, 5),\n",
    "        min_df=1,\n",
    "        lowercase=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# ===== 2) ColumnTransformer =====\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"text\", text_pipe, text_col),\n",
    "        (\"num_count\", count_pipe, count_like_cols),\n",
    "        (\"num_km\", km_pipe, km_cols),\n",
    "        (\"num_pct\", pct_pipe, pct_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    transformer_weights={\n",
    "        \"text\": 1.0,\n",
    "        \"num_count\": 10.0,\n",
    "        \"num_km\": 8.0,\n",
    "        \"num_pct\": 2.0,\n",
    "    },\n",
    "    sparse_threshold=0.3\n",
    ")\n",
    "\n",
    "\n",
    "# ===== 3) Modell (XGBoost) =====\n",
    "clf = Pipeline(steps=[\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", XGBClassifier(\n",
    "        tree_method=\"hist\",            # \"gpu_hist\" falls GPU vorhanden\n",
    "        n_estimators=1200,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=6,\n",
    "        min_child_weight=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=2.0,\n",
    "        reg_alpha=0.5,\n",
    "        objective=\"multi:softprob\",\n",
    "        eval_metric=\"mlogloss\",\n",
    "        num_class=n_classes,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# ===== 4) Splits =====\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_enc, test_size=0.25, random_state=42, stratify=y_enc\n",
    ")\n",
    "\n",
    "# Innerer Validation-Split NUR aus dem Training (für Threshold-Tuning)\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "\n",
    "# ===== 5) Class Imbalance via Sample Weights =====\n",
    "alpha = 0.5  # 0.5–1.0 testen\n",
    "class_counts_tr = pd.Series(y_tr).value_counts().sort_index().values\n",
    "class_weights_tr = (1.0 / (class_counts_tr ** alpha))\n",
    "sw_tr = class_weights_tr[y_tr]  # y_tr sind 0..K-1 → Indexierung passt\n",
    "\n",
    "# Fit fürs Tuning\n",
    "clf.fit(X_tr, y_tr, model__sample_weight=sw_tr)\n",
    "\n",
    "\n",
    "# ===== 6) Threshold-Tuning auf Validation (per-Klasse) =====\n",
    "P_val = clf.predict_proba(X_val)  # (n_val, n_classes)\n",
    "n_classes = P_val.shape[1]\n",
    "\n",
    "def predict_with_class_thresholds(P: np.ndarray, th_vec: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Skaliert pro Klasse die Wahrscheinlichkeiten mit 1/th.\n",
    "    Kleinere th_j -> Klasse j wird bevorzugt.\n",
    "    \"\"\"\n",
    "    th_safe = np.clip(th_vec, 1e-6, 1.0)\n",
    "    scores = P / th_safe\n",
    "    return scores.argmax(axis=1)\n",
    "\n",
    "def coordinate_descent_thresholds(P: np.ndarray, y_true: np.ndarray,\n",
    "                                  n_classes: int, iters: int = 3,\n",
    "                                  grid: np.ndarray | None = None,\n",
    "                                  init: float = 0.5):\n",
    "    \"\"\"\n",
    "    Einfache koordinatenweise Suche nach per-Klasse-Thresholds, die Macro-F1 maximieren.\n",
    "    \"\"\"\n",
    "    if grid is None:\n",
    "        grid = np.linspace(0.2, 0.8, 13)\n",
    "    th = np.full(n_classes, init, dtype=float)\n",
    "    best_pred = predict_with_class_thresholds(P, th)\n",
    "    best_f1 = f1_score(y_true, best_pred, average=\"macro\")\n",
    "\n",
    "    for _ in range(iters):\n",
    "        improved = False\n",
    "        for c in range(n_classes):\n",
    "            best_th_c = th[c]\n",
    "            best_local = best_f1\n",
    "            for t in grid:\n",
    "                trial = th.copy(); trial[c] = t\n",
    "                pred = predict_with_class_thresholds(P, trial)\n",
    "                f1 = f1_score(y_true, pred, average=\"macro\")\n",
    "                if f1 > best_local:\n",
    "                    best_local = f1\n",
    "                    best_th_c = t\n",
    "            if best_th_c != th[c]:\n",
    "                th[c] = best_th_c\n",
    "                best_f1 = best_local\n",
    "                improved = True\n",
    "        if not improved:\n",
    "            break\n",
    "    return th, best_f1\n",
    "\n",
    "best_th_vec, best_f1_val = coordinate_descent_thresholds(\n",
    "    P_val, y_val, n_classes=n_classes, iters=3, grid=np.linspace(0.2, 0.8, 13), init=0.5\n",
    ")\n",
    "print(f\"[Validation] Macro-F1 (per-Klasse Thresholds): {best_f1_val:.4f}\")\n",
    "# Optional: Thresholds je Klasse anschauen\n",
    "# for cls, th in zip(le.classes_, best_th_vec):\n",
    "#     print(f\"{cls:20s} -> th={th:.2f}\")\n",
    "\n",
    "\n",
    "# ===== 7) Finaler Fit auf komplettem Training + Test-Evaluation =====\n",
    "class_counts_train = pd.Series(y_train).value_counts().sort_index().values\n",
    "class_weights_train = (1.0 / (class_counts_train ** alpha))\n",
    "sw_train = class_weights_train[y_train]\n",
    "\n",
    "clf.fit(X_train, y_train, model__sample_weight=sw_train)\n",
    "\n",
    "# Test-Probas\n",
    "P_test = clf.predict_proba(X_test)\n",
    "\n",
    "# a) Thresholded Predictions\n",
    "y_pred_thr = predict_with_class_thresholds(P_test, best_th_vec)\n",
    "\n",
    "# b) Vergleich: Argmax (Baseline)\n",
    "y_pred_argmax = P_test.argmax(axis=1)\n",
    "\n",
    "print(\"\\n=== Thresholded ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_thr))\n",
    "print(\"Macro-F1:\", f1_score(y_test, y_pred_thr, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred_thr, zero_division=0, target_names=le.classes_))\n",
    "\n",
    "print(\"\\n=== Argmax (Baseline) ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_argmax))\n",
    "print(\"Macro-F1:\", f1_score(y_test, y_pred_argmax, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred_argmax, zero_division=0, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17b763bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6290400385914134\n",
      "Macro-F1: 0.4178898751477149\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.91      0.78      3140\n",
      "           1       0.50      0.41      0.45       635\n",
      "           2       0.61      0.61      0.61       382\n",
      "           3       0.48      0.23      0.31       449\n",
      "           4       0.50      0.21      0.29        34\n",
      "           5       0.46      0.28      0.35      1042\n",
      "           6       0.43      0.10      0.17       212\n",
      "           7       0.67      0.35      0.46       161\n",
      "           8       0.33      0.12      0.18         8\n",
      "           9       0.59      0.56      0.58       156\n",
      "\n",
      "    accuracy                           0.63      6219\n",
      "   macro avg       0.53      0.38      0.42      6219\n",
      "weighted avg       0.60      0.63      0.59      6219\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "print(\"Macro-F1:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2961fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.37256793696735807\n",
      "Macro-F1: 0.22631878427191832\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       Cable/DSL/ISP       0.76      0.49      0.60      3140\n",
      "             Content       0.27      0.28      0.28       635\n",
      "Educational/Research       0.30      0.52      0.38       382\n",
      "          Enterprise       0.21      0.17      0.19       449\n",
      "          Government       0.09      0.68      0.15        34\n",
      "                 NSP       0.35      0.20      0.26      1042\n",
      "    Network Services       0.08      0.10      0.09       212\n",
      "          Non-Profit       0.12      0.20      0.15       161\n",
      "     Route Collector       0.00      0.25      0.00         8\n",
      "        Route Server       0.13      0.21      0.16       156\n",
      "\n",
      "            accuracy                           0.37      6219\n",
      "           macro avg       0.23      0.31      0.23      6219\n",
      "        weighted avg       0.51      0.37      0.42      6219\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (6219, 3) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 110\u001b[0m\n\u001b[1;32m    108\u001b[0m topk \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m    109\u001b[0m top_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(y_proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[:, \u001b[38;5;241m-\u001b[39mtopk:][:, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]     \u001b[38;5;66;03m# Indizes der Top-k Klassen\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m top_lbl \u001b[38;5;241m=\u001b[39m \u001b[43mle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m top_pct \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtake_along_axis(y_proba, top_idx, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# in %\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Beispiel-Output als DataFrame\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:155\u001b[0m, in \u001b[0;36mLabelEncoder.inverse_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform labels back to original encoding.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m    Original encoding.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    154\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 155\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mcolumn_or_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# inverse transform of empty array is empty array\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1192\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, dtype, warn)\u001b[0m\n\u001b[1;32m   1183\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1184\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA column-vector y was passed when a 1d array was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1185\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected. Please change the shape of y to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1188\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1189\u001b[0m         )\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _asarray_with_order(xp\u001b[38;5;241m.\u001b[39mreshape(y, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m-> 1192\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1193\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my should be a 1d array, got an array of shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape)\n\u001b[1;32m   1194\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (6219, 3) instead."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# -------- Spalten (dein Setting) --------\n",
    "text_col = \"org_name\"\n",
    "pct_cols  = ['pct_ips_le_100km','pct_ips_le_500km','pct_ips_le_1000km']\n",
    "km_cols   = ['mean_km','var_km2','std_km','iqr_km','p25_km','p50_km','p75_km','p90_km','p95_km','p99_km','min_km','max_km']\n",
    "count_cols= ['rank','asnDegree_total','asnDegree_customer','asnDegree_peer','asnDegree_provider',\n",
    "             'cone_numberAsns','cone_numberPrefixes','cone_numberAddresses','domains',\n",
    "             'total_weight','unique_points','country_count']\n",
    "num_cols  = count_cols + km_cols + pct_cols\n",
    "\n",
    "# -------- Pipelines --------\n",
    "# Text → sparse TF-IDF (leicht begrenzen, weniger Rauschen)\n",
    "text_pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        analyzer=\"char_wb\", ngram_range=(3,5),\n",
    "        min_df=3, max_features=100_000, sublinear_tf=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Numerik → impute + (optional) log1p für schiefe, am Ende in CSR, damit alles sparse bleibt\n",
    "def _log1p_nonneg(X):\n",
    "    return np.log1p(np.clip(X, a_min=0, a_max=None))\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n",
    "    (\"log1p\", FunctionTransformer(_log1p_nonneg, feature_names_out=\"one-to-one\")),\n",
    "    (\"to_csr\", FunctionTransformer(lambda X: csr_matrix(X)))\n",
    "])\n",
    "\n",
    "# ColumnTransformer → liefert sparse Matrix\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"text\", text_pipe, text_col),\n",
    "        (\"num\",  num_pipe,  num_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=1.0\n",
    ")\n",
    "\n",
    "# -------- Daten laden --------\n",
    "df = peering_df_joined_with_asrank_and_domains_and_geoloc.copy()\n",
    "X = df[[text_col] + num_cols].copy()\n",
    "X[text_col] = X[text_col].fillna(\"\").astype(str)\n",
    "y = df[\"info_type\"]\n",
    "\n",
    "# Label-Encode für XGBoost (multiclass)\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "n_classes = len(le.classes_)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_enc, test_size=0.25, random_state=42, stratify=y_enc\n",
    ")\n",
    "\n",
    "# Preprocess\n",
    "Xtr = preprocessor.fit_transform(X_train)\n",
    "Xte = preprocessor.transform(X_test)\n",
    "\n",
    "# Imbalance: Sample-Weights\n",
    "w_train = compute_sample_weight(class_weight=\"balanced\", y=y_train)\n",
    "\n",
    "# -------- XGBoost (liefert Wahrscheinlichkeiten) --------\n",
    "xgb = XGBClassifier(\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=n_classes,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    n_estimators=800,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=8,\n",
    "    min_child_weight=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    gamma=0,\n",
    "    reg_lambda=1.0,\n",
    "    tree_method=\"gpu_hist\",        # \"gpu_hist\" falls NVIDIA-GPU\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# -------- Kalibrierung der Wahrscheinlichkeiten --------\n",
    "# Hinweis: CalibratedClassifierCV refittet das Basismodell intern (cv=3).\n",
    "calibrated = CalibratedClassifierCV(xgb, method=\"isotonic\", cv=3)\n",
    "calibrated.fit(Xtr, y_train, sample_weight=w_train)\n",
    "\n",
    "# Vorhersage + kalibrierte Probas\n",
    "y_proba = calibrated.predict_proba(Xte)\n",
    "y_pred  = y_proba.argmax(axis=1)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Macro-F1:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print(classification_report(le.inverse_transform(y_test),\n",
    "                            le.inverse_transform(y_pred),\n",
    "                            zero_division=0))\n",
    "\n",
    "# -------- Top-k Prozent je Beispiel (z.B. Top-3) --------\n",
    "topk = 3\n",
    "top_idx = np.argsort(y_proba, axis=1)[:, -topk:][:, ::-1]     # Indizes der Top-k Klassen\n",
    "top_lbl = le.inverse_transform(top_idx)\n",
    "top_pct = np.take_along_axis(y_proba, top_idx, axis=1) * 100  # in %\n",
    "\n",
    "# Beispiel-Output als DataFrame\n",
    "out = pd.DataFrame({\n",
    "    \"org_name\": X_test[text_col].values,\n",
    "    \"true_label\": le.inverse_transform(y_test),\n",
    "    \"pred_label\": le.inverse_transform(y_pred),\n",
    "})\n",
    "# Top-1..3 Spalten anhängen\n",
    "for k in range(topk):\n",
    "    out[f\"top{k+1}_label\"] = top_lbl[:, k]\n",
    "    out[f\"top{k+1}_pct\"]   = top_pct[:, k].round(1)\n",
    "\n",
    "# Zeig dir mal die ersten 10 Zeilen\n",
    "print(out.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d09d541",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 106\u001b[0m\n\u001b[1;32m     75\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m ColumnTransformer(\n\u001b[1;32m     76\u001b[0m     transformers\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     77\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, text_pipe, text_col),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     89\u001b[0m     sparse_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m\n\u001b[1;32m     90\u001b[0m )\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[1;32m     92\u001b[0m clf \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[1;32m     93\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprep\u001b[39m\u001b[38;5;124m\"\u001b[39m, preprocessor),\n\u001b[1;32m     94\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, XGBClassifier(\n\u001b[1;32m     95\u001b[0m         tree_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu_hist\u001b[39m\u001b[38;5;124m\"\u001b[39m,           \u001b[38;5;66;03m# \"gpu_hist\" falls wirklich GPU\u001b[39;00m\n\u001b[1;32m     96\u001b[0m         n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1200\u001b[39m,\n\u001b[1;32m     97\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.03\u001b[39m,\n\u001b[1;32m     98\u001b[0m         max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m,\n\u001b[1;32m     99\u001b[0m         min_child_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m    100\u001b[0m         subsample\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m,\n\u001b[1;32m    101\u001b[0m         colsample_bytree\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m,\n\u001b[1;32m    102\u001b[0m         reg_lambda\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m,\n\u001b[1;32m    103\u001b[0m         reg_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m    104\u001b[0m         objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti:softprob\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    105\u001b[0m         eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlogloss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 106\u001b[0m         num_class\u001b[38;5;241m=\u001b[39m\u001b[43mn_classes\u001b[49m,\n\u001b[1;32m    107\u001b[0m         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m    108\u001b[0m     ))\n\u001b[1;32m    109\u001b[0m ])\n\u001b[1;32m    110\u001b[0m y_enc \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39mfit_transform(y)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# ===== 1) Train/Val/Test sauber aufsetzen =====\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_classes' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dee90900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Validation] Macro-F1 (per-Klasse Thresholds): 0.2869\n",
      "\n",
      "=== Thresholded ===\n",
      "Accuracy: 0.5169641421450394\n",
      "Macro-F1: 0.29090373800733715\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       Cable/DSL/ISP       0.73      0.71      0.72      3140\n",
      "             Content       0.30      0.43      0.35       635\n",
      "Educational/Research       0.45      0.48      0.47       382\n",
      "          Enterprise       0.23      0.28      0.25       449\n",
      "          Government       0.15      0.35      0.21        34\n",
      "                 NSP       0.40      0.29      0.34      1042\n",
      "    Network Services       0.08      0.03      0.04       212\n",
      "          Non-Profit       0.25      0.20      0.22       161\n",
      "     Route Collector       0.00      0.00      0.00         8\n",
      "        Route Server       0.24      0.45      0.31       156\n",
      "\n",
      "            accuracy                           0.52      6219\n",
      "           macro avg       0.28      0.32      0.29      6219\n",
      "        weighted avg       0.53      0.52      0.52      6219\n",
      "\n",
      "\n",
      "=== Argmax (Baseline) ===\n",
      "Accuracy: 0.5460684997588037\n",
      "Macro-F1: 0.26706487616787944\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       Cable/DSL/ISP       0.69      0.80      0.74      3140\n",
      "             Content       0.33      0.28      0.31       635\n",
      "Educational/Research       0.40      0.56      0.47       382\n",
      "          Enterprise       0.27      0.21      0.23       449\n",
      "          Government       0.50      0.03      0.06        34\n",
      "                 NSP       0.43      0.26      0.32      1042\n",
      "    Network Services       0.14      0.00      0.01       212\n",
      "          Non-Profit       0.29      0.16      0.21       161\n",
      "     Route Collector       0.00      0.00      0.00         8\n",
      "        Route Server       0.23      0.59      0.33       156\n",
      "\n",
      "            accuracy                           0.55      6219\n",
      "           macro avg       0.33      0.29      0.27      6219\n",
      "        weighted avg       0.52      0.55      0.52      6219\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Voller Code: Tabular + TF-IDF Pipeline, XGBoost, per-Klasse Threshold-Tuning (ohne Test-Leakage)\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer, RobustScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# ===== 0) Daten vorbereiten =====\n",
    "# Erwartet: peering_df_joined_with_asrank_and_domains_and_geoloc (DataFrame)\n",
    "df = peering_df_joined_with_asrank_and_domains_and_geoloc.copy()\n",
    "\n",
    "X = df[['org_name',\n",
    "        'rank', 'asnDegree_total', 'asnDegree_customer', 'asnDegree_peer', 'asnDegree_provider',\n",
    "        'cone_numberAsns', 'cone_numberPrefixes', 'cone_numberAddresses',\n",
    "        'domains', 'total_weight', 'unique_points', 'country_count',\n",
    "        'mean_km', 'var_km2', 'std_km', 'iqr_km', 'p25_km', 'p50_km', 'p75_km', 'p90_km', 'p95_km',\n",
    "        'p99_km', 'min_km', 'max_km',\n",
    "        'pct_ips_le_100km', 'pct_ips_le_500km', 'pct_ips_le_1000km'\n",
    "       ]].copy()\n",
    "\n",
    "y = df['info_type'].copy()\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "n_classes = len(le.classes_)\n",
    "\n",
    "text_col = 'org_name'\n",
    "\n",
    "# Distanz-/Streumaße (km): oft rechtsschief\n",
    "km_cols = ['mean_km', 'var_km2', 'std_km', 'iqr_km', 'p25_km', 'p50_km', 'p75_km', 'p90_km', 'p95_km',\n",
    "           'p99_km', 'min_km', 'max_km']\n",
    "\n",
    "# Zähl-/Skalenwerte: Ränge, Degrees, Cones, Domains, Gewichte, Länder, Punkte\n",
    "count_like_cols = ['rank', 'asnDegree_total', 'asnDegree_customer', 'asnDegree_peer', 'asnDegree_provider',\n",
    "                   'cone_numberAsns', 'cone_numberPrefixes', 'cone_numberAddresses',\n",
    "                   'domains', 'total_weight', 'unique_points', 'country_count']\n",
    "\n",
    "# Prozent-Features (aus SQL typischerweise 0..100)\n",
    "pct_cols = ['pct_ips_le_100km', 'pct_ips_le_500km', 'pct_ips_le_1000km']\n",
    "\n",
    "\n",
    "# ===== 1) Teil-Pipelines =====\n",
    "\n",
    "# 1a) Count/Skalen: impute -> log1p -> robust scale\n",
    "count_pipe = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n",
    "    (\"log1p\", FunctionTransformer(lambda A: np.log1p(np.clip(A, a_min=0, a_max=None)),\n",
    "                                  feature_names_out=\"one-to-one\")),\n",
    "    (\"scale\", RobustScaler())\n",
    "])\n",
    "\n",
    "# 1b) km-Metriken: impute -> log1p -> robust scale\n",
    "km_pipe = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n",
    "    (\"log1p\", FunctionTransformer(lambda A: np.log1p(np.clip(A, a_min=0, a_max=None)),\n",
    "                                  feature_names_out=\"one-to-one\")),\n",
    "    (\"scale\", RobustScaler())\n",
    "])\n",
    "\n",
    "# 1c) Prozent: 0..100 -> 0..1, dann standardisieren\n",
    "pct_pipe = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n",
    "    (\"to01\", FunctionTransformer(lambda A: np.clip(A / 100.0, 0.0, 1.0),\n",
    "                                 feature_names_out=\"one-to-one\")),\n",
    "    (\"scale\", StandardScaler())\n",
    "])\n",
    "\n",
    "# 1d) Text: TF-IDF (Char-NGrams). Optional könntest du hier noch Word-NGrams hinzufügen.\n",
    "text_pipe = Pipeline(steps=[\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        analyzer=\"char\",\n",
    "        ngram_range=(2, 5),\n",
    "        min_df=1,\n",
    "        lowercase=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# ===== 2) ColumnTransformer =====\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"text\", text_pipe, text_col),\n",
    "        (\"num_count\", count_pipe, count_like_cols),\n",
    "        (\"num_km\", km_pipe, km_cols),\n",
    "        (\"num_pct\", pct_pipe, pct_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    transformer_weights={\n",
    "        \"text\": 1.0,\n",
    "        \"num_count\": 10.0,\n",
    "        \"num_km\": 8.0,\n",
    "        \"num_pct\": 2.0,\n",
    "    },\n",
    "    sparse_threshold=0.3\n",
    ")\n",
    "\n",
    "\n",
    "# ===== 3) Modell (XGBoost) =====\n",
    "clf = Pipeline(steps=[\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", XGBClassifier(\n",
    "        tree_method=\"gpu_hist\",            # \"gpu_hist\" falls GPU vorhanden\n",
    "        n_estimators=1200,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=6,\n",
    "        min_child_weight=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=2.0,\n",
    "        reg_alpha=0.5,\n",
    "        objective=\"multi:softprob\",\n",
    "        eval_metric=\"mlogloss\",\n",
    "        num_class=n_classes,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# ===== 4) Splits =====\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_enc, test_size=0.25, random_state=42, stratify=y_enc\n",
    ")\n",
    "\n",
    "# Innerer Validation-Split NUR aus dem Training (für Threshold-Tuning)\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "\n",
    "# ===== 5) Class Imbalance via Sample Weights =====\n",
    "alpha = 0.5  # 0.5–1.0 testen\n",
    "class_counts_tr = pd.Series(y_tr).value_counts().sort_index().values\n",
    "class_weights_tr = (1.0 / (class_counts_tr ** alpha))\n",
    "sw_tr = class_weights_tr[y_tr]  # y_tr sind 0..K-1 → Indexierung passt\n",
    "\n",
    "# Fit fürs Tuning\n",
    "clf.fit(X_tr, y_tr, model__sample_weight=sw_tr)\n",
    "\n",
    "\n",
    "# ===== 6) Threshold-Tuning auf Validation (per-Klasse) =====\n",
    "P_val = clf.predict_proba(X_val)  # (n_val, n_classes)\n",
    "n_classes = P_val.shape[1]\n",
    "\n",
    "def predict_with_class_thresholds(P: np.ndarray, th_vec: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Skaliert pro Klasse die Wahrscheinlichkeiten mit 1/th.\n",
    "    Kleinere th_j -> Klasse j wird bevorzugt.\n",
    "    \"\"\"\n",
    "    th_safe = np.clip(th_vec, 1e-6, 1.0)\n",
    "    scores = P / th_safe\n",
    "    return scores.argmax(axis=1)\n",
    "\n",
    "def coordinate_descent_thresholds(P: np.ndarray, y_true: np.ndarray,\n",
    "                                  n_classes: int, iters: int = 3,\n",
    "                                  grid: np.ndarray | None = None,\n",
    "                                  init: float = 0.5):\n",
    "    \"\"\"\n",
    "    Einfache koordinatenweise Suche nach per-Klasse-Thresholds, die Macro-F1 maximieren.\n",
    "    \"\"\"\n",
    "    if grid is None:\n",
    "        grid = np.linspace(0.2, 0.8, 13)\n",
    "    th = np.full(n_classes, init, dtype=float)\n",
    "    best_pred = predict_with_class_thresholds(P, th)\n",
    "    best_f1 = f1_score(y_true, best_pred, average=\"macro\")\n",
    "\n",
    "    for _ in range(iters):\n",
    "        improved = False\n",
    "        for c in range(n_classes):\n",
    "            best_th_c = th[c]\n",
    "            best_local = best_f1\n",
    "            for t in grid:\n",
    "                trial = th.copy(); trial[c] = t\n",
    "                pred = predict_with_class_thresholds(P, trial)\n",
    "                f1 = f1_score(y_true, pred, average=\"macro\")\n",
    "                if f1 > best_local:\n",
    "                    best_local = f1\n",
    "                    best_th_c = t\n",
    "            if best_th_c != th[c]:\n",
    "                th[c] = best_th_c\n",
    "                best_f1 = best_local\n",
    "                improved = True\n",
    "        if not improved:\n",
    "            break\n",
    "    return th, best_f1\n",
    "\n",
    "best_th_vec, best_f1_val = coordinate_descent_thresholds(\n",
    "    P_val, y_val, n_classes=n_classes, iters=3, grid=np.linspace(0.2, 0.8, 13), init=0.5\n",
    ")\n",
    "print(f\"[Validation] Macro-F1 (per-Klasse Thresholds): {best_f1_val:.4f}\")\n",
    "# Optional: Thresholds je Klasse anschauen\n",
    "# for cls, th in zip(le.classes_, best_th_vec):\n",
    "#     print(f\"{cls:20s} -> th={th:.2f}\")\n",
    "\n",
    "\n",
    "# ===== 7) Finaler Fit auf komplettem Training + Test-Evaluation =====\n",
    "class_counts_train = pd.Series(y_train).value_counts().sort_index().values\n",
    "class_weights_train = (1.0 / (class_counts_train ** alpha))\n",
    "sw_train = class_weights_train[y_train]\n",
    "\n",
    "clf.fit(X_train, y_train, model__sample_weight=sw_train)\n",
    "\n",
    "# Test-Probas\n",
    "P_test = clf.predict_proba(X_test)\n",
    "\n",
    "# a) Thresholded Predictions\n",
    "y_pred_thr = predict_with_class_thresholds(P_test, best_th_vec)\n",
    "\n",
    "# b) Vergleich: Argmax (Baseline)\n",
    "y_pred_argmax = P_test.argmax(axis=1)\n",
    "\n",
    "print(\"\\n=== Thresholded ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_thr))\n",
    "print(\"Macro-F1:\", f1_score(y_test, y_pred_thr, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred_thr, zero_division=0, target_names=le.classes_))\n",
    "\n",
    "print(\"\\n=== Argmax (Baseline) ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_argmax))\n",
    "print(\"Macro-F1:\", f1_score(y_test, y_pred_argmax, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred_argmax, zero_division=0, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be16916a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mit Early Stopping...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 171\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# 7) Fit mit Early Stopping\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining mit Early Stopping...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 171\u001b[0m prep_tr \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m X_tr_prep \u001b[38;5;241m=\u001b[39m prep_tr\u001b[38;5;241m.\u001b[39mtransform(X_tr)\n\u001b[1;32m    173\u001b[0m X_val_prep \u001b[38;5;241m=\u001b[39m prep_tr\u001b[38;5;241m.\u001b[39mtransform(X_val)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py:693\u001b[0m, in \u001b[0;36mColumnTransformer.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit all transformers using X.\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \n\u001b[1;32m    677\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;124;03m    This estimator.\u001b[39;00m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;66;03m# we use fit_transform to make sure to set sparse_output_ (for which we\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;66;03m# need the transformed data) to have consistent output type in predict\u001b[39;00m\n\u001b[0;32m--> 693\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 142\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    147\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    148\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py:726\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_column_callables(X)\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[0;32m--> 726\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_fit_transform_one\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result:\n\u001b[1;32m    729\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fitted_transformers([])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py:657\u001b[0m, in \u001b[0;36mColumnTransformer._fit_transform\u001b[0;34m(self, X, y, func, fitted, column_as_strings)\u001b[0m\n\u001b[1;32m    651\u001b[0m transformers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m    652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(\n\u001b[1;32m    653\u001b[0m         fitted\u001b[38;5;241m=\u001b[39mfitted, replace_strings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, column_as_strings\u001b[38;5;241m=\u001b[39mcolumn_as_strings\n\u001b[1;32m    654\u001b[0m     )\n\u001b[1;32m    655\u001b[0m )\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 657\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfitted\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m            \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m            \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mColumnTransformer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py:1855\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1853\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1854\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1855\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1860\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1861\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py:1784\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1784\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py:894\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 894\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    896\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py:446\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    444\u001b[0m fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(last_step, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlast_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m last_step\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_last_step)\u001b[38;5;241m.\u001b[39mtransform(Xt)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:2121\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2116\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2117\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2118\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2119\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2120\u001b[0m )\n\u001b[0;32m-> 2121\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2123\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2124\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:1377\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1369\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1371\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1373\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1374\u001b[0m             )\n\u001b[1;32m   1375\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1377\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1380\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:1264\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1263\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1264\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1265\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1266\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:69\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[0;32m---> 69\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Produktionsreife XGBoost Klassifikation\n",
    "- OHNE skopt, torch, tabnet\n",
    "- Nur: sklearn, xgboost, pandas, numpy, matplotlib, seaborn, shap\n",
    "- Per-Klasse Threshold-Tuning mit smarter Grid-Suche\n",
    "- Early Stopping, SHAP, Confusion Matrix, Modell-Save\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer, RobustScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "# 0) Daten laden\n",
    "# =============================================================================\n",
    "df = peering_df_joined_with_asrank_and_domains_and_geoloc.copy()\n",
    "\n",
    "X = df[['org_name',\n",
    "        'rank', 'asnDegree_total', 'asnDegree_customer', 'asnDegree_peer', 'asnDegree_provider',\n",
    "        'cone_numberAsns', 'cone_numberPrefixes', 'cone_numberAddresses',\n",
    "        'domains', 'total_weight', 'unique_points', 'country_count',\n",
    "        'mean_km', 'var_km2', 'std_km', 'iqr_km', 'p25_km', 'p50_km', 'p75_km', 'p90_km', 'p95_km',\n",
    "        'p99_km', 'min_km', 'max_km',\n",
    "        'pct_ips_le_100km', 'pct_ips_le_500km', 'pct_ips_le_1000km'\n",
    "       ]].copy()\n",
    "\n",
    "y = df['info_type'].copy()\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "n_classes = len(le.classes_)\n",
    "\n",
    "# Spalten\n",
    "text_col = 'org_name'\n",
    "km_cols = ['mean_km', 'var_km2', 'std_km', 'iqr_km', 'p25_km', 'p50_km', 'p75_km', 'p90_km', 'p95_km',\n",
    "           'p99_km', 'min_km', 'max_km']\n",
    "count_like_cols = ['rank', 'asnDegree_total', 'asnDegree_customer', 'asnDegree_peer', 'asnDegree_provider',\n",
    "                   'cone_numberAsns', 'cone_numberPrefixes', 'cone_numberAddresses',\n",
    "                   'domains', 'total_weight', 'unique_points', 'country_count']\n",
    "pct_cols = ['pct_ips_le_100km', 'pct_ips_le_500km', 'pct_ips_le_1000km']\n",
    "\n",
    "# =============================================================================\n",
    "# 1) Text-Cleaning\n",
    "# =============================================================================\n",
    "def clean_org_name(X):\n",
    "    def _clean(s):\n",
    "        if not isinstance(s, str):\n",
    "            return \"\"\n",
    "        s = s.lower()\n",
    "        s = re.sub(r'\\b(gmbh|inc|ltd|llc|corp|co|ag|sa|limited|plc|bv)\\b', ' ', s)\n",
    "        s = re.sub(r'[^a-z0-9\\s]', ' ', s)\n",
    "        s = re.sub(r'\\s+', ' ', s).strip()\n",
    "        return s\n",
    "    return np.array([_clean(x) for x in X.ravel()]).reshape(-1, 1)\n",
    "\n",
    "# =============================================================================\n",
    "# 2) Pipelines\n",
    "# =============================================================================\n",
    "count_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n",
    "    (\"log1p\", FunctionTransformer(np.log1p, feature_names_out=\"one-to-one\")),\n",
    "    (\"scale\", RobustScaler())\n",
    "])\n",
    "\n",
    "km_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n",
    "    (\"log1p\", FunctionTransformer(np.log1p, feature_names_out=\"one-to-one\")),\n",
    "    (\"scale\", RobustScaler())\n",
    "])\n",
    "\n",
    "pct_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n",
    "    (\"to01\", FunctionTransformer(lambda x: np.clip(x / 100.0, 0.0, 1.0), feature_names_out=\"one-to-one\")),\n",
    "    (\"scale\", StandardScaler())\n",
    "])\n",
    "\n",
    "text_pipe = Pipeline([\n",
    "    (\"clean\", FunctionTransformer(clean_org_name, validate=False)),\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        analyzer='char_wb',\n",
    "        ngram_range=(3, 6),\n",
    "        min_df=2,\n",
    "        max_df=0.95,\n",
    "        lowercase=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "# =============================================================================\n",
    "# 3) Preprocessor\n",
    "# =============================================================================\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"text\", text_pipe, text_col),\n",
    "        (\"num_count\", count_pipe, count_like_cols),\n",
    "        (\"num_km\", km_pipe, km_cols),\n",
    "        (\"num_pct\", pct_pipe, pct_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    transformer_weights={\n",
    "        \"text\": 1.0,\n",
    "        \"num_count\": 12.0,\n",
    "        \"num_km\": 10.0,\n",
    "        \"num_pct\": 3.0,\n",
    "    },\n",
    "    sparse_threshold=0.3\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# 4) Modell\n",
    "# =============================================================================\n",
    "clf = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", XGBClassifier(\n",
    "        tree_method=\"hist\",\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=7,\n",
    "        min_child_weight=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=2.0,\n",
    "        reg_alpha=0.5,\n",
    "        objective=\"multi:softprob\",\n",
    "        eval_metric=\"mlogloss\",\n",
    "        num_class=n_classes,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# =============================================================================\n",
    "# 5) Splits\n",
    "# =============================================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_enc, test_size=0.25, random_state=42, stratify=y_enc\n",
    ")\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# 6) Sample Weights\n",
    "# =============================================================================\n",
    "alpha = 0.7\n",
    "class_counts_tr = pd.Series(y_tr).value_counts().sort_index()\n",
    "class_weights_tr = 1.0 / (class_counts_tr ** alpha)\n",
    "class_weights_tr = class_weights_tr / class_weights_tr.mean()\n",
    "sw_tr = class_weights_tr[y_tr].values\n",
    "\n",
    "# =============================================================================\n",
    "# 7) Fit mit Early Stopping\n",
    "# =============================================================================\n",
    "print(\"Training mit Early Stopping...\")\n",
    "prep_tr = preprocessor.fit(X_tr)\n",
    "X_tr_prep = prep_tr.transform(X_tr)\n",
    "X_val_prep = prep_tr.transform(X_val)\n",
    "\n",
    "clf.named_steps['model'].fit(\n",
    "    X_tr_prep, y_tr,\n",
    "    sample_weight=sw_tr,\n",
    "    eval_set=[(X_val_prep, y_val)],\n",
    "    early_stopping_rounds=80,\n",
    "    verbose=False\n",
    ")\n",
    "print(f\"   → Best iteration: {clf.named_steps['model'].best_iteration}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 8) Threshold-Tuning: Smarte Grid-Suche (ohne skopt)\n",
    "# =============================================================================\n",
    "P_val = clf.predict_proba(X_val)\n",
    "\n",
    "# Informierte Initialisierung\n",
    "prior = class_counts_tr / class_counts_tr.sum()\n",
    "init_th = np.clip(prior / prior.max(), 0.3, 0.9)\n",
    "\n",
    "def predict_with_thresholds(P, th):\n",
    "    th = np.clip(th, 1e-6, 1.0)\n",
    "    return (P / th).argmax(axis=1)\n",
    "\n",
    "def tune_thresholds_grid(P, y_true, init_th, grid_size=15, iters=3):\n",
    "    grid = np.linspace(0.2, 1.0, grid_size)\n",
    "    th = init_th.copy()\n",
    "    best_f1 = f1_score(y_true, predict_with_thresholds(P, th), average='macro')\n",
    "    \n",
    "    for it in range(iters):\n",
    "        improved = False\n",
    "        for c in range(len(th)):\n",
    "            best_local = -1\n",
    "            best_th_c = th[c]\n",
    "            for t in grid:\n",
    "                th_trial = th.copy()\n",
    "                th_trial[c] = t\n",
    "                pred = predict_with_thresholds(P, th_trial)\n",
    "                f1 = f1_score(y_true, pred, average='macro')\n",
    "                if f1 > best_local:\n",
    "                    best_local = f1\n",
    "                    best_th_c = t\n",
    "            if best_th_c != th[c]:\n",
    "                th[c] = best_th_c\n",
    "                best_f1 = best_local\n",
    "                improved = True\n",
    "        if not improved:\n",
    "            break\n",
    "    return th, best_f1\n",
    "\n",
    "print(\"Threshold-Tuning mit smarter Grid-Suche...\")\n",
    "best_th_vec, best_f1_val = tune_thresholds_grid(P_val, y_val, init_th, grid_size=13, iters=3)\n",
    "print(f\"   → Validation Macro-F1: {best_f1_val:.4f}\")\n",
    "\n",
    "# Optional: Thresholds anzeigen\n",
    "# for cls, th in zip(le.classes_, best_th_vec):\n",
    "#     print(f\"{cls:20s} -> th={th:.3f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 9) Final Fit\n",
    "# =============================================================================\n",
    "class_counts_train = pd.Series(y_train).value_counts().sort_index()\n",
    "class_weights_train = 1.0 / (class_counts_train ** alpha)\n",
    "class_weights_train = class_weights_train / class_weights_train.mean()\n",
    "sw_train = class_weights_train[y_train].values\n",
    "\n",
    "prep_train = preprocessor.fit(X_train)\n",
    "X_train_prep = prep_train.transform(X_train)\n",
    "X_test_prep = prep_train.transform(X_test)\n",
    "\n",
    "clf.named_steps['model'].fit(\n",
    "    X_train_prep, y_train,\n",
    "    sample_weight=sw_train,\n",
    "    eval_set=[(prep_train.transform(X_val), y_val)],\n",
    "    early_stopping_rounds=80,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# 10) Evaluation\n",
    "# =============================================================================\n",
    "P_test = clf.predict_proba(X_test)\n",
    "y_pred_thr = predict_with_thresholds(P_test, best_th_vec)\n",
    "y_pred_argmax = P_test.argmax(axis=1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL TEST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"Thresholded:\")\n",
    "print(f\"   Accuracy: {accuracy_score(y_test, y_pred_thr):.4f}\")\n",
    "print(f\"   Macro-F1: {f1_score(y_test, y_pred_thr, average='macro'):.4f}\")\n",
    "print(\"\\nArgmax:\")\n",
    "print(f\"   Accuracy: {accuracy_score(y_test, y_pred_argmax):.4f}\")\n",
    "print(f\"   Macro-F1: {f1_score(y_test, y_pred_argmax, average='macro'):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Thresholded):\")\n",
    "print(classification_report(y_test, y_pred_thr, target_names=le.classes_, zero_division=0))\n",
    "\n",
    "# =============================================================================\n",
    "# 11) Confusion Matrix\n",
    "# =============================================================================\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_test, y_pred_thr)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title('Confusion Matrix (Thresholded)')\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 12) SHAP\n",
    "# =============================================================================\n",
    "print(\"\\nBerechne SHAP Values...\")\n",
    "explainer = shap.TreeExplainer(clf.named_steps['model'])\n",
    "X_sample = shap.sample(X_train_prep, 100)\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", max_display=20)\n",
    "plt.title(\"SHAP Feature Importance\")\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 13) Speichern\n",
    "# =============================================================================\n",
    "import os\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "joblib.dump(clf, 'models/xgboost_asn_classifier.pkl')\n",
    "joblib.dump(le, 'models/label_encoder.pkl')\n",
    "joblib.dump(best_th_vec, 'models/thresholds.npy')\n",
    "joblib.dump(preprocessor, 'models/preprocessor.pkl')\n",
    "\n",
    "print(\"\\nModelle gespeichert in ./models/\")\n",
    "\n",
    "# =============================================================================\n",
    "# 14) Inference-Funktion\n",
    "# =============================================================================\n",
    "def predict_asn_type(org_name, **kwargs):\n",
    "    row = pd.DataFrame([{**{'org_name': org_name}, **kwargs}])\n",
    "    row = row[X.columns]\n",
    "    proba = clf.predict_proba(row)[0]\n",
    "    pred = predict_with_thresholds(proba.reshape(1, -1), best_th_vec)[0]\n",
    "    return {\n",
    "        'class': le.classes_[pred],\n",
    "        'probabilities': dict(zip(le.classes_, proba))\n",
    "    }\n",
    "\n",
    "# Beispiel:\n",
    "# pred = predict_asn_type(\"Google LLC\", rank=1, asnDegree_total=5000, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3462a8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-Klasse-Thresholds (Validation):\n",
      "  Cable/DSL/ISP        -> 0.397\n",
      "  Content              -> 0.167\n",
      "  Educational/Research -> 0.355\n",
      "  Enterprise           -> 0.193\n",
      "  Government           -> 0.136\n",
      "  NSP                  -> 0.202\n",
      "  Network Services     -> 0.103\n",
      "  Non-Profit           -> 0.129\n",
      "  Route Collector      -> 0.054\n",
      "  Route Server         -> 0.314\n",
      "\n",
      "=== Thresholded (calibrated) ===\n",
      "Accuracy: 0.5598970895642387\n",
      "Macro-F1: 0.3188717783511447\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       Cable/DSL/ISP       0.71      0.81      0.76      3140\n",
      "             Content       0.32      0.51      0.39       635\n",
      "Educational/Research       0.55      0.47      0.51       382\n",
      "          Enterprise       0.23      0.13      0.17       449\n",
      "          Government       0.24      0.24      0.24        34\n",
      "                 NSP       0.42      0.25      0.31      1042\n",
      "    Network Services       0.06      0.02      0.03       212\n",
      "          Non-Profit       0.25      0.26      0.26       161\n",
      "     Route Collector       0.00      0.00      0.00         8\n",
      "        Route Server       0.64      0.44      0.52       156\n",
      "\n",
      "            accuracy                           0.56      6219\n",
      "           macro avg       0.34      0.31      0.32      6219\n",
      "        weighted avg       0.54      0.56      0.54      6219\n",
      "\n",
      "\n",
      "=== Argmax (calibrated) ===\n",
      "Accuracy: 0.583855925389934\n",
      "Macro-F1: 0.30767980214701957\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       Cable/DSL/ISP       0.64      0.91      0.75      3140\n",
      "             Content       0.37      0.32      0.35       635\n",
      "Educational/Research       0.44      0.60      0.50       382\n",
      "          Enterprise       0.34      0.10      0.16       449\n",
      "          Government       0.46      0.18      0.26        34\n",
      "                 NSP       0.53      0.18      0.27      1042\n",
      "    Network Services       0.12      0.00      0.01       212\n",
      "          Non-Profit       0.62      0.15      0.24       161\n",
      "     Route Collector       0.00      0.00      0.00         8\n",
      "        Route Server       0.65      0.46      0.54       156\n",
      "\n",
      "            accuracy                           0.58      6219\n",
      "           macro avg       0.42      0.29      0.31      6219\n",
      "        weighted avg       0.54      0.58      0.52      6219\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, precision_recall_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer, RobustScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import clone\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ===== Daten =====\n",
    "df = peering_df_joined_with_asrank_and_domains_and_geoloc.copy()\n",
    "\n",
    "X = df[['org_name',\n",
    "        'rank','asnDegree_total','asnDegree_customer','asnDegree_peer','asnDegree_provider',\n",
    "        'cone_numberAsns','cone_numberPrefixes','cone_numberAddresses',\n",
    "        'domains','total_weight','unique_points','country_count',\n",
    "        'mean_km','var_km2','std_km','iqr_km','p25_km','p50_km','p75_km','p90_km','p95_km',\n",
    "        'p99_km','min_km','max_km',\n",
    "        'pct_ips_le_100km','pct_ips_le_500km','pct_ips_le_1000km']].copy()\n",
    "\n",
    "y = df['info_type'].copy()\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "n_classes = len(le.classes_)\n",
    "\n",
    "text_col = 'org_name'\n",
    "km_cols = ['mean_km','var_km2','std_km','iqr_km','p25_km','p50_km','p75_km','p90_km','p95_km','p99_km','min_km','max_km']\n",
    "count_like_cols = ['rank','asnDegree_total','asnDegree_customer','asnDegree_peer','asnDegree_provider',\n",
    "                   'cone_numberAsns','cone_numberPrefixes','cone_numberAddresses',\n",
    "                   'domains','total_weight','unique_points','country_count']\n",
    "pct_cols = ['pct_ips_le_100km','pct_ips_le_500km','pct_ips_le_1000km']\n",
    "\n",
    "# ===== Teil-Pipelines =====\n",
    "count_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n",
    "    (\"log1p\", FunctionTransformer(lambda A: np.log1p(np.clip(A, a_min=0, a_max=None)),\n",
    "                                  feature_names_out=\"one-to-one\")),\n",
    "    (\"scale\", RobustScaler())\n",
    "])\n",
    "\n",
    "km_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n",
    "    (\"log1p\", FunctionTransformer(lambda A: np.log1p(np.clip(A, a_min=0, a_max=None)),\n",
    "                                  feature_names_out=\"one-to-one\")),\n",
    "    (\"scale\", RobustScaler())\n",
    "])\n",
    "\n",
    "pct_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n",
    "    (\"to01\", FunctionTransformer(lambda A: np.clip(A / 100.0, 0.0, 1.0),\n",
    "                                 feature_names_out=\"one-to-one\")),\n",
    "    (\"scale\", StandardScaler())\n",
    "])\n",
    "\n",
    "text_pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        analyzer=\"char\",\n",
    "        ngram_range=(2,5),\n",
    "        min_df=1,\n",
    "        lowercase=True\n",
    "    ))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"text\", text_pipe, text_col),\n",
    "        (\"num_count\", count_pipe, count_like_cols),\n",
    "        (\"num_km\", km_pipe, km_cols),\n",
    "        (\"num_pct\", pct_pipe, pct_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    transformer_weights={\n",
    "        # gib dem Text etwas mehr Gewicht, hilft oft kleineren Klassen\n",
    "        \"text\": 1.0,\n",
    "        \"num_count\": 10.0,\n",
    "        \"num_km\": 8.0,\n",
    "        \"num_pct\": 8.0,\n",
    "    },\n",
    "    sparse_threshold=0.3\n",
    ")\n",
    "\n",
    "base_model = XGBClassifier(\n",
    "    tree_method=\"gpu_hist\",          # \"gpu_hist\" falls GPU\n",
    "    n_estimators=1200,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=6,\n",
    "    min_child_weight=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=2.0,\n",
    "    reg_alpha=0.5,\n",
    "    objective=\"multi:softprob\",\n",
    "    eval_metric=\"mlogloss\",\n",
    "    num_class=n_classes,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", base_model)\n",
    "])\n",
    "\n",
    "# ===== Splits =====\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_enc, test_size=0.25, random_state=42, stratify=y_enc\n",
    ")\n",
    "\n",
    "# Innerer Val-Split für Kalibrierung + Threshold-Tuning\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# ===== Class Imbalance (vorsichtig) =====\n",
    "alpha = 0.35   # etwas schwächer gewichten, um Overfitting großer Klassen zu vermeiden\n",
    "class_counts_tr = pd.Series(y_tr).value_counts().sort_index().values\n",
    "class_weights_tr = (1.0 / (class_counts_tr ** alpha))\n",
    "sw_tr = class_weights_tr[y_tr]\n",
    "\n",
    "# Fit Basismodell auf (X_tr, y_tr)\n",
    "clf.fit(X_tr, y_tr, model__sample_weight=sw_tr)\n",
    "\n",
    "# ===== Kalibrierung der Wahrscheinlichkeiten =====\n",
    "# 'prefit=True': nutzt die bereits trainierte Pipeline und kalibriert darauf.\n",
    "# Methode: 'isotonic' (besser, aber langsamer) oder 'sigmoid' (robust, schnell).\n",
    "cal = CalibratedClassifierCV(estimator=clf, method=\"isotonic\", cv=\"prefit\")\n",
    "cal.fit(X_val, y_val)\n",
    "\n",
    "# Probas (kalibriert) auf Validation\n",
    "P_val = cal.predict_proba(X_val)  # (n_val, K)\n",
    "\n",
    "# ===== Per-Klasse Thresholds über PR-Kurve (F1-optimal) =====\n",
    "def f1_opt_threshold_for_class(y_true_bin, scores):\n",
    "    # scores: Wahrscheinlichkeiten für Klasse c (kalibriert)\n",
    "    p, r, th = precision_recall_curve(y_true_bin, scores)\n",
    "    # precision_recall_curve gibt th mit Länge len(p)-1 zurück\n",
    "    f1s = (2 * p[1:] * r[1:]) / np.clip(p[1:] + r[1:], 1e-12, None)\n",
    "    if len(f1s) == 0:\n",
    "        return 0.5  # fallback\n",
    "    idx = np.nanargmax(f1s)\n",
    "    return float(th[idx])\n",
    "\n",
    "best_th_vec = np.zeros(n_classes, dtype=float)\n",
    "for c in range(n_classes):\n",
    "    y_bin = (y_val == c).astype(int)\n",
    "    scores_c = P_val[:, c]\n",
    "    best_th_vec[c] = f1_opt_threshold_for_class(y_bin, scores_c)\n",
    "\n",
    "print(\"Per-Klasse-Thresholds (Validation):\")\n",
    "for cls, th in zip(le.classes_, best_th_vec):\n",
    "    print(f\"  {cls:20s} -> {th:.3f}\")\n",
    "\n",
    "# ===== Final: Refit auf ganzem Training, Kalibrieren auf X_val, Test evaluieren =====\n",
    "# (Wir refitten absichtlich NICHT erneut vor der Kalibrierung, da cal schon 'prefit' nutzt.\n",
    "#  Für maximale Strenge kannst du neu fitten auf X_train und dann erneut CalibratedCV(cv='prefit') auf X_val machen.)\n",
    "\n",
    "# Kalibrierte Probas auf Test\n",
    "P_test = cal.predict_proba(X_test)\n",
    "\n",
    "# a) Thresholded Vorhersage: pro Klasse eigener Schwellwert\n",
    "def predict_with_thresholds(P, th_vec):\n",
    "    # weicher One-vs-Rest: nimm argmax der Scores, aber „zulassen“ nur wenn >= th der jeweiligen Klasse?\n",
    "    # Für Multiclass ohne Reject reicht: klassifiziere zur Klasse mit größtem (P_c - th_c Bias).\n",
    "    # Hier nutzen wir den OVR-Ansatz: Score = P_c - th_c, wähle argmax.\n",
    "    scores = P - th_vec[None, :]\n",
    "    return scores.argmax(axis=1)\n",
    "\n",
    "y_pred_thr = predict_with_thresholds(P_test, best_th_vec)\n",
    "\n",
    "# b) Argmax (Baseline)\n",
    "y_pred_argmax = P_test.argmax(axis=1)\n",
    "\n",
    "print(\"\\n=== Thresholded (calibrated) ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_thr))\n",
    "print(\"Macro-F1:\", f1_score(y_test, y_pred_thr, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred_thr, zero_division=0, target_names=le.classes_))\n",
    "\n",
    "print(\"\\n=== Argmax (calibrated) ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_argmax))\n",
    "print(\"Macro-F1:\", f1_score(y_test, y_pred_argmax, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred_argmax, zero_division=0, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "381c4dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-Klasse-Thresholds (Validation):\n",
      "  Cable/DSL/ISP        -> 0.395\n",
      "  Content              -> 0.138\n",
      "  Educational/Research -> 0.169\n",
      "  Enterprise           -> 0.105\n",
      "  Government           -> 0.037\n",
      "  NSP                  -> 0.214\n",
      "  Network Services     -> 0.064\n",
      "  Non-Profit           -> 0.065\n",
      "  Route Collector      -> 0.048\n",
      "  Route Server         -> 0.500\n",
      "\n",
      "=== Thresholded (calibrated) ===\n",
      "Accuracy: 0.5632738382376588\n",
      "Macro-F1: 0.3322605330035301\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       Cable/DSL/ISP       0.71      0.80      0.75      3140\n",
      "             Content       0.36      0.41      0.38       635\n",
      "Educational/Research       0.49      0.57      0.52       382\n",
      "          Enterprise       0.26      0.29      0.27       449\n",
      "          Government       0.38      0.18      0.24        34\n",
      "                 NSP       0.43      0.24      0.31      1042\n",
      "    Network Services       0.07      0.05      0.06       212\n",
      "          Non-Profit       0.31      0.28      0.30       161\n",
      "     Route Collector       0.00      0.00      0.00         8\n",
      "        Route Server       0.69      0.38      0.49       156\n",
      "\n",
      "            accuracy                           0.56      6219\n",
      "           macro avg       0.37      0.32      0.33      6219\n",
      "        weighted avg       0.54      0.56      0.55      6219\n",
      "\n",
      "\n",
      "=== Argmax (calibrated) ===\n",
      "Accuracy: 0.5856246985045828\n",
      "Macro-F1: 0.3120256238058812\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       Cable/DSL/ISP       0.62      0.92      0.74      3140\n",
      "             Content       0.40      0.29      0.34       635\n",
      "Educational/Research       0.53      0.52      0.52       382\n",
      "          Enterprise       0.39      0.17      0.23       449\n",
      "          Government       0.67      0.12      0.20        34\n",
      "                 NSP       0.51      0.19      0.28      1042\n",
      "    Network Services       0.11      0.00      0.01       212\n",
      "          Non-Profit       0.51      0.16      0.24       161\n",
      "     Route Collector       0.00      0.00      0.00         8\n",
      "        Route Server       0.64      0.49      0.56       156\n",
      "\n",
      "            accuracy                           0.59      6219\n",
      "           macro avg       0.44      0.29      0.31      6219\n",
      "        weighted avg       0.54      0.59      0.53      6219\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cal = CalibratedClassifierCV(estimator=clf, method=\"sigmoid\", cv=\"prefit\")\n",
    "cal.fit(X_val, y_val)\n",
    "\n",
    "# Probas (kalibriert) auf Validation\n",
    "P_val = cal.predict_proba(X_val)  # (n_val, K)\n",
    "\n",
    "# ===== Per-Klasse Thresholds über PR-Kurve (F1-optimal) =====\n",
    "def f1_opt_threshold_for_class(y_true_bin, scores):\n",
    "    # scores: Wahrscheinlichkeiten für Klasse c (kalibriert)\n",
    "    p, r, th = precision_recall_curve(y_true_bin, scores)\n",
    "    # precision_recall_curve gibt th mit Länge len(p)-1 zurück\n",
    "    f1s = (2 * p[1:] * r[1:]) / np.clip(p[1:] + r[1:], 1e-12, None)\n",
    "    if len(f1s) == 0:\n",
    "        return 0.5  # fallback\n",
    "    idx = np.nanargmax(f1s)\n",
    "    return float(th[idx])\n",
    "\n",
    "best_th_vec = np.zeros(n_classes, dtype=float)\n",
    "for c in range(n_classes):\n",
    "    y_bin = (y_val == c).astype(int)\n",
    "    scores_c = P_val[:, c]\n",
    "    best_th_vec[c] = f1_opt_threshold_for_class(y_bin, scores_c)\n",
    "\n",
    "print(\"Per-Klasse-Thresholds (Validation):\")\n",
    "for cls, th in zip(le.classes_, best_th_vec):\n",
    "    print(f\"  {cls:20s} -> {th:.3f}\")\n",
    "\n",
    "# ===== Final: Refit auf ganzem Training, Kalibrieren auf X_val, Test evaluieren =====\n",
    "# (Wir refitten absichtlich NICHT erneut vor der Kalibrierung, da cal schon 'prefit' nutzt.\n",
    "#  Für maximale Strenge kannst du neu fitten auf X_train und dann erneut CalibratedCV(cv='prefit') auf X_val machen.)\n",
    "\n",
    "# Kalibrierte Probas auf Test\n",
    "P_test = cal.predict_proba(X_test)\n",
    "\n",
    "# a) Thresholded Vorhersage: pro Klasse eigener Schwellwert\n",
    "def predict_with_thresholds(P, th_vec):\n",
    "    # weicher One-vs-Rest: nimm argmax der Scores, aber „zulassen“ nur wenn >= th der jeweiligen Klasse?\n",
    "    # Für Multiclass ohne Reject reicht: klassifiziere zur Klasse mit größtem (P_c - th_c Bias).\n",
    "    # Hier nutzen wir den OVR-Ansatz: Score = P_c - th_c, wähle argmax.\n",
    "    scores = P - th_vec[None, :]\n",
    "    return scores.argmax(axis=1)\n",
    "\n",
    "y_pred_thr = predict_with_thresholds(P_test, best_th_vec)\n",
    "\n",
    "# b) Argmax (Baseline)\n",
    "y_pred_argmax = P_test.argmax(axis=1)\n",
    "\n",
    "print(\"\\n=== Thresholded (calibrated) ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_thr))\n",
    "print(\"Macro-F1:\", f1_score(y_test, y_pred_thr, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred_thr, zero_division=0, target_names=le.classes_))\n",
    "\n",
    "print(\"\\n=== Argmax (calibrated) ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_argmax))\n",
    "print(\"Macro-F1:\", f1_score(y_test, y_pred_argmax, average=\"macro\"))\n",
    "print(classification_report(y_test, y_pred_argmax, zero_division=0, target_names=le.classes_))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
